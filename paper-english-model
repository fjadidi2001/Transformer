# ============================================================================
# STEP 6: MULTIMODAL MODEL ARCHITECTURE
# ============================================================================

class MultiModalTransformer(nn.Module):
    """Multimodal Transformer for Alzheimer's Detection combining BERT and ViT"""

    def __init__(self, num_classes=2, dropout_rate=0.3, fusion_dim=512):
        super(MultiModalTransformer, self).__init__()

        # Text branch - BERT
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.text_dropout = nn.Dropout(dropout_rate)
        self.text_projection = nn.Linear(768, fusion_dim)

        # Audio branch - ViT for spectrograms
        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')
        self.audio_dropout = nn.Dropout(dropout_rate)
        self.audio_projection = nn.Linear(768, fusion_dim)

        # Fusion layers
        self.fusion_attention = nn.MultiheadAttention(
            embed_dim=fusion_dim,
            num_heads=8,
            dropout=dropout_rate,
            batch_first=True
        )

        # Classification head
        self.classifier = nn.Sequential(
            nn.Linear(fusion_dim * 2, fusion_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(fusion_dim, fusion_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(fusion_dim // 2, num_classes)
        )

        # Initialize weights
        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize classifier weights"""
        for module in [self.text_projection, self.audio_projection, self.classifier]:
            if isinstance(module, nn.Sequential):
                for layer in module:
                    if isinstance(layer, nn.Linear):
                        nn.init.xavier_uniform_(layer.weight)
                        nn.init.zeros_(layer.bias)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.zeros_(module.bias)

    def forward(self, input_ids, attention_mask, audio_features):
        # Text processing
        text_outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        text_features = text_outputs.pooler_output  # [CLS] token representation
        text_features = self.text_dropout(text_features)
        text_features = self.text_projection(text_features)  # Shape: [batch, fusion_dim]

        # Audio processing
        audio_outputs = self.vit(pixel_values=audio_features)
        audio_features = audio_outputs.pooler_output  # Global representation
        audio_features = self.audio_dropout(audio_features)
        audio_features = self.audio_projection(audio_features)  # Shape: [batch, fusion_dim]

        # Cross-modal attention fusion
        # Prepare for attention: [batch, seq_len, embed_dim]
        text_for_attention = text_features.unsqueeze(1)  # [batch, 1, fusion_dim]
        audio_for_attention = audio_features.unsqueeze(1)  # [batch, 1, fusion_dim]

        # Cross-attention: text attending to audio
        text_attended, _ = self.fusion_attention(
            text_for_attention, audio_for_attention, audio_for_attention
        )
        text_attended = text_attended.squeeze(1)  # [batch, fusion_dim]

        # Cross-attention: audio attending to text
        audio_attended, _ = self.fusion_attention(
            audio_for_attention, text_for_attention, text_for_attention
        )
        audio_attended = audio_attended.squeeze(1)  # [batch, fusion_dim]

        # Concatenate attended features
        fused_features = torch.cat([text_attended, audio_attended], dim=1)  # [batch, fusion_dim*2]

        # Classification
        logits = self.classifier(fused_features)

        return logits, {
            'text_features': text_features,
            'audio_features': audio_features,
            'fused_features': fused_features,
            'text_attended': text_attended,
            'audio_attended': audio_attended
        }

# ============================================================================
# STEP 7: TRAINING UTILITIES
# ============================================================================

class EarlyStopping:
    """Early stopping utility"""

    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_score = None
        self.counter = 0
        self.best_weights = None

    def __call__(self, val_score, model):
        if self.best_score is None:
            self.best_score = val_score
            self.save_checkpoint(model)
        elif val_score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights:
                    model.load_state_dict(self.best_weights)
                return True
        else:
            self.best_score = val_score
            self.counter = 0
            self.save_checkpoint(model)
        return False

    def save_checkpoint(self, model):
        """Save model checkpoint for best performance"""
        self.best_weights = model.state_dict().copy()

class MetricsTracker:
    """Track training metrics"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.losses = []
        self.accuracies = []
        self.precisions = []
        self.recalls = []
        self.f1_scores = []

    def update(self, loss, y_true, y_pred):
        self.losses.append(loss)

        # Convert to numpy for sklearn metrics
        y_true_np = y_true.cpu().numpy() if torch.is_tensor(y_true) else y_true
        y_pred_np = y_pred.cpu().numpy() if torch.is_tensor(y_pred) else y_pred

        # Calculate metrics
        acc = accuracy_score(y_true_np, y_pred_np)
        prec = precision_score(y_true_np, y_pred_np, average='weighted', zero_division=0)
        rec = recall_score(y_true_np, y_pred_np, average='weighted', zero_division=0)
        f1 = f1_score(y_true_np, y_pred_np, average='weighted', zero_division=0)

        self.accuracies.append(acc)
        self.precisions.append(prec)
        self.recalls.append(rec)
        self.f1_scores.append(f1)

    def get_average_metrics(self):
        return {
            'loss': np.mean(self.losses) if self.losses else 0,
            'accuracy': np.mean(self.accuracies) if self.accuracies else 0,
            'precision': np.mean(self.precisions) if self.precisions else 0,
            'recall': np.mean(self.recalls) if self.recalls else 0,
            'f1_score': np.mean(self.f1_scores) if self.f1_scores else 0
        }

# ============================================================================
# STEP 8: TRAINING PIPELINE
# ============================================================================

class AlzheimerDetectionTrainer:
    """Complete training pipeline for Alzheimer's detection"""

    def __init__(self, model, train_loader, val_loader, device,
                 learning_rate=2e-5, weight_decay=0.01):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device

        # Loss function with class weighting
        self.criterion = nn.CrossEntropyLoss()

        # Optimizer with different learning rates for pretrained and new components
        self.optimizer = self._setup_optimizer(learning_rate, weight_decay)

        # Scheduler
        total_steps = len(train_loader) * 50  # Assuming max 50 epochs
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=total_steps, eta_min=1e-7
        )

        # Early stopping
        self.early_stopping = EarlyStopping(patience=10, min_delta=0.001)

        # Metrics tracking
        self.train_metrics = MetricsTracker()
        self.val_metrics = MetricsTracker()

        # History
        self.history = {
            'train_loss': [], 'val_loss': [],
            'train_acc': [], 'val_acc': [],
            'train_f1': [], 'val_f1': []
        }

    def _setup_optimizer(self, learning_rate, weight_decay):
        """Setup optimizer with different learning rates for different components"""
        # Separate parameters for different learning rates
        bert_params = list(self.model.bert.parameters())
        vit_params = list(self.model.vit.parameters())
        other_params = []

        # Collect other parameters
        for name, param in self.model.named_parameters():
            if not name.startswith('bert.') and not name.startswith('vit.'):
                other_params.append(param)

        optimizer = optim.AdamW([
            {'params': bert_params, 'lr': learning_rate * 0.1},  # Lower LR for pretrained BERT
            {'params': vit_params, 'lr': learning_rate * 0.1},   # Lower LR for pretrained ViT
            {'params': other_params, 'lr': learning_rate}        # Standard LR for new layers
        ], weight_decay=weight_decay)

        return optimizer

    def train_epoch(self):
        """Train for one epoch"""
        self.model.train()
        self.train_metrics.reset()

        pbar = tqdm(self.train_loader, desc="Training")

        for batch in pbar:
            # Move to device
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            audio_features = batch['audio_features'].to(self.device)
            labels = batch['label'].to(self.device)

            # Forward pass
            self.optimizer.zero_grad()

            try:
                logits, features = self.model(input_ids, attention_mask, audio_features)
                loss = self.criterion(logits, labels)

                # Backward pass
                loss.backward()

                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.optimizer.step()
                self.scheduler.step()

                # Predictions
                _, predicted = torch.max(logits.data, 1)

                # Update metrics
                self.train_metrics.update(loss.item(), labels, predicted)

                # Update progress bar
                current_metrics = self.train_metrics.get_average_metrics()
                pbar.set_postfix({
                    'Loss': f"{current_metrics['loss']:.4f}",
                    'Acc': f"{current_metrics['accuracy']:.4f}",
                    'F1': f"{current_metrics['f1_score']:.4f}"
                })

            except Exception as e:
                print(f"Error in training batch: {e}")
                continue

        return self.train_metrics.get_average_metrics()

    def validate_epoch(self):
        """Validate for one epoch"""
        self.model.eval()
        self.val_metrics.reset()

        all_predictions = []
        all_labels = []
        all_features = []

        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc="Validation")

            for batch in pbar:
                try:
                    # Move to device
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    audio_features = batch['audio_features'].to(self.device)
                    labels = batch['label'].to(self.device)

                    # Forward pass
                    logits, features = self.model(input_ids, attention_mask, audio_features)
                    loss = self.criterion(logits, labels)

                    # Predictions
                    _, predicted = torch.max(logits.data, 1)

                    # Store for analysis
                    all_predictions.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
                    all_features.append(features['fused_features'].cpu().numpy())

                    # Update metrics
                    self.val_metrics.update(loss.item(), labels, predicted)

                    # Update progress bar
                    current_metrics = self.val_metrics.get_average_metrics()
                    pbar.set_postfix({
                        'Loss': f"{current_metrics['loss']:.4f}",
                        'Acc': f"{current_metrics['accuracy']:.4f}",
                        'F1': f"{current_metrics['f1_score']:.4f}"
                    })

                except Exception as e:
                    print(f"Error in validation batch: {e}")
                    continue

        val_metrics = self.val_metrics.get_average_metrics()

        # Store predictions and features for analysis
        self.last_val_predictions = np.array(all_predictions)
        self.last_val_labels = np.array(all_labels)
        self.last_val_features = np.concatenate(all_features, axis=0) if all_features else None

        return val_metrics

    def train(self, num_epochs=30):
        """Complete training loop"""
        print(f"Starting training for {num_epochs} epochs...")
        print(f"Training samples: {len(self.train_loader.dataset)}")
        print(f"Validation samples: {len(self.val_loader.dataset)}")
        print("-" * 60)

        best_f1 = 0

        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch+1}/{num_epochs}")

            # Train
            train_metrics = self.train_epoch()

            # Validate
            val_metrics = self.validate_epoch()

            # Update history
            self.history['train_loss'].append(train_metrics['loss'])
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['train_acc'].append(train_metrics['accuracy'])
            self.history['val_acc'].append(val_metrics['accuracy'])
            self.history['train_f1'].append(train_metrics['f1_score'])
            self.history['val_f1'].append(val_metrics['f1_score'])

            # Print epoch results
            print(f"Train - Loss: {train_metrics['loss']:.4f}, "
                  f"Acc: {train_metrics['accuracy']:.4f}, "
                  f"F1: {train_metrics['f1_score']:.4f}")
            print(f"Val   - Loss: {val_metrics['loss']:.4f}, "
                  f"Acc: {val_metrics['accuracy']:.4f}, "
                  f"F1: {val_metrics['f1_score']:.4f}")

            # Save best model
            if val_metrics['f1_score'] > best_f1:
                best_f1 = val_metrics['f1_score']
                torch.save(self.model.state_dict(), 'best_alzheimer_model.pth')
                print(f"New best F1 score: {best_f1:.4f} - Model saved!")

            # Early stopping
            if self.early_stopping(val_metrics['f1_score'], self.model):
                print(f"\nEarly stopping triggered after {epoch+1} epochs")
                break

        print(f"\nTraining completed! Best F1 score: {best_f1:.4f}")
        return self.history

    def plot_training_history(self):
        """Plot training history"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Loss
        axes[0, 0].plot(self.history['train_loss'], label='Train Loss', alpha=0.8)
        axes[0, 0].plot(self.history['val_loss'], label='Val Loss', alpha=0.8)
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # Accuracy
        axes[0, 1].plot(self.history['train_acc'], label='Train Accuracy', alpha=0.8)
        axes[0, 1].plot(self.history['val_acc'], label='Val Accuracy', alpha=0.8)
        axes[0, 1].set_title('Training and Validation Accuracy')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # F1 Score
        axes[1, 0].plot(self.history['train_f1'], label='Train F1', alpha=0.8)
        axes[1, 0].plot(self.history['val_f1'], label='Val F1', alpha=0.8)
        axes[1, 0].set_title('Training and Validation F1 Score')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('F1 Score')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # Confusion Matrix
        if hasattr(self, 'last_val_predictions') and hasattr(self, 'last_val_labels'):
            cm = confusion_matrix(self.last_val_labels, self.last_val_predictions)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])
            axes[1, 1].set_title('Validation Confusion Matrix')
            axes[1, 1].set_xlabel('Predicted')
            axes[1, 1].set_ylabel('Actual')

        plt.tight_layout()
        plt.show()

    def generate_classification_report(self):
        """Generate detailed classification report"""
        if hasattr(self, 'last_val_predictions') and hasattr(self, 'last_val_labels'):
            print("\nDetailed Classification Report:")
            print("-" * 50)
            print(classification_report(
                self.last_val_labels,
                self.last_val_predictions,
                target_names=['Control (CN)', 'Alzheimer\'s (AD)'],
                digits=4
            ))
        else:
            print("No validation predictions available for report generation.")

# ============================================================================
# STEP 9: MAIN TRAINING EXECUTION
# ============================================================================

def train_alzheimer_model(train_loader, val_loader, device, num_epochs=25):
    """Main function to train the Alzheimer's detection model"""

    print("="*60)
    print("ALZHEIMER'S DETECTION MODEL TRAINING")
    print("="*60)

    # Initialize model
    print("Initializing multimodal model...")
    model = MultiModalTransformer(
        num_classes=2,
        dropout_rate=0.3,
        fusion_dim=512
    )

    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

    # Initialize trainer
    trainer = AlzheimerDetectionTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        learning_rate=2e-5,
        weight_decay=0.01
    )

    # Train model
    print(f"\nStarting training on {device}...")
    history = trainer.train(num_epochs=num_epochs)

    # Plot results
    trainer.plot_training_history()

    # Generate report
    trainer.generate_classification_report()

    return model, trainer, history

# Execute training if train_loader and val_loader are available
if 'train_loader' in globals() and 'val_loader' in globals() and train_loader is not None:
    print("Starting model training...")

    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Train the model
    trained_model, trainer, training_history = train_alzheimer_model(
        train_loader, val_loader, device, num_epochs=20
    )

    print("\nTraining completed successfully!")
    print("Model saved as 'best_alzheimer_model.pth'")

else:
    print("train_loader and val_loader not found. Please run the data processing steps first.")
    print("You can run the training with:")
    print("trained_model, trainer, history = train_alzheimer_model(train_loader, val_loader, device)")

output:
Train - Loss: 0.0226, Acc: 0.9896, F1: 0.9870
Val   - Loss: 0.4731, Acc: 0.8958, F1: 0.8806

Training completed! Best F1 score: 0.9103
Detailed Classification Report:
--------------------------------------------------
                  precision    recall  f1-score   support

    Control (CN)     0.8667    0.8125    0.8387        16
Alzheimer's (AD)     0.9091    0.9375    0.9231        32

        accuracy                         0.8958        48
       macro avg     0.8879    0.8750    0.8809        48
    weighted avg     0.8949    0.8958    0.8950        48


Training completed successfully!
Model saved as 'best_alzheimer_model.pth'
