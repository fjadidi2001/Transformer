{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMznpVb6/U621fMtR0gMzaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a7eaa4b2d5f403cb80d07312619a667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8dcad052691c478a979a069cfc9caccf",
              "IPY_MODEL_b2b9c39212a94262a3cb2897f979d550",
              "IPY_MODEL_c68c80b82b114c558074fcdb668bd4d9"
            ],
            "layout": "IPY_MODEL_7c4c625ec9d5470b94c52ad4209a6deb"
          }
        },
        "8dcad052691c478a979a069cfc9caccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_becc661ac52d43309989eae176a92149",
            "placeholder": "​",
            "style": "IPY_MODEL_ac9c985f6ef2411fb80c2c7083ec8972",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b2b9c39212a94262a3cb2897f979d550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df85847ec24e416390d1c8db8f588760",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e29c8dce32344395ad9510c3f86e6170",
            "value": 48
          }
        },
        "c68c80b82b114c558074fcdb668bd4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d45e4db1c694728944ab31d6f7cedd8",
            "placeholder": "​",
            "style": "IPY_MODEL_cb8f0335297d4f1f97e26c06a689cc71",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.04kB/s]"
          }
        },
        "7c4c625ec9d5470b94c52ad4209a6deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "becc661ac52d43309989eae176a92149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac9c985f6ef2411fb80c2c7083ec8972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df85847ec24e416390d1c8db8f588760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e29c8dce32344395ad9510c3f86e6170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d45e4db1c694728944ab31d6f7cedd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb8f0335297d4f1f97e26c06a689cc71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5ebbc015be14acebce22b2172c030df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_456da9b8496841c5bedefa1d060bf597",
              "IPY_MODEL_c085f9e257794495a69066787e640f8a",
              "IPY_MODEL_85e73bb9d98146e5872cde14ab9af0b8"
            ],
            "layout": "IPY_MODEL_5e259c8e9a2e4fe0b5c4b93c16f55978"
          }
        },
        "456da9b8496841c5bedefa1d060bf597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b8aad7048ea4be1b4fad8c03f563126",
            "placeholder": "​",
            "style": "IPY_MODEL_ce12e59fff784143b1e6f7e692218bb0",
            "value": "config.json: 100%"
          }
        },
        "c085f9e257794495a69066787e640f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2224a2f68c1648dbb3964d7e320eeed3",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aef1eb196714d12aede492560c992b8",
            "value": 570
          }
        },
        "85e73bb9d98146e5872cde14ab9af0b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f21c6aeef74c43afaa59816ab01bfaab",
            "placeholder": "​",
            "style": "IPY_MODEL_91f1a331a5d7409691e2f1b5b57d2c21",
            "value": " 570/570 [00:00&lt;00:00, 61.7kB/s]"
          }
        },
        "5e259c8e9a2e4fe0b5c4b93c16f55978": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8aad7048ea4be1b4fad8c03f563126": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce12e59fff784143b1e6f7e692218bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2224a2f68c1648dbb3964d7e320eeed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aef1eb196714d12aede492560c992b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f21c6aeef74c43afaa59816ab01bfaab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f1a331a5d7409691e2f1b5b57d2c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3ab2005b2fc464894b85871a5f9c1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cba99f177641484289fe61d739749c05",
              "IPY_MODEL_a4da2fa8f91544518f8d0f17191dcb11",
              "IPY_MODEL_2b2d247b8fd44f51a900433cad9ed2d0"
            ],
            "layout": "IPY_MODEL_f2c531929ab946a4a76516d5eeda1807"
          }
        },
        "cba99f177641484289fe61d739749c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5fa90d20fae472ba7d1024d6fa1673f",
            "placeholder": "​",
            "style": "IPY_MODEL_d2e2a458185b48fdb2101c4bc57471fd",
            "value": "vocab.txt: 100%"
          }
        },
        "a4da2fa8f91544518f8d0f17191dcb11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1032802e3544c228106e4dc089bba4f",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53200fd4ef104c2e8ecb338be26794d0",
            "value": 231508
          }
        },
        "2b2d247b8fd44f51a900433cad9ed2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_518b0cc591fb46f281b628e48f735c37",
            "placeholder": "​",
            "style": "IPY_MODEL_1d7dccb3395640bf96b220967b90d2e9",
            "value": " 232k/232k [00:00&lt;00:00, 551kB/s]"
          }
        },
        "f2c531929ab946a4a76516d5eeda1807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5fa90d20fae472ba7d1024d6fa1673f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2e2a458185b48fdb2101c4bc57471fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1032802e3544c228106e4dc089bba4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53200fd4ef104c2e8ecb338be26794d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "518b0cc591fb46f281b628e48f735c37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7dccb3395640bf96b220967b90d2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "653825fc95694b6786610510b9e95a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2baad0073fe41f7b394dd191eb016ef",
              "IPY_MODEL_009fef0472be4b0ab1e12f61ecf72f79",
              "IPY_MODEL_5334458e2e8f4d8a975409f644564829"
            ],
            "layout": "IPY_MODEL_6a38ec3abfaa46448ab3df6fbcbeef86"
          }
        },
        "a2baad0073fe41f7b394dd191eb016ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74083638268e4d31b45ce70e8432fb68",
            "placeholder": "​",
            "style": "IPY_MODEL_b33424c9b1f04cf4919eb4001c738d40",
            "value": "tokenizer.json: 100%"
          }
        },
        "009fef0472be4b0ab1e12f61ecf72f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53bab7cf2b8643e18818a6b57b1238c5",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ce5b68a94954c20bdf105f1410afd44",
            "value": 466062
          }
        },
        "5334458e2e8f4d8a975409f644564829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8e7b231ea7942da8bc8944e3bf7c924",
            "placeholder": "​",
            "style": "IPY_MODEL_852fec41002544ff99e8d3d194062af3",
            "value": " 466k/466k [00:00&lt;00:00, 31.8MB/s]"
          }
        },
        "6a38ec3abfaa46448ab3df6fbcbeef86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74083638268e4d31b45ce70e8432fb68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b33424c9b1f04cf4919eb4001c738d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53bab7cf2b8643e18818a6b57b1238c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce5b68a94954c20bdf105f1410afd44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8e7b231ea7942da8bc8944e3bf7c924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852fec41002544ff99e8d3d194062af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "331d8e39401744d0b2fd6ea1c281fafa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bab993f2a334e079c27930205fdb0d5",
              "IPY_MODEL_9d74e3dae780484986df9bdaed3d0716",
              "IPY_MODEL_88f778eefd7142bb9eafdfe89632fb02"
            ],
            "layout": "IPY_MODEL_4db5677ece2f4a358f798634e750d049"
          }
        },
        "3bab993f2a334e079c27930205fdb0d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bdf4bfe07e24c7b89e6b9ec6d3d109f",
            "placeholder": "​",
            "style": "IPY_MODEL_4e3da68f42ac40bdb59de8ee7ef2fc25",
            "value": "model.safetensors: 100%"
          }
        },
        "9d74e3dae780484986df9bdaed3d0716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_901d39f2b4154876905fdca3b8b1e846",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5ebd61a7f8740e89c3c0a65dad9b858",
            "value": 440449768
          }
        },
        "88f778eefd7142bb9eafdfe89632fb02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c680ad5b71f0475eacc55acc0988f4bf",
            "placeholder": "​",
            "style": "IPY_MODEL_7c1ce8d3080e4132a03a2a87552ef7ee",
            "value": " 440M/440M [00:01&lt;00:00, 344MB/s]"
          }
        },
        "4db5677ece2f4a358f798634e750d049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bdf4bfe07e24c7b89e6b9ec6d3d109f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e3da68f42ac40bdb59de8ee7ef2fc25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "901d39f2b4154876905fdca3b8b1e846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5ebd61a7f8740e89c3c0a65dad9b858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c680ad5b71f0475eacc55acc0988f4bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c1ce8d3080e4132a03a2a87552ef7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/ADReSSo21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4B7Zj3PzRLv",
        "outputId": "2cfa641e-1a59-468a-d566-e0a2456d4367"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.13.2)\n",
            "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.3 pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-by-Step Audio Transcript Extractor for ADReSSo21 Dataset\n",
        "# This script will:\n",
        "# 1. Mount Google Drive\n",
        "# 2. Extract dataset files\n",
        "# 3. Find all WAV files\n",
        "# 4. Extract transcripts from audio using speech recognition\n",
        "# 5. Save organized transcripts\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import speech_recognition as sr\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ADReSSo21 AUDIO TRANSCRIPT EXTRACTOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: MOUNT GOOGLE DRIVE\n",
        "print(\"\\nSTEP 1: Mounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted successfully!\")\n",
        "except:\n",
        "    print(\"⚠ Not running in Colab or Drive already mounted\")\n",
        "\n",
        "# STEP 2: INSTALL REQUIRED PACKAGES\n",
        "print(\"\\nSTEP 2: Installing required packages...\")\n",
        "print(\"Installing speech recognition and audio processing libraries...\")\n",
        "\n",
        "# Install packages (run once)\n",
        "!pip install SpeechRecognition\n",
        "!pip install pydub\n",
        "!pip install librosa\n",
        "!pip install soundfile\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "print(\"✓ Packages ready (make sure to install them first)\")\n",
        "\n",
        "# STEP 3: SET UP PATHS AND CONFIGURATION\n",
        "print(\"\\nSTEP 3: Setting up paths and configuration...\")\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/Voice/\"\n",
        "EXTRACT_PATH = \"/content/drive/MyDrive/Voice/extracted/\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Voice/transcripts/\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "datasets = {\n",
        "    'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "    'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "    'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "}\n",
        "\n",
        "print(f\"✓ Base path: {BASE_PATH}\")\n",
        "print(f\"✓ Extract path: {EXTRACT_PATH}\")\n",
        "print(f\"✓ Output path: {OUTPUT_PATH}\")\n",
        "\n",
        "# STEP 4: EXTRACT DATASET FILES\n",
        "print(\"\\nSTEP 4: Extracting dataset files...\")\n",
        "\n",
        "def extract_datasets():\n",
        "    \"\"\"Extract all tgz files\"\"\"\n",
        "    for dataset_name, filename in datasets.items():\n",
        "        file_path = os.path.join(BASE_PATH, filename)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"  Extracting {filename}...\")\n",
        "            try:\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=EXTRACT_PATH)\n",
        "                print(f\"  ✓ {filename} extracted successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Error extracting {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"  ⚠ {filename} not found at {file_path}\")\n",
        "\n",
        "extract_datasets()\n",
        "\n",
        "# STEP 5: FIND ALL WAV FILES\n",
        "print(\"\\nSTEP 5: Finding all WAV files...\")\n",
        "\n",
        "def find_wav_files():\n",
        "    \"\"\"Find all WAV files and organize by dataset and label\"\"\"\n",
        "    wav_files = {\n",
        "        'progression_train': {'decline': [], 'no_decline': []},\n",
        "        'progression_test': [],\n",
        "        'diagnosis_train': {'ad': [], 'cn': []}\n",
        "    }\n",
        "\n",
        "    # Progression training files\n",
        "    prog_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/train/audio/\")\n",
        "\n",
        "    # Decline cases\n",
        "    decline_path = os.path.join(prog_train_base, \"decline/\")\n",
        "    if os.path.exists(decline_path):\n",
        "        decline_wavs = [f for f in os.listdir(decline_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_train']['decline'] = [os.path.join(decline_path, f) for f in decline_wavs]\n",
        "        print(f\"  Found {len(decline_wavs)} decline WAV files\")\n",
        "\n",
        "    # No decline cases\n",
        "    no_decline_path = os.path.join(prog_train_base, \"no_decline/\")\n",
        "    if os.path.exists(no_decline_path):\n",
        "        no_decline_wavs = [f for f in os.listdir(no_decline_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_train']['no_decline'] = [os.path.join(no_decline_path, f) for f in no_decline_wavs]\n",
        "        print(f\"  Found {len(no_decline_wavs)} no_decline WAV files\")\n",
        "\n",
        "    # Progression test files\n",
        "    prog_test_path = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/test-dist/audio/\")\n",
        "    if os.path.exists(prog_test_path):\n",
        "        test_wavs = [f for f in os.listdir(prog_test_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_test'] = [os.path.join(prog_test_path, f) for f in test_wavs]\n",
        "        print(f\"  Found {len(test_wavs)} test WAV files\")\n",
        "\n",
        "    # Diagnosis training files\n",
        "    diag_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/diagnosis/train/audio/\")\n",
        "\n",
        "    # AD cases\n",
        "    ad_path = os.path.join(diag_train_base, \"ad/\")\n",
        "    if os.path.exists(ad_path):\n",
        "        ad_wavs = [f for f in os.listdir(ad_path) if f.endswith('.wav')]\n",
        "        wav_files['diagnosis_train']['ad'] = [os.path.join(ad_path, f) for f in ad_wavs]\n",
        "        print(f\"  Found {len(ad_wavs)} AD WAV files\")\n",
        "\n",
        "    # CN cases\n",
        "    cn_path = os.path.join(diag_train_base, \"cn/\")\n",
        "    if os.path.exists(cn_path):\n",
        "        cn_wavs = [f for f in os.listdir(cn_path) if f.endswith('.wav')]\n",
        "        wav_files['diagnosis_train']['cn'] = [os.path.join(cn_path, f) for f in cn_wavs]\n",
        "        print(f\"  Found {len(cn_wavs)} CN WAV files\")\n",
        "\n",
        "    return wav_files\n",
        "\n",
        "wav_files = find_wav_files()\n",
        "\n",
        "# STEP 6: AUDIO PREPROCESSING FUNCTIONS\n",
        "print(\"\\nSTEP 6: Setting up audio preprocessing...\")\n",
        "\n",
        "def preprocess_audio(audio_path, target_sr=16000):\n",
        "    \"\"\"Preprocess audio file for speech recognition\"\"\"\n",
        "    try:\n",
        "        # Load audio with librosa\n",
        "        audio, sr = librosa.load(audio_path, sr=target_sr)\n",
        "\n",
        "        # Normalize audio\n",
        "        audio = librosa.util.normalize(audio)\n",
        "\n",
        "        # Remove silence\n",
        "        audio_trimmed, _ = librosa.effects.trim(audio, top_db=20)\n",
        "\n",
        "        return audio_trimmed, target_sr\n",
        "    except Exception as e:\n",
        "        print(f\"    Error preprocessing {audio_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def convert_to_wav_if_needed(audio_path):\n",
        "    \"\"\"Convert audio to WAV format if needed\"\"\"\n",
        "    try:\n",
        "        if not audio_path.endswith('.wav'):\n",
        "            # Convert using pydub\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "            wav_path = audio_path.rsplit('.', 1)[0] + '_converted.wav'\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "            return wav_path\n",
        "        return audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"    Error converting {audio_path}: {e}\")\n",
        "        return audio_path\n",
        "\n",
        "# STEP 7: SPEECH RECOGNITION FUNCTION\n",
        "print(\"\\nSTEP 7: Setting up speech recognition...\")\n",
        "\n",
        "def extract_transcript_from_audio(audio_path, method='google'):\n",
        "    \"\"\"Extract transcript from audio file using speech recognition\"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    try:\n",
        "        # Convert to WAV if needed\n",
        "        wav_path = convert_to_wav_if_needed(audio_path)\n",
        "\n",
        "        # Preprocess audio\n",
        "        audio_data, sr_rate = preprocess_audio(wav_path, target_sr=16000)\n",
        "\n",
        "        if audio_data is None:\n",
        "            return None, \"Preprocessing failed\"\n",
        "\n",
        "        # Save preprocessed audio temporarily\n",
        "        temp_wav = audio_path.replace('.wav', '_temp.wav')\n",
        "        sf.write(temp_wav, audio_data, sr_rate)\n",
        "\n",
        "        # Use speech recognition\n",
        "        with sr.AudioFile(temp_wav) as source:\n",
        "            # Adjust for ambient noise\n",
        "            recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
        "            audio = recognizer.listen(source)\n",
        "\n",
        "        # Try different recognition methods\n",
        "        transcript = None\n",
        "        error_msg = \"\"\n",
        "\n",
        "        if method == 'google':\n",
        "            try:\n",
        "                transcript = recognizer.recognize_google(audio)\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg = \"Google Speech Recognition could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg = f\"Google Speech Recognition error: {e}\"\n",
        "\n",
        "        # Fallback to other methods if Google fails\n",
        "        if transcript is None:\n",
        "            try:\n",
        "                transcript = recognizer.recognize_sphinx(audio)\n",
        "                method = 'sphinx'\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg += \"; Sphinx could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg += f\"; Sphinx error: {e}\"\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_wav):\n",
        "            os.remove(temp_wav)\n",
        "\n",
        "        if transcript:\n",
        "            return transcript.strip(), method\n",
        "        else:\n",
        "            return None, error_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error processing audio: {str(e)}\"\n",
        "\n",
        "# STEP 8: PROCESS ALL AUDIO FILES AND EXTRACT TRANSCRIPTS\n",
        "print(\"\\nSTEP 8: Processing audio files and extracting transcripts...\")\n",
        "print(\"This may take a while depending on the number and length of audio files...\")\n",
        "\n",
        "def process_audio_files(wav_files):\n",
        "    \"\"\"Process all audio files and extract transcripts\"\"\"\n",
        "    all_transcripts = []\n",
        "\n",
        "    # Process progression training data\n",
        "    print(\"\\n  Processing progression training data...\")\n",
        "    for label in ['decline', 'no_decline']:\n",
        "        files = wav_files['progression_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "            transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'progression_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method_or_error if transcript else None,\n",
        "                'error': None if transcript else method_or_error,\n",
        "                'success': transcript is not None\n",
        "            })\n",
        "\n",
        "    # Process progression test data\n",
        "    print(\"\\n  Processing progression test data...\")\n",
        "    files = wav_files['progression_test']\n",
        "    print(f\"    Processing {len(files)} test files...\")\n",
        "\n",
        "    for i, audio_path in enumerate(files):\n",
        "        print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "        transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "        all_transcripts.append({\n",
        "            'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "            'file_path': audio_path,\n",
        "            'dataset': 'progression_test',\n",
        "            'label': 'test',\n",
        "            'transcript': transcript,\n",
        "            'recognition_method': method_or_error if transcript else None,\n",
        "            'error': None if transcript else method_or_error,\n",
        "            'success': transcript is not None\n",
        "        })\n",
        "\n",
        "    # Process diagnosis training data\n",
        "    print(\"\\n  Processing diagnosis training data...\")\n",
        "    for label in ['ad', 'cn']:\n",
        "        files = wav_files['diagnosis_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "            transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'diagnosis_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method_or_error if transcript else None,\n",
        "                'error': None if transcript else method_or_error,\n",
        "                'success': transcript is not None\n",
        "            })\n",
        "\n",
        "    return all_transcripts\n",
        "\n",
        "# Process all files\n",
        "transcripts = process_audio_files(wav_files)\n",
        "\n",
        "# STEP 9: SAVE RESULTS\n",
        "print(\"\\nSTEP 9: Saving transcription results...\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(transcripts)\n",
        "\n",
        "# Save complete results\n",
        "complete_output = os.path.join(OUTPUT_PATH, \"all_transcripts.csv\")\n",
        "df.to_csv(complete_output, index=False)\n",
        "print(f\"✓ Saved complete results to: {complete_output}\")\n",
        "\n",
        "# Save successful transcripts only\n",
        "successful_df = df[df['success'] == True].copy()\n",
        "success_output = os.path.join(OUTPUT_PATH, \"successful_transcripts.csv\")\n",
        "successful_df.to_csv(success_output, index=False)\n",
        "print(f\"✓ Saved successful transcripts to: {success_output}\")\n",
        "\n",
        "# Save by dataset\n",
        "datasets_to_save = df['dataset'].unique()\n",
        "for dataset in datasets_to_save:\n",
        "    dataset_df = df[df['dataset'] == dataset].copy()\n",
        "    dataset_output = os.path.join(OUTPUT_PATH, f\"{dataset}_transcripts.csv\")\n",
        "    dataset_df.to_csv(dataset_output, index=False)\n",
        "    print(f\"✓ Saved {dataset} transcripts to: {dataset_output}\")\n",
        "\n",
        "# STEP 10: DISPLAY SUMMARY STATISTICS\n",
        "print(\"\\nSTEP 10: Summary Statistics\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "total_files = len(df)\n",
        "successful = len(successful_df)\n",
        "failed = total_files - successful\n",
        "\n",
        "print(f\"Total audio files processed: {total_files}\")\n",
        "print(f\"Successful transcriptions: {successful} ({successful/total_files*100:.1f}%)\")\n",
        "print(f\"Failed transcriptions: {failed} ({failed/total_files*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nDataset breakdown:\")\n",
        "for dataset in df['dataset'].unique():\n",
        "    dataset_total = len(df[df['dataset'] == dataset])\n",
        "    dataset_success = len(df[(df['dataset'] == dataset) & (df['success'] == True)])\n",
        "    print(f\"  {dataset}: {dataset_success}/{dataset_total} successful ({dataset_success/dataset_total*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nLabel distribution (successful transcripts only):\")\n",
        "if not successful_df.empty:\n",
        "    print(successful_df['label'].value_counts())\n",
        "\n",
        "print(f\"\\nRecognition methods used:\")\n",
        "if not successful_df.empty:\n",
        "    print(successful_df['recognition_method'].value_counts())\n",
        "\n",
        "# Show sample transcripts\n",
        "print(f\"\\nSample successful transcripts:\")\n",
        "sample_transcripts = successful_df['transcript'].dropna().head(3)\n",
        "for i, transcript in enumerate(sample_transcripts):\n",
        "    print(f\"  Sample {i+1}: {transcript[:200]}...\")\n",
        "\n",
        "# Show common errors\n",
        "print(f\"\\nMost common errors:\")\n",
        "error_df = df[df['success'] == False]\n",
        "if not error_df.empty:\n",
        "    error_counts = error_df['error'].value_counts().head(5)\n",
        "    for error, count in error_counts.items():\n",
        "        print(f\"  {error}: {count} files\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRANSCRIPT EXTRACTION COMPLETE!\")\n",
        "print(f\"All results saved in: {OUTPUT_PATH}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56RZ8BRdy4pM",
        "outputId": "59419cb2-f3c8-41e2-f915-5a84cd8c4a69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ADReSSo21 AUDIO TRANSCRIPT EXTRACTOR\n",
            "============================================================\n",
            "\n",
            "STEP 1: Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted successfully!\n",
            "\n",
            "STEP 2: Installing required packages...\n",
            "Installing speech recognition and audio processing libraries...\n",
            "✓ Packages ready (make sure to install them first)\n",
            "\n",
            "STEP 3: Setting up paths and configuration...\n",
            "✓ Base path: /content/drive/MyDrive/Voice/\n",
            "✓ Extract path: /content/drive/MyDrive/Voice/extracted/\n",
            "✓ Output path: /content/drive/MyDrive/Voice/transcripts/\n",
            "\n",
            "STEP 4: Extracting dataset files...\n",
            "  Extracting ADReSSo21-progression-train.tgz...\n",
            "  ✓ ADReSSo21-progression-train.tgz extracted successfully\n",
            "  Extracting ADReSSo21-progression-test.tgz...\n",
            "  ✓ ADReSSo21-progression-test.tgz extracted successfully\n",
            "  Extracting ADReSSo21-diagnosis-train.tgz...\n",
            "  ✓ ADReSSo21-diagnosis-train.tgz extracted successfully\n",
            "\n",
            "STEP 5: Finding all WAV files...\n",
            "  Found 15 decline WAV files\n",
            "  Found 58 no_decline WAV files\n",
            "  Found 32 test WAV files\n",
            "  Found 87 AD WAV files\n",
            "  Found 79 CN WAV files\n",
            "\n",
            "STEP 6: Setting up audio preprocessing...\n",
            "\n",
            "STEP 7: Setting up speech recognition...\n",
            "\n",
            "STEP 8: Processing audio files and extracting transcripts...\n",
            "This may take a while depending on the number and length of audio files...\n",
            "\n",
            "  Processing progression training data...\n",
            "    Processing 15 decline files...\n",
            "      Processing 1/15: adrsp055.wav\n",
            "      Processing 2/15: adrsp003.wav\n",
            "      Processing 3/15: adrsp266.wav\n",
            "      Processing 4/15: adrsp300.wav\n",
            "      Processing 5/15: adrsp320.wav\n",
            "      Processing 6/15: adrsp313.wav\n",
            "      Processing 7/15: adrsp179.wav\n",
            "      Processing 8/15: adrsp357.wav\n",
            "      Processing 9/15: adrsp051.wav\n",
            "      Processing 10/15: adrsp101.wav\n",
            "      Processing 11/15: adrsp326.wav\n",
            "      Processing 12/15: adrsp127.wav\n",
            "      Processing 13/15: adrsp276.wav\n",
            "      Processing 14/15: adrsp209.wav\n",
            "      Processing 15/15: adrsp318.wav\n",
            "    Processing 58 no_decline files...\n",
            "      Processing 1/58: adrsp196.wav\n",
            "      Processing 2/58: adrsp137.wav\n",
            "      Processing 3/58: adrsp130.wav\n",
            "      Processing 4/58: adrsp349.wav\n",
            "      Processing 5/58: adrsp198.wav\n",
            "      Processing 6/58: adrsp321.wav\n",
            "      Processing 7/58: adrsp136.wav\n",
            "      Processing 8/58: adrsp024.wav\n",
            "      Processing 9/58: adrsp007.wav\n",
            "      Processing 10/58: adrsp382.wav\n",
            "      Processing 11/58: adrsp043.wav\n",
            "      Processing 12/58: adrsp019.wav\n",
            "      Processing 13/58: adrsp333.wav\n",
            "      Processing 14/58: adrsp056.wav\n",
            "      Processing 15/58: adrsp042.wav\n",
            "      Processing 16/58: adrsp310.wav\n",
            "      Processing 17/58: adrsp377.wav\n",
            "      Processing 18/58: adrsp363.wav\n",
            "      Processing 19/58: adrsp028.wav\n",
            "      Processing 20/58: adrsp350.wav\n",
            "      Processing 21/58: adrsp096.wav\n",
            "      Processing 22/58: adrsp052.wav\n",
            "      Processing 23/58: adrsp204.wav\n",
            "      Processing 24/58: adrsp380.wav\n",
            "      Processing 25/58: adrsp109.wav\n",
            "      Processing 26/58: adrsp255.wav\n",
            "      Processing 27/58: adrsp157.wav\n",
            "      Processing 28/58: adrsp306.wav\n",
            "      Processing 29/58: adrsp197.wav\n",
            "      Processing 30/58: adrsp031.wav\n",
            "      Processing 31/58: adrsp368.wav\n",
            "      Processing 32/58: adrsp032.wav\n",
            "      Processing 33/58: adrsp091.wav\n",
            "      Processing 34/58: adrsp344.wav\n",
            "      Processing 35/58: adrsp124.wav\n",
            "      Processing 36/58: adrsp195.wav\n",
            "      Processing 37/58: adrsp253.wav\n",
            "      Processing 38/58: adrsp251.wav\n",
            "      Processing 39/58: adrsp039.wav\n",
            "      Processing 40/58: adrsp001.wav\n",
            "      Processing 41/58: adrsp041.wav\n",
            "      Processing 42/58: adrsp384.wav\n",
            "      Processing 43/58: adrsp207.wav\n",
            "      Processing 44/58: adrsp379.wav\n",
            "      Processing 45/58: adrsp324.wav\n",
            "      Processing 46/58: adrsp177.wav\n",
            "      Processing 47/58: adrsp148.wav\n",
            "      Processing 48/58: adrsp023.wav\n",
            "      Processing 49/58: adrsp359.wav\n",
            "      Processing 50/58: adrsp122.wav\n",
            "      Processing 51/58: adrsp200.wav\n",
            "      Processing 52/58: adrsp030.wav\n",
            "      Processing 53/58: adrsp319.wav\n",
            "      Processing 54/58: adrsp378.wav\n",
            "      Processing 55/58: adrsp193.wav\n",
            "      Processing 56/58: adrsp128.wav\n",
            "      Processing 57/58: adrsp161.wav\n",
            "      Processing 58/58: adrsp192.wav\n",
            "\n",
            "  Processing progression test data...\n",
            "    Processing 32 test files...\n",
            "      Processing 1/32: adrspt20.wav\n",
            "      Processing 2/32: adrspt15.wav\n",
            "      Processing 3/32: adrspt4.wav\n",
            "      Processing 4/32: adrspt28.wav\n",
            "      Processing 5/32: adrspt16.wav\n",
            "      Processing 6/32: adrspt27.wav\n",
            "      Processing 7/32: adrspt9.wav\n",
            "      Processing 8/32: adrspt10.wav\n",
            "      Processing 9/32: adrspt13.wav\n",
            "      Processing 10/32: adrspt26.wav\n",
            "      Processing 11/32: adrspt23.wav\n",
            "      Processing 12/32: adrspt31.wav\n",
            "      Processing 13/32: adrspt14.wav\n",
            "      Processing 14/32: adrspt6.wav\n",
            "      Processing 15/32: adrspt12.wav\n",
            "      Processing 16/32: adrspt32.wav\n",
            "      Processing 17/32: adrspt21.wav\n",
            "      Processing 18/32: adrspt1.wav\n",
            "      Processing 19/32: adrspt29.wav\n",
            "      Processing 20/32: adrspt30.wav\n",
            "      Processing 21/32: adrspt3.wav\n",
            "      Processing 22/32: adrspt8.wav\n",
            "      Processing 23/32: adrspt19.wav\n",
            "      Processing 24/32: adrspt18.wav\n",
            "      Processing 25/32: adrspt25.wav\n",
            "      Processing 26/32: adrspt2.wav\n",
            "      Processing 27/32: adrspt24.wav\n",
            "      Processing 28/32: adrspt11.wav\n",
            "      Processing 29/32: adrspt17.wav\n",
            "      Processing 30/32: adrspt22.wav\n",
            "      Processing 31/32: adrspt7.wav\n",
            "      Processing 32/32: adrspt5.wav\n",
            "\n",
            "  Processing diagnosis training data...\n",
            "    Processing 87 ad files...\n",
            "      Processing 1/87: adrso047.wav\n",
            "      Processing 2/87: adrso128.wav\n",
            "      Processing 3/87: adrso045.wav\n",
            "      Processing 4/87: adrso110.wav\n",
            "      Processing 5/87: adrso036.wav\n",
            "      Processing 6/87: adrso189.wav\n",
            "      Processing 7/87: adrso093.wav\n",
            "      Processing 8/87: adrso112.wav\n",
            "      Processing 9/87: adrso205.wav\n",
            "      Processing 10/87: adrso089.wav\n",
            "      Processing 11/87: adrso060.wav\n",
            "      Processing 12/87: adrso232.wav\n",
            "      Processing 13/87: adrso075.wav\n",
            "      Processing 14/87: adrso063.wav\n",
            "      Processing 15/87: adrso106.wav\n",
            "      Processing 16/87: adrso202.wav\n",
            "      Processing 17/87: adrso043.wav\n",
            "      Processing 18/87: adrso206.wav\n",
            "      Processing 19/87: adrso039.wav\n",
            "      Processing 20/87: adrso109.wav\n",
            "      Processing 21/87: adrso126.wav\n",
            "      Processing 22/87: adrso071.wav\n",
            "      Processing 23/87: adrso209.wav\n",
            "      Processing 24/87: adrso244.wav\n",
            "      Processing 25/87: adrso228.wav\n",
            "      Processing 26/87: adrso122.wav\n",
            "      Processing 27/87: adrso116.wav\n",
            "      Processing 28/87: adrso141.wav\n",
            "      Processing 29/87: adrso130.wav\n",
            "      Processing 30/87: adrso248.wav\n",
            "      Processing 31/87: adrso070.wav\n",
            "      Processing 32/87: adrso055.wav\n",
            "      Processing 33/87: adrso222.wav\n",
            "      Processing 34/87: adrso190.wav\n",
            "      Processing 35/87: adrso215.wav\n",
            "      Processing 36/87: adrso223.wav\n",
            "      Processing 37/87: adrso192.wav\n",
            "      Processing 38/87: adrso236.wav\n",
            "      Processing 39/87: adrso234.wav\n",
            "      Processing 40/87: adrso059.wav\n",
            "      Processing 41/87: adrso098.wav\n",
            "      Processing 42/87: adrso090.wav\n",
            "      Processing 43/87: adrso250.wav\n",
            "      Processing 44/87: adrso025.wav\n",
            "      Processing 45/87: adrso031.wav\n",
            "      Processing 46/87: adrso197.wav\n",
            "      Processing 47/87: adrso224.wav\n",
            "      Processing 48/87: adrso074.wav\n",
            "      Processing 49/87: adrso049.wav\n",
            "      Processing 50/87: adrso211.wav\n",
            "      Processing 51/87: adrso229.wav\n",
            "      Processing 52/87: adrso138.wav\n",
            "      Processing 53/87: adrso123.wav\n",
            "      Processing 54/87: adrso027.wav\n",
            "      Processing 55/87: adrso072.wav\n",
            "      Processing 56/87: adrso056.wav\n",
            "      Processing 57/87: adrso068.wav\n",
            "      Processing 58/87: adrso054.wav\n",
            "      Processing 59/87: adrso187.wav\n",
            "      Processing 60/87: adrso078.wav\n",
            "      Processing 61/87: adrso053.wav\n",
            "      Processing 62/87: adrso200.wav\n",
            "      Processing 63/87: adrso249.wav\n",
            "      Processing 64/87: adrso028.wav\n",
            "      Processing 65/87: adrso245.wav\n",
            "      Processing 66/87: adrso216.wav\n",
            "      Processing 67/87: adrso092.wav\n",
            "      Processing 68/87: adrso220.wav\n",
            "      Processing 69/87: adrso134.wav\n",
            "      Processing 70/87: adrso142.wav\n",
            "      Processing 71/87: adrso198.wav\n",
            "      Processing 72/87: adrso077.wav\n",
            "      Processing 73/87: adrso024.wav\n",
            "      Processing 74/87: adrso212.wav\n",
            "      Processing 75/87: adrso046.wav\n",
            "      Processing 76/87: adrso035.wav\n",
            "      Processing 77/87: adrso233.wav\n",
            "      Processing 78/87: adrso247.wav\n",
            "      Processing 79/87: adrso033.wav\n",
            "      Processing 80/87: adrso125.wav\n",
            "      Processing 81/87: adrso188.wav\n",
            "      Processing 82/87: adrso237.wav\n",
            "      Processing 83/87: adrso032.wav\n",
            "      Processing 84/87: adrso253.wav\n",
            "      Processing 85/87: adrso218.wav\n",
            "      Processing 86/87: adrso144.wav\n",
            "      Processing 87/87: adrso246.wav\n",
            "    Processing 79 cn files...\n",
            "      Processing 1/79: adrso173.wav\n",
            "      Processing 2/79: adrso015.wav\n",
            "      Processing 3/79: adrso307.wav\n",
            "      Processing 4/79: adrso283.wav\n",
            "      Processing 5/79: adrso167.wav\n",
            "      Processing 6/79: adrso168.wav\n",
            "      Processing 7/79: adrso172.wav\n",
            "      Processing 8/79: adrso292.wav\n",
            "      Processing 9/79: adrso316.wav\n",
            "      Processing 10/79: adrso162.wav\n",
            "      Processing 11/79: adrso296.wav\n",
            "      Processing 12/79: adrso278.wav\n",
            "      Processing 13/79: adrso300.wav\n",
            "      Processing 14/79: adrso291.wav\n",
            "      Processing 15/79: adrso169.wav\n",
            "      Processing 16/79: adrso178.wav\n",
            "      Processing 17/79: adrso165.wav\n",
            "      Processing 18/79: adrso262.wav\n",
            "      Processing 19/79: adrso177.wav\n",
            "      Processing 20/79: adrso265.wav\n",
            "      Processing 21/79: adrso014.wav\n",
            "      Processing 22/79: adrso261.wav\n",
            "      Processing 23/79: adrso268.wav\n",
            "      Processing 24/79: adrso021.wav\n",
            "      Processing 25/79: adrso156.wav\n",
            "      Processing 26/79: adrso310.wav\n",
            "      Processing 27/79: adrso016.wav\n",
            "      Processing 28/79: adrso148.wav\n",
            "      Processing 29/79: adrso302.wav\n",
            "      Processing 30/79: adrso308.wav\n",
            "      Processing 31/79: adrso018.wav\n",
            "      Processing 32/79: adrso309.wav\n",
            "      Processing 33/79: adrso180.wav\n",
            "      Processing 34/79: adrso298.wav\n",
            "      Processing 35/79: adrso154.wav\n",
            "      Processing 36/79: adrso273.wav\n",
            "      Processing 37/79: adrso259.wav\n",
            "      Processing 38/79: adrso151.wav\n",
            "      Processing 39/79: adrso159.wav\n",
            "      Processing 40/79: adrso267.wav\n",
            "      Processing 41/79: adrso274.wav\n",
            "      Processing 42/79: adrso019.wav\n",
            "      Processing 43/79: adrso153.wav\n",
            "      Processing 44/79: adrso023.wav\n",
            "      Processing 45/79: adrso012.wav\n",
            "      Processing 46/79: adrso280.wav\n",
            "      Processing 47/79: adrso002.wav\n",
            "      Processing 48/79: adrso266.wav\n",
            "      Processing 49/79: adrso022.wav\n",
            "      Processing 50/79: adrso007.wav\n",
            "      Processing 51/79: adrso152.wav\n",
            "      Processing 52/79: adrso276.wav\n",
            "      Processing 53/79: adrso260.wav\n",
            "      Processing 54/79: adrso005.wav\n",
            "      Processing 55/79: adrso017.wav\n",
            "      Processing 56/79: adrso299.wav\n",
            "      Processing 57/79: adrso157.wav\n",
            "      Processing 58/79: adrso182.wav\n",
            "      Processing 59/79: adrso008.wav\n",
            "      Processing 60/79: adrso161.wav\n",
            "      Processing 61/79: adrso263.wav\n",
            "      Processing 62/79: adrso257.wav\n",
            "      Processing 63/79: adrso164.wav\n",
            "      Processing 64/79: adrso270.wav\n",
            "      Processing 65/79: adrso289.wav\n",
            "      Processing 66/79: adrso264.wav\n",
            "      Processing 67/79: adrso277.wav\n",
            "      Processing 68/79: adrso160.wav\n",
            "      Processing 69/79: adrso286.wav\n",
            "      Processing 70/79: adrso003.wav\n",
            "      Processing 71/79: adrso186.wav\n",
            "      Processing 72/79: adrso285.wav\n",
            "      Processing 73/79: adrso170.wav\n",
            "      Processing 74/79: adrso183.wav\n",
            "      Processing 75/79: adrso281.wav\n",
            "      Processing 76/79: adrso315.wav\n",
            "      Processing 77/79: adrso010.wav\n",
            "      Processing 78/79: adrso312.wav\n",
            "      Processing 79/79: adrso158.wav\n",
            "\n",
            "STEP 9: Saving transcription results...\n",
            "✓ Saved complete results to: /content/drive/MyDrive/Voice/transcripts/all_transcripts.csv\n",
            "✓ Saved successful transcripts to: /content/drive/MyDrive/Voice/transcripts/successful_transcripts.csv\n",
            "✓ Saved progression_train transcripts to: /content/drive/MyDrive/Voice/transcripts/progression_train_transcripts.csv\n",
            "✓ Saved progression_test transcripts to: /content/drive/MyDrive/Voice/transcripts/progression_test_transcripts.csv\n",
            "✓ Saved diagnosis_train transcripts to: /content/drive/MyDrive/Voice/transcripts/diagnosis_train_transcripts.csv\n",
            "\n",
            "STEP 10: Summary Statistics\n",
            "==================================================\n",
            "Total audio files processed: 271\n",
            "Successful transcriptions: 155 (57.2%)\n",
            "Failed transcriptions: 116 (42.8%)\n",
            "\n",
            "Dataset breakdown:\n",
            "  progression_train: 42/73 successful (57.5%)\n",
            "  progression_test: 16/32 successful (50.0%)\n",
            "  diagnosis_train: 97/166 successful (58.4%)\n",
            "\n",
            "Label distribution (successful transcripts only):\n",
            "label\n",
            "ad            51\n",
            "cn            46\n",
            "no_decline    34\n",
            "test          16\n",
            "decline        8\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Recognition methods used:\n",
            "recognition_method\n",
            "google    155\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample successful transcripts:\n",
            "  Sample 1: you can start now...\n",
            "  Sample 2: is in 1 minute time I want you to name as many...\n",
            "  Sample 3: cat dog giraffe...\n",
            "\n",
            "Most common errors:\n",
            "  Google Speech Recognition could not understand audio; Sphinx error: missing PocketSphinx module: ensure that PocketSphinx is set up correctly.: 116 files\n",
            "\n",
            "============================================================\n",
            "TRANSCRIPT EXTRACTION COMPLETE!\n",
            "All results saved in: /content/drive/MyDrive/Voice/transcripts/\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Audio-Text AD Classification Pipeline\n",
        "# This script combines audio feature extraction, BERT processing, and DARTS classification\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For BERT processing\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE AD CLASSIFICATION PIPELINE\")\n",
        "print(\"Audio Features + BERT + DARTS Architecture\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuuPR--FzKPd",
        "outputId": "037d4448-81b7-4c6a-cdfa-6ed596332a90"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "COMPREHENSIVE AD CLASSIFICATION PIPELINE\n",
            "Audio Features + BERT + DARTS Architecture\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 1: ADVANCED AUDIO FEATURE EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "class AudioFeatureExtractor:\n",
        "    def __init__(self, sr=16000, n_mfcc=13, n_fft=2048, hop_length=512):\n",
        "        self.sr = sr\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "    def extract_mfcc_features(self, audio_path):\n",
        "        \"\"\"Extract comprehensive MFCC features including deltas\"\"\"\n",
        "        try:\n",
        "            # Load audio\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            # Extract MFCC features\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc,\n",
        "                                       n_fft=self.n_fft, hop_length=self.hop_length)\n",
        "\n",
        "            # Extract delta features (first derivative)\n",
        "            delta_mfccs = librosa.feature.delta(mfccs)\n",
        "\n",
        "            # Extract delta-delta features (second derivative)\n",
        "            delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
        "\n",
        "            # Combine all MFCC features\n",
        "            combined_mfccs = np.concatenate([mfccs, delta_mfccs, delta2_mfccs], axis=0)\n",
        "\n",
        "            # Statistical features for each coefficient\n",
        "            features = {}\n",
        "\n",
        "            # Mean, std, min, max for each coefficient\n",
        "            features['mfcc_mean'] = np.mean(combined_mfccs, axis=1)\n",
        "            features['mfcc_std'] = np.std(combined_mfccs, axis=1)\n",
        "            features['mfcc_min'] = np.min(combined_mfccs, axis=1)\n",
        "            features['mfcc_max'] = np.max(combined_mfccs, axis=1)\n",
        "            features['mfcc_median'] = np.median(combined_mfccs, axis=1)\n",
        "            features['mfcc_skew'] = self._calculate_skewness(combined_mfccs)\n",
        "            features['mfcc_kurtosis'] = self._calculate_kurtosis(combined_mfccs)\n",
        "\n",
        "            return features, combined_mfccs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting MFCC from {audio_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def extract_spectral_features(self, audio_path):\n",
        "        \"\"\"Extract spectral features\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            features = {}\n",
        "\n",
        "            # Spectral centroid\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "            features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
        "            features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
        "\n",
        "            # Spectral bandwidth\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
        "            features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
        "            features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
        "\n",
        "            # Spectral rolloff\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "            features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
        "            features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
        "\n",
        "            # Zero crossing rate\n",
        "            zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "            features['zcr_mean'] = np.mean(zcr)\n",
        "            features['zcr_std'] = np.std(zcr)\n",
        "\n",
        "            # Chroma features\n",
        "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            features['chroma_mean'] = np.mean(chroma, axis=1)\n",
        "            features['chroma_std'] = np.std(chroma, axis=1)\n",
        "\n",
        "            # Tempo\n",
        "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "            features['tempo'] = tempo\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting spectral features from {audio_path}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def extract_prosodic_features(self, audio_path):\n",
        "        \"\"\"Extract prosodic features (pitch, energy, etc.)\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            features = {}\n",
        "\n",
        "            # Fundamental frequency (pitch)\n",
        "            f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'),\n",
        "                                                       fmax=librosa.note_to_hz('C7'))\n",
        "\n",
        "            # Remove NaN values\n",
        "            f0_clean = f0[~np.isnan(f0)]\n",
        "            if len(f0_clean) > 0:\n",
        "                features['f0_mean'] = np.mean(f0_clean)\n",
        "                features['f0_std'] = np.std(f0_clean)\n",
        "                features['f0_min'] = np.min(f0_clean)\n",
        "                features['f0_max'] = np.max(f0_clean)\n",
        "                features['f0_range'] = np.max(f0_clean) - np.min(f0_clean)\n",
        "            else:\n",
        "                features.update({\n",
        "                    'f0_mean': 0, 'f0_std': 0, 'f0_min': 0,\n",
        "                    'f0_max': 0, 'f0_range': 0\n",
        "                })\n",
        "\n",
        "            # RMS energy\n",
        "            rms = librosa.feature.rms(y=y)[0]\n",
        "            features['rms_mean'] = np.mean(rms)\n",
        "            features['rms_std'] = np.std(rms)\n",
        "\n",
        "            # Spectral contrast\n",
        "            contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "            features['contrast_mean'] = np.mean(contrast, axis=1)\n",
        "            features['contrast_std'] = np.std(contrast, axis=1)\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting prosodic features from {audio_path}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _calculate_skewness(self, data):\n",
        "        \"\"\"Calculate skewness for each row\"\"\"\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        std[std == 0] = 1  # Avoid division by zero\n",
        "        normalized = (data - mean) / std\n",
        "        skewness = np.mean(normalized**3, axis=1)\n",
        "        return skewness\n",
        "\n",
        "    def _calculate_kurtosis(self, data):\n",
        "        \"\"\"Calculate kurtosis for each row\"\"\"\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        std[std == 0] = 1  # Avoid division by zero\n",
        "        normalized = (data - mean) / std\n",
        "        kurtosis = np.mean(normalized**4, axis=1) - 3\n",
        "        return kurtosis\n",
        "\n",
        "    def extract_all_features(self, audio_path):\n",
        "        \"\"\"Extract all audio features\"\"\"\n",
        "        all_features = {}\n",
        "\n",
        "        # MFCC features\n",
        "        mfcc_features, mfcc_matrix = self.extract_mfcc_features(audio_path)\n",
        "        if mfcc_features:\n",
        "            all_features.update(mfcc_features)\n",
        "\n",
        "        # Spectral features\n",
        "        spectral_features = self.extract_spectral_features(audio_path)\n",
        "        all_features.update(spectral_features)\n",
        "\n",
        "        # Prosodic features\n",
        "        prosodic_features = self.extract_prosodic_features(audio_path)\n",
        "        all_features.update(prosodic_features)\n",
        "\n",
        "        # Flatten nested arrays\n",
        "        flattened_features = {}\n",
        "        for key, value in all_features.items():\n",
        "            if isinstance(value, np.ndarray):\n",
        "                if value.ndim == 1:\n",
        "                    for i, v in enumerate(value):\n",
        "                        flattened_features[f\"{key}_{i}\"] = v\n",
        "                else:\n",
        "                    flattened_features[key] = np.mean(value)\n",
        "            else:\n",
        "                flattened_features[key] = value\n",
        "\n",
        "        return flattened_features, mfcc_matrix\n"
      ],
      "metadata": {
        "id": "2bTxcZLizRWu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# PART 2: BERT TEXT PROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class BERTTextProcessor:\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        if pd.isna(text) or text is None:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to string and clean\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove special characters but keep spaces and basic punctuation\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def extract_bert_features(self, text):\n",
        "        \"\"\"Extract BERT embeddings from text\"\"\"\n",
        "        try:\n",
        "            # Preprocess text\n",
        "            clean_text = self.preprocess_text(text)\n",
        "\n",
        "            if not clean_text:\n",
        "                # Return zero vector for empty text\n",
        "                return np.zeros(768)\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                clean_text,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "                # Use [CLS] token embedding (first token)\n",
        "                cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
        "\n",
        "                # Also compute mean pooling of all tokens\n",
        "                attention_mask = inputs['attention_mask']\n",
        "                token_embeddings = outputs.last_hidden_state\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "                mean_embedding = sum_embeddings / sum_mask\n",
        "                mean_embedding = mean_embedding.squeeze()\n",
        "\n",
        "                # Combine CLS and mean pooling\n",
        "                combined_embedding = (cls_embedding + mean_embedding) / 2\n",
        "\n",
        "                return combined_embedding.numpy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting BERT features: {e}\")\n",
        "            return np.zeros(768)\n",
        "\n",
        "    def extract_linguistic_features(self, text):\n",
        "        \"\"\"Extract basic linguistic features\"\"\"\n",
        "        try:\n",
        "            clean_text = self.preprocess_text(text)\n",
        "\n",
        "            if not clean_text:\n",
        "                return {\n",
        "                    'word_count': 0, 'char_count': 0, 'avg_word_length': 0,\n",
        "                    'sentence_count': 0, 'question_count': 0, 'exclamation_count': 0\n",
        "                }\n",
        "\n",
        "            words = clean_text.split()\n",
        "            sentences = clean_text.split('.')\n",
        "\n",
        "            features = {\n",
        "                'word_count': len(words),\n",
        "                'char_count': len(clean_text),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'question_count': clean_text.count('?'),\n",
        "                'exclamation_count': clean_text.count('!')\n",
        "            }\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting linguistic features: {e}\")\n",
        "            return {'word_count': 0, 'char_count': 0, 'avg_word_length': 0,\n",
        "                   'sentence_count': 0, 'question_count': 0, 'exclamation_count': 0}\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRNEytjwzVGs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 3: IMPROVED DARTS ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class ImprovedDARTSCell(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_ops=8):\n",
        "        super(ImprovedDARTSCell, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Ensure dimensions match for operations\n",
        "        if input_dim != output_dim:\n",
        "            self.projection = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            self.projection = nn.Identity()\n",
        "\n",
        "        # Define possible operations with proper dimensionality\n",
        "        self.operations = nn.ModuleList([\n",
        "            nn.Identity(),  # Skip connection\n",
        "            nn.ReLU(),      # Activation\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU()),  # Linear + ReLU\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.Tanh()),  # Linear + Tanh\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU(), nn.Dropout(0.1)),  # With dropout\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim // 2), nn.ReLU(), nn.Linear(output_dim // 2, output_dim)),  # Bottleneck\n",
        "            nn.Sequential(nn.LayerNorm(output_dim), nn.ReLU()),  # Layer norm + activation\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU(), nn.Linear(output_dim, output_dim))  # Deep linear\n",
        "        ])\n",
        "\n",
        "        # Architecture parameters (alpha) - learnable weights for each operation\n",
        "        self.alpha = nn.Parameter(torch.randn(len(self.operations)))\n",
        "\n",
        "        # Temperature parameter for gumbel softmax (learnable)\n",
        "        self.temperature = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project input to correct dimension\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Apply Gumbel Softmax for differentiable architecture search\n",
        "        if self.training:\n",
        "            # Use Gumbel Softmax during training\n",
        "            gumbel_weights = F.gumbel_softmax(self.alpha, tau=self.temperature, hard=False)\n",
        "        else:\n",
        "            # Use regular softmax during evaluation\n",
        "            gumbel_weights = F.softmax(self.alpha / self.temperature, dim=0)\n",
        "\n",
        "        # Apply operations\n",
        "        outputs = []\n",
        "        for op in self.operations:\n",
        "            try:\n",
        "                out = op(x)\n",
        "                outputs.append(out)\n",
        "            except Exception as e:\n",
        "                # Fallback to identity if operation fails\n",
        "                outputs.append(x)\n",
        "\n",
        "        # Weighted combination of all operations\n",
        "        result = sum(w * out for w, out in zip(gumbel_weights, outputs))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_selected_operation(self):\n",
        "        \"\"\"Get the operation with highest weight (for inference)\"\"\"\n",
        "        selected_idx = torch.argmax(self.alpha).item()\n",
        "        return selected_idx, self.operations[selected_idx]\n",
        "\n",
        "class MultimodalDARTSClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=256, num_classes=2):\n",
        "        super(MultimodalDARTSClassifier, self).__init__()\n",
        "\n",
        "        # Audio processing branch with DARTS\n",
        "        self.audio_projection = nn.Sequential(\n",
        "            nn.Linear(audio_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.audio_darts_cells = nn.ModuleList([\n",
        "            ImprovedDARTSCell(hidden_dim, hidden_dim) for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        # Text processing branch with DARTS\n",
        "        self.text_projection = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.text_darts_cells = nn.ModuleList([\n",
        "            ImprovedDARTSCell(hidden_dim, hidden_dim) for _ in range(2)\n",
        "        ])\n",
        "\n",
        "        # Cross-modal attention\n",
        "        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
        "\n",
        "        # Fusion and classification\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LayerNorm(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        # Process audio through DARTS\n",
        "        audio_x = self.audio_projection(audio_features)\n",
        "        for cell in self.audio_darts_cells:\n",
        "            residual = audio_x\n",
        "            audio_x = cell(audio_x)\n",
        "            audio_x = audio_x + residual  # Residual connection\n",
        "\n",
        "        # Process text through DARTS\n",
        "        text_x = self.text_projection(text_features)\n",
        "        for cell in self.text_darts_cells:\n",
        "            residual = text_x\n",
        "            text_x = cell(text_x)\n",
        "            text_x = text_x + residual  # Residual connection\n",
        "\n",
        "        # Cross-modal attention\n",
        "        audio_attended, _ = self.cross_attention(\n",
        "            audio_x.unsqueeze(1), text_x.unsqueeze(1), text_x.unsqueeze(1)\n",
        "        )\n",
        "        audio_attended = audio_attended.squeeze(1)\n",
        "\n",
        "        # Fusion\n",
        "        fused = torch.cat([audio_attended, text_x], dim=1)\n",
        "        fused = self.fusion(fused)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(fused)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_architecture_info(self):\n",
        "        \"\"\"Get information about the learned architecture\"\"\"\n",
        "        arch_info = {}\n",
        "\n",
        "        # Audio DARTS info\n",
        "        for i, cell in enumerate(self.audio_darts_cells):\n",
        "            selected_idx, _ = cell.get_selected_operation()\n",
        "            arch_info[f'audio_cell_{i}'] = {\n",
        "                'selected_operation_idx': selected_idx,\n",
        "                'operation_weights': cell.alpha.detach().cpu().numpy(),\n",
        "                'temperature': cell.temperature.item()\n",
        "            }\n",
        "\n",
        "        # Text DARTS info\n",
        "        for i, cell in enumerate(self.text_darts_cells):\n",
        "            selected_idx, _ = cell.get_selected_operation()\n",
        "            arch_info[f'text_cell_{i}'] = {\n",
        "                'selected_operation_idx': selected_idx,\n",
        "                'operation_weights': cell.alpha.detach().cpu().numpy(),\n",
        "                'temperature': cell.temperature.item()\n",
        "            }\n",
        "\n",
        "        return arch_info\n"
      ],
      "metadata": {
        "id": "xlcNZpa1zZVJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 4: DATASET AND TRAINING UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ADDataset(Dataset):\n",
        "    def __init__(self, csv_path=None, audio_features=None, text_features=None, labels=None,\n",
        "                 dataset_type='diagnosis', normalize_audio=True, normalize_text=True):\n",
        "        \"\"\"\n",
        "        Enhanced AD Dataset class for multimodal classification\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to CSV file with transcripts (from transcript extraction)\n",
        "            audio_features: Pre-extracted audio features array\n",
        "            text_features: Pre-extracted text features array\n",
        "            labels: Labels array\n",
        "            dataset_type: 'diagnosis' (AD/CN) or 'progression' (decline/no_decline)\n",
        "            normalize_audio: Whether to normalize audio features\n",
        "            normalize_text: Whether to normalize text features\n",
        "        \"\"\"\n",
        "        self.dataset_type = dataset_type\n",
        "        self.normalize_audio = normalize_audio\n",
        "        self.normalize_text = normalize_text\n",
        "\n",
        "        # Initialize scalers\n",
        "        self.audio_scaler = StandardScaler() if normalize_audio else None\n",
        "        self.text_scaler = StandardScaler() if normalize_text else None\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        if csv_path is not None:\n",
        "            # Load from CSV file (transcript extraction results)\n",
        "            self._load_from_csv(csv_path)\n",
        "        else:\n",
        "            # Load from pre-processed arrays\n",
        "            self._load_from_arrays(audio_features, text_features, labels)\n",
        "\n",
        "    def _load_from_csv(self, csv_path):\n",
        "        \"\"\"Load dataset from CSV file with transcripts\"\"\"\n",
        "        print(f\"Loading dataset from {csv_path}...\")\n",
        "\n",
        "        # Load CSV\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Filter successful transcripts only\n",
        "        df = df[df['success'] == True].copy()\n",
        "        print(f\"Found {len(df)} successful transcripts\")\n",
        "\n",
        "        # Filter by dataset type if specified\n",
        "        if self.dataset_type == 'diagnosis':\n",
        "            df = df[df['dataset'] == 'diagnosis_train'].copy()\n",
        "            valid_labels = ['ad', 'cn']\n",
        "        elif self.dataset_type == 'progression':\n",
        "            df = df[df['dataset'] == 'progression_train'].copy()\n",
        "            valid_labels = ['decline', 'no_decline']\n",
        "        else:\n",
        "            # Keep all data\n",
        "            valid_labels = df['label'].unique()\n",
        "\n",
        "        # Filter valid labels\n",
        "        df = df[df['label'].isin(valid_labels)].copy()\n",
        "        print(f\"After filtering: {len(df)} samples\")\n",
        "\n",
        "        # Extract information\n",
        "        self.file_ids = df['file_id'].tolist()\n",
        "        self.file_paths = df['file_path'].tolist()\n",
        "        self.transcripts = df['transcript'].tolist()\n",
        "        self.raw_labels = df['label'].tolist()\n",
        "\n",
        "        # Encode labels\n",
        "        self.labels = torch.LongTensor(self.label_encoder.fit_transform(self.raw_labels))\n",
        "        self.label_mapping = dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))\n",
        "\n",
        "        print(f\"Label mapping: {self.label_mapping}\")\n",
        "        print(f\"Label distribution: {pd.Series(self.raw_labels).value_counts().to_dict()}\")\n",
        "\n",
        "        # Initialize feature placeholders (will be filled by feature extractors)\n",
        "        self.audio_features = None\n",
        "        self.text_features = None\n",
        "\n",
        "    def _load_from_arrays(self, audio_features, text_features, labels):\n",
        "        \"\"\"Load dataset from pre-processed feature arrays\"\"\"\n",
        "        print(\"Loading dataset from pre-processed arrays...\")\n",
        "\n",
        "        if audio_features is None or text_features is None or labels is None:\n",
        "            raise ValueError(\"All feature arrays must be provided\")\n",
        "\n",
        "        # Convert to numpy arrays if needed\n",
        "        audio_features = np.array(audio_features)\n",
        "        text_features = np.array(text_features)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Ensure same number of samples\n",
        "        assert len(audio_features) == len(text_features) == len(labels), \\\n",
        "            \"All arrays must have the same number of samples\"\n",
        "\n",
        "        # Normalize features if requested\n",
        "        if self.normalize_audio and self.audio_scaler:\n",
        "            audio_features = self.audio_scaler.fit_transform(audio_features)\n",
        "\n",
        "        if self.normalize_text and self.text_scaler:\n",
        "            text_features = self.text_scaler.fit_transform(text_features)\n",
        "\n",
        "        # Convert to tensors\n",
        "        self.audio_features = torch.FloatTensor(audio_features)\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "\n",
        "        # Handle labels\n",
        "        if labels.dtype == 'object' or isinstance(labels[0], str):\n",
        "            # String labels - encode them\n",
        "            self.raw_labels = labels.tolist()\n",
        "            self.labels = torch.LongTensor(self.label_encoder.fit_transform(labels))\n",
        "            self.label_mapping = dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))\n",
        "        else:\n",
        "            # Numeric labels\n",
        "            self.labels = torch.LongTensor(labels)\n",
        "            self.raw_labels = labels.tolist()\n",
        "            self.label_mapping = {i: i for i in range(len(np.unique(labels)))}\n",
        "\n",
        "        print(f\"Dataset loaded: {len(self.labels)} samples\")\n",
        "        print(f\"Audio features shape: {self.audio_features.shape}\")\n",
        "        print(f\"Text features shape: {self.text_features.shape}\")\n",
        "        print(f\"Label mapping: {self.label_mapping}\")\n",
        "\n",
        "    def set_audio_features(self, audio_features):\n",
        "        \"\"\"Set audio features after extraction\"\"\"\n",
        "        audio_features = np.array(audio_features)\n",
        "\n",
        "        if self.normalize_audio and self.audio_scaler:\n",
        "            audio_features = self.audio_scaler.fit_transform(audio_features)\n",
        "\n",
        "        self.audio_features = torch.FloatTensor(audio_features)\n",
        "        print(f\"Audio features set: {self.audio_features.shape}\")\n",
        "\n",
        "    def set_text_features(self, text_features):\n",
        "        \"\"\"Set text features after extraction\"\"\"\n",
        "        text_features = np.array(text_features)\n",
        "\n",
        "        if self.normalize_text and self.text_scaler:\n",
        "            text_features = self.text_scaler.fit_transform(text_features)\n",
        "\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "        print(f\"Text features set: {self.text_features.shape}\")\n",
        "\n",
        "    def extract_features_from_transcripts(self, audio_extractor, text_processor):\n",
        "        \"\"\"\n",
        "        Extract features from audio files and transcripts\n",
        "\n",
        "        Args:\n",
        "            audio_extractor: AudioFeatureExtractor instance\n",
        "            text_processor: BERTTextProcessor instance\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'file_paths') or not hasattr(self, 'transcripts'):\n",
        "            raise ValueError(\"Dataset must be loaded from CSV to extract features\")\n",
        "\n",
        "        print(\"Extracting audio and text features...\")\n",
        "\n",
        "        # Extract audio features\n",
        "        print(\"Extracting audio features...\")\n",
        "        audio_features_list = []\n",
        "        failed_audio = 0\n",
        "\n",
        "        for i, audio_path in enumerate(self.file_paths):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"  Processing audio {i+1}/{len(self.file_paths)}\")\n",
        "\n",
        "            features, _ = audio_extractor.extract_all_features(audio_path)\n",
        "            if features:\n",
        "                # Convert to list of values in consistent order\n",
        "                feature_vector = [features.get(key, 0) for key in sorted(features.keys())]\n",
        "                audio_features_list.append(feature_vector)\n",
        "            else:\n",
        "                # Use zero vector for failed extractions\n",
        "                failed_audio += 1\n",
        "                if audio_features_list:\n",
        "                    audio_features_list.append([0] * len(audio_features_list[0]))\n",
        "                else:\n",
        "                    audio_features_list.append([0] * 100)  # Default size\n",
        "\n",
        "        if failed_audio > 0:\n",
        "            print(f\"  Warning: {failed_audio} audio files failed feature extraction\")\n",
        "\n",
        "        # Extract text features\n",
        "        print(\"Extracting text features...\")\n",
        "        text_features_list = []\n",
        "\n",
        "        for i, transcript in enumerate(self.transcripts):\n",
        "            if i % 20 == 0:\n",
        "                print(f\"  Processing text {i+1}/{len(self.transcripts)}\")\n",
        "\n",
        "            # Extract BERT features\n",
        "            bert_features = text_processor.extract_bert_features(transcript)\n",
        "\n",
        "            # Extract linguistic features\n",
        "            ling_features = text_processor.extract_linguistic_features(transcript)\n",
        "\n",
        "            # Combine features\n",
        "            combined_features = np.concatenate([\n",
        "                bert_features,\n",
        "                [ling_features[key] for key in sorted(ling_features.keys())]\n",
        "            ])\n",
        "\n",
        "            text_features_list.append(combined_features)\n",
        "\n",
        "        # Set features\n",
        "        self.set_audio_features(audio_features_list)\n",
        "        self.set_text_features(text_features_list)\n",
        "\n",
        "        print(f\"Feature extraction complete!\")\n",
        "        print(f\"  Audio features: {self.audio_features.shape}\")\n",
        "        print(f\"  Text features: {self.text_features.shape}\")\n",
        "\n",
        "    def get_feature_info(self):\n",
        "        \"\"\"Get information about the features\"\"\"\n",
        "        info = {\n",
        "            'num_samples': len(self.labels),\n",
        "            'num_classes': len(self.label_mapping),\n",
        "            'label_mapping': self.label_mapping\n",
        "        }\n",
        "\n",
        "        if self.audio_features is not None:\n",
        "            info['audio_feature_dim'] = self.audio_features.shape[1]\n",
        "\n",
        "        if self.text_features is not None:\n",
        "            info['text_feature_dim'] = self.text_features.shape[1]\n",
        "\n",
        "        return info\n",
        "\n",
        "    def get_class_weights(self):\n",
        "        \"\"\"Calculate class weights for imbalanced datasets\"\"\"\n",
        "        if hasattr(self, 'raw_labels'):\n",
        "            label_counts = pd.Series(self.raw_labels).value_counts()\n",
        "            total_samples = len(self.raw_labels)\n",
        "\n",
        "            # Calculate inverse frequency weights\n",
        "            weights = {}\n",
        "            for label, count in label_counts.items():\n",
        "                weights[self.label_mapping[label]] = total_samples / (len(label_counts) * count)\n",
        "\n",
        "            # Convert to tensor\n",
        "            weight_tensor = torch.zeros(len(self.label_mapping))\n",
        "            for class_idx, weight in weights.items():\n",
        "                weight_tensor[class_idx] = weight\n",
        "\n",
        "            return weight_tensor\n",
        "\n",
        "        return None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.audio_features is None or self.text_features is None:\n",
        "            raise ValueError(\"Features not set. Call set_audio_features() and set_text_features() first, or extract_features_from_transcripts()\")\n",
        "\n",
        "        return self.audio_features[idx], self.text_features[idx], self.labels[idx]\n",
        "\n",
        "    def get_sample_info(self, idx):\n",
        "        \"\"\"Get detailed information about a specific sample\"\"\"\n",
        "        info = {\n",
        "            'index': idx,\n",
        "            'label': self.labels[idx].item(),\n",
        "            'raw_label': self.raw_labels[idx] if hasattr(self, 'raw_labels') else None\n",
        "        }\n",
        "\n",
        "        if hasattr(self, 'file_ids'):\n",
        "            info['file_id'] = self.file_ids[idx]\n",
        "\n",
        "        if hasattr(self, 'transcripts'):\n",
        "            info['transcript'] = self.transcripts[idx]\n",
        "\n",
        "        if hasattr(self, 'file_paths'):\n",
        "            info['file_path'] = self.file_paths[idx]\n",
        "\n",
        "        return info\n",
        "\n",
        "# Example usage function\n",
        "def create_dataset_from_transcripts(transcript_csv_path, audio_extractor, text_processor,\n",
        "                                  dataset_type='diagnosis', test_size=0.2, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Create train/val/test datasets from transcript CSV\n",
        "\n",
        "    Args:\n",
        "        transcript_csv_path: Path to successful_transcripts.csv\n",
        "        audio_extractor: AudioFeatureExtractor instance\n",
        "        text_processor: BERTTextProcessor instance\n",
        "        dataset_type: 'diagnosis' or 'progression'\n",
        "        test_size: Proportion for test set\n",
        "        val_size: Proportion for validation set\n",
        "\n",
        "    Returns:\n",
        "        train_dataset, val_dataset, test_dataset\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Load full dataset\n",
        "    full_dataset = ADDataset(csv_path=transcript_csv_path, dataset_type=dataset_type)\n",
        "\n",
        "    # Extract features\n",
        "    full_dataset.extract_features_from_transcripts(audio_extractor, text_processor)\n",
        "\n",
        "    # Get indices for splitting\n",
        "    indices = list(range(len(full_dataset)))\n",
        "    labels = [full_dataset.raw_labels[i] for i in indices]\n",
        "\n",
        "    # First split: separate test set\n",
        "    train_val_idx, test_idx = train_test_split(\n",
        "        indices, test_size=test_size, stratify=labels, random_state=42\n",
        "    )\n",
        "\n",
        "    # Second split: separate train and validation\n",
        "    train_labels = [labels[i] for i in train_val_idx]\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        train_val_idx, test_size=val_size/(1-test_size), stratify=train_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    def create_subset(indices):\n",
        "        audio_subset = full_dataset.audio_features[indices]\n",
        "        text_subset = full_dataset.text_features[indices]\n",
        "        label_subset = full_dataset.labels[indices]\n",
        "\n",
        "        return ADDataset(\n",
        "            audio_features=audio_subset,\n",
        "            text_features=text_subset,\n",
        "            labels=label_subset,\n",
        "            dataset_type=dataset_type,\n",
        "            normalize_audio=False,  # Already normalized\n",
        "            normalize_text=False\n",
        "        )\n",
        "\n",
        "    train_dataset = create_subset(train_idx)\n",
        "    val_dataset = create_subset(val_idx)\n",
        "    test_dataset = create_subset(test_idx)\n",
        "\n",
        "    print(f\"\\nDataset split completed:\")\n",
        "    print(f\"  Training: {len(train_dataset)} samples\")\n",
        "    print(f\"  Validation: {len(val_dataset)} samples\")\n",
        "    print(f\"  Test: {len(test_dataset)} samples\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "2SgZdh7FonhH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gr"
      ],
      "metadata": {
        "id": "z4tAwNCS1DRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition\n",
        "!pip install pydub\n",
        "!pip install librosa\n",
        "!pip install soundfile\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "!echo \"✓ Packages ready (make sure to install them first)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYSdRXcS1cTy",
        "outputId": "463143bb-6716-4c7c-cb5e-c9be13711a45"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.13.2)\n",
            "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.3\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (2.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "✓ Packages ready (make sure to install them first)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import speech_recognition as sr\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ADReSSo21 AUDIO TRANSCRIPT EXTRACTOR\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubz5vWb71EfF",
        "outputId": "0a1a8aaa-9dbd-4380-ab3e-c7c2a63137c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ADReSSo21 AUDIO TRANSCRIPT EXTRACTOR\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 1: Mounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted successfully!\")\n",
        "except:\n",
        "    print(\"⚠ Not running in Colab or Drive already mounted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FhGrZuw1njg",
        "outputId": "f251104b-9faf-443e-b8d6-733d46f1ca23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 1: Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 3: Setting up paths and configuration...\")\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/Voice/\"\n",
        "EXTRACT_PATH = \"/content/drive/MyDrive/Voice/extracted/\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Voice/transcripts/\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "datasets = {\n",
        "    'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "    'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "    'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "}\n",
        "\n",
        "print(f\"✓ Base path: {BASE_PATH}\")\n",
        "print(f\"✓ Extract path: {EXTRACT_PATH}\")\n",
        "print(f\"✓ Output path: {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD_1lR1w2dSN",
        "outputId": "cc907128-e336-4c3e-8de3-c2581b5673c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 3: Setting up paths and configuration...\n",
            "✓ Base path: /content/drive/MyDrive/Voice/\n",
            "✓ Extract path: /content/drive/MyDrive/Voice/extracted/\n",
            "✓ Output path: /content/drive/MyDrive/Voice/transcripts/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 4: Extracting dataset files...\")\n",
        "\n",
        "def extract_datasets():\n",
        "    \"\"\"Extract all tgz files\"\"\"\n",
        "    for dataset_name, filename in datasets.items():\n",
        "        file_path = os.path.join(BASE_PATH, filename)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"  Extracting {filename}...\")\n",
        "            try:\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=EXTRACT_PATH)\n",
        "                print(f\"  ✓ {filename} extracted successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Error extracting {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"  ⚠ {filename} not found at {file_path}\")\n",
        "\n",
        "extract_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8sWcFQJ2lLq",
        "outputId": "05ef66b3-80c9-4313-fc96-e91a02f97615"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 4: Extracting dataset files...\n",
            "  Extracting ADReSSo21-progression-train.tgz...\n",
            "  ✓ ADReSSo21-progression-train.tgz extracted successfully\n",
            "  Extracting ADReSSo21-progression-test.tgz...\n",
            "  ✓ ADReSSo21-progression-test.tgz extracted successfully\n",
            "  Extracting ADReSSo21-diagnosis-train.tgz...\n",
            "  ✓ ADReSSo21-diagnosis-train.tgz extracted successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 5: Finding all WAV files...\")\n",
        "\n",
        "def find_wav_files():\n",
        "    \"\"\"Find all WAV files and organize by dataset and label\"\"\"\n",
        "    wav_files = {\n",
        "        'progression_train': {'decline': [], 'no_decline': []},\n",
        "        'progression_test': [],\n",
        "        'diagnosis_train': {'ad': [], 'cn': []}\n",
        "    }\n",
        "\n",
        "    # Progression training files\n",
        "    prog_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/train/audio/\")\n",
        "\n",
        "    # Decline cases\n",
        "    decline_path = os.path.join(prog_train_base, \"decline/\")\n",
        "    if os.path.exists(decline_path):\n",
        "        decline_wavs = [f for f in os.listdir(decline_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_train']['decline'] = [os.path.join(decline_path, f) for f in decline_wavs]\n",
        "        print(f\"  Found {len(decline_wavs)} decline WAV files\")\n",
        "\n",
        "    # No decline cases\n",
        "    no_decline_path = os.path.join(prog_train_base, \"no_decline/\")\n",
        "    if os.path.exists(no_decline_path):\n",
        "        no_decline_wavs = [f for f in os.listdir(no_decline_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_train']['no_decline'] = [os.path.join(no_decline_path, f) for f in no_decline_wavs]\n",
        "        print(f\"  Found {len(no_decline_wavs)} no_decline WAV files\")\n",
        "\n",
        "    # Progression test files\n",
        "    prog_test_path = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/test-dist/audio/\")\n",
        "    if os.path.exists(prog_test_path):\n",
        "        test_wavs = [f for f in os.listdir(prog_test_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_test'] = [os.path.join(prog_test_path, f) for f in test_wavs]\n",
        "        print(f\"  Found {len(test_wavs)} test WAV files\")\n",
        "\n",
        "    # Diagnosis training files\n",
        "    diag_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/diagnosis/train/audio/\")\n",
        "\n",
        "    # AD cases\n",
        "    ad_path = os.path.join(diag_train_base, \"ad/\")\n",
        "    if os.path.exists(ad_path):\n",
        "        ad_wavs = [f for f in os.listdir(ad_path) if f.endswith('.wav')]\n",
        "        wav_files['diagnosis_train']['ad'] = [os.path.join(ad_path, f) for f in ad_wavs]\n",
        "        print(f\"  Found {len(ad_wavs)} AD WAV files\")\n",
        "\n",
        "    # CN cases\n",
        "    cn_path = os.path.join(diag_train_base, \"cn/\")\n",
        "    if os.path.exists(cn_path):\n",
        "        cn_wavs = [f for f in os.listdir(cn_path) if f.endswith('.wav')]\n",
        "        wav_files['diagnosis_train']['cn'] = [os.path.join(cn_path, f) for f in cn_wavs]\n",
        "        print(f\"  Found {len(cn_wavs)} CN WAV files\")\n",
        "\n",
        "    return wav_files\n",
        "\n",
        "wav_files = find_wav_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvK6Vbmq6KQV",
        "outputId": "f115ffec-b55f-4fe6-f4a9-192d01361821"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 5: Finding all WAV files...\n",
            "  Found 15 decline WAV files\n",
            "  Found 58 no_decline WAV files\n",
            "  Found 32 test WAV files\n",
            "  Found 87 AD WAV files\n",
            "  Found 79 CN WAV files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 6: Setting up audio preprocessing...\")\n",
        "\n",
        "def preprocess_audio(audio_path, target_sr=16000):\n",
        "    \"\"\"Preprocess audio file for speech recognition\"\"\"\n",
        "    try:\n",
        "        # Load audio with librosa\n",
        "        audio, sr = librosa.load(audio_path, sr=target_sr)\n",
        "\n",
        "        # Normalize audio\n",
        "        audio = librosa.util.normalize(audio)\n",
        "\n",
        "        # Remove silence\n",
        "        audio_trimmed, _ = librosa.effects.trim(audio, top_db=20)\n",
        "\n",
        "        return audio_trimmed, target_sr\n",
        "    except Exception as e:\n",
        "        print(f\"    Error preprocessing {audio_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def convert_to_wav_if_needed(audio_path):\n",
        "    \"\"\"Convert audio to WAV format if needed\"\"\"\n",
        "    try:\n",
        "        if not audio_path.endswith('.wav'):\n",
        "            # Convert using pydub\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "            wav_path = audio_path.rsplit('.', 1)[0] + '_converted.wav'\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "            return wav_path\n",
        "        return audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"    Error converting {audio_path}: {e}\")\n",
        "        return audio_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaZnqo2z6Pyx",
        "outputId": "c43bf2cc-1544-4e76-bf7f-bcc8bd90bc75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 6: Setting up audio preprocessing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 7: Setting up speech recognition...\")\n",
        "\n",
        "def extract_transcript_from_audio(audio_path, method='google'):\n",
        "    \"\"\"Extract transcript from audio file using speech recognition\"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    try:\n",
        "        # Convert to WAV if needed\n",
        "        wav_path = convert_to_wav_if_needed(audio_path)\n",
        "\n",
        "        # Preprocess audio\n",
        "        audio_data, sr_rate = preprocess_audio(wav_path, target_sr=16000)\n",
        "\n",
        "        if audio_data is None:\n",
        "            return None, \"Preprocessing failed\"\n",
        "\n",
        "        # Save preprocessed audio temporarily\n",
        "        temp_wav = audio_path.replace('.wav', '_temp.wav')\n",
        "        sf.write(temp_wav, audio_data, sr_rate)\n",
        "\n",
        "        # Use speech recognition\n",
        "        with sr.AudioFile(temp_wav) as source:\n",
        "            # Adjust for ambient noise\n",
        "            recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
        "            audio = recognizer.listen(source)\n",
        "\n",
        "        # Try different recognition methods\n",
        "        transcript = None\n",
        "        error_msg = \"\"\n",
        "\n",
        "        if method == 'google':\n",
        "            try:\n",
        "                transcript = recognizer.recognize_google(audio)\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg = \"Google Speech Recognition could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg = f\"Google Speech Recognition error: {e}\"\n",
        "\n",
        "        # Fallback to other methods if Google fails\n",
        "        if transcript is None:\n",
        "            try:\n",
        "                transcript = recognizer.recognize_sphinx(audio)\n",
        "                method = 'sphinx'\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg += \"; Sphinx could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg += f\"; Sphinx error: {e}\"\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_wav):\n",
        "            os.remove(temp_wav)\n",
        "\n",
        "        if transcript:\n",
        "            return transcript.strip(), method\n",
        "        else:\n",
        "            return None, error_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error processing audio: {str(e)}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_75X-hW6W2-",
        "outputId": "fd16d9cd-6f63-40b5-d06d-f2cac2bc504c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 7: Setting up speech recognition...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 8: Processing audio files and extracting transcripts...\")\n",
        "print(\"This may take a while depending on the number and length of audio files...\")\n",
        "\n",
        "def process_audio_files(wav_files):\n",
        "    \"\"\"Process all audio files and extract transcripts\"\"\"\n",
        "    all_transcripts = []\n",
        "\n",
        "    # Process progression training data\n",
        "    print(\"\\n  Processing progression training data...\")\n",
        "    for label in ['decline', 'no_decline']:\n",
        "        files = wav_files['progression_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "            transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'progression_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method_or_error if transcript else None,\n",
        "                'error': None if transcript else method_or_error,\n",
        "                'success': transcript is not None\n",
        "            })\n",
        "\n",
        "    # Process progression test data\n",
        "    print(\"\\n  Processing progression test data...\")\n",
        "    files = wav_files['progression_test']\n",
        "    print(f\"    Processing {len(files)} test files...\")\n",
        "\n",
        "    for i, audio_path in enumerate(files):\n",
        "        print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "        transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "        all_transcripts.append({\n",
        "            'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "            'file_path': audio_path,\n",
        "            'dataset': 'progression_test',\n",
        "            'label': 'test',\n",
        "            'transcript': transcript,\n",
        "            'recognition_method': method_or_error if transcript else None,\n",
        "            'error': None if transcript else method_or_error,\n",
        "            'success': transcript is not None\n",
        "        })\n",
        "\n",
        "    # Process diagnosis training data\n",
        "    print(\"\\n  Processing diagnosis training data...\")\n",
        "    for label in ['ad', 'cn']:\n",
        "        files = wav_files['diagnosis_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "            transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'diagnosis_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method_or_error if transcript else None,\n",
        "                'error': None if transcript else method_or_error,\n",
        "                'success': transcript is not None\n",
        "            })\n",
        "\n",
        "    return all_transcripts\n",
        "\n",
        "transcripts = process_audio_files(wav_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2S8PtMo6Zpr",
        "outputId": "8991e119-17d9-4275-c2a0-6f870ac7aab7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 8: Processing audio files and extracting transcripts...\n",
            "This may take a while depending on the number and length of audio files...\n",
            "\n",
            "  Processing progression training data...\n",
            "    Processing 15 decline files...\n",
            "      Processing 1/15: adrsp055.wav\n",
            "      Processing 2/15: adrsp003.wav\n",
            "      Processing 3/15: adrsp266.wav\n",
            "      Processing 4/15: adrsp300.wav\n",
            "      Processing 5/15: adrsp320.wav\n",
            "      Processing 6/15: adrsp313.wav\n",
            "      Processing 7/15: adrsp179.wav\n",
            "      Processing 8/15: adrsp357.wav\n",
            "      Processing 9/15: adrsp051.wav\n",
            "      Processing 10/15: adrsp101.wav\n",
            "      Processing 11/15: adrsp326.wav\n",
            "      Processing 12/15: adrsp127.wav\n",
            "      Processing 13/15: adrsp276.wav\n",
            "      Processing 14/15: adrsp209.wav\n",
            "      Processing 15/15: adrsp318.wav\n",
            "    Processing 58 no_decline files...\n",
            "      Processing 1/58: adrsp196.wav\n",
            "      Processing 2/58: adrsp137.wav\n",
            "      Processing 3/58: adrsp130.wav\n",
            "      Processing 4/58: adrsp349.wav\n",
            "      Processing 5/58: adrsp198.wav\n",
            "      Processing 6/58: adrsp321.wav\n",
            "      Processing 7/58: adrsp136.wav\n",
            "      Processing 8/58: adrsp024.wav\n",
            "      Processing 9/58: adrsp007.wav\n",
            "      Processing 10/58: adrsp382.wav\n",
            "      Processing 11/58: adrsp043.wav\n",
            "      Processing 12/58: adrsp019.wav\n",
            "      Processing 13/58: adrsp333.wav\n",
            "      Processing 14/58: adrsp056.wav\n",
            "      Processing 15/58: adrsp310.wav\n",
            "      Processing 16/58: adrsp042.wav\n",
            "      Processing 17/58: adrsp377.wav\n",
            "      Processing 18/58: adrsp363.wav\n",
            "      Processing 19/58: adrsp028.wav\n",
            "      Processing 20/58: adrsp350.wav\n",
            "      Processing 21/58: adrsp096.wav\n",
            "      Processing 22/58: adrsp052.wav\n",
            "      Processing 23/58: adrsp204.wav\n",
            "      Processing 24/58: adrsp380.wav\n",
            "      Processing 25/58: adrsp109.wav\n",
            "      Processing 26/58: adrsp255.wav\n",
            "      Processing 27/58: adrsp157.wav\n",
            "      Processing 28/58: adrsp306.wav\n",
            "      Processing 29/58: adrsp197.wav\n",
            "      Processing 30/58: adrsp031.wav\n",
            "      Processing 31/58: adrsp368.wav\n",
            "      Processing 32/58: adrsp032.wav\n",
            "      Processing 33/58: adrsp091.wav\n",
            "      Processing 34/58: adrsp344.wav\n",
            "      Processing 35/58: adrsp124.wav\n",
            "      Processing 36/58: adrsp195.wav\n",
            "      Processing 37/58: adrsp253.wav\n",
            "      Processing 38/58: adrsp251.wav\n",
            "      Processing 39/58: adrsp039.wav\n",
            "      Processing 40/58: adrsp001.wav\n",
            "      Processing 41/58: adrsp041.wav\n",
            "      Processing 42/58: adrsp384.wav\n",
            "      Processing 43/58: adrsp207.wav\n",
            "      Processing 44/58: adrsp379.wav\n",
            "      Processing 45/58: adrsp324.wav\n",
            "      Processing 46/58: adrsp177.wav\n",
            "      Processing 47/58: adrsp148.wav\n",
            "      Processing 48/58: adrsp023.wav\n",
            "      Processing 49/58: adrsp359.wav\n",
            "      Processing 50/58: adrsp122.wav\n",
            "      Processing 51/58: adrsp200.wav\n",
            "      Processing 52/58: adrsp030.wav\n",
            "      Processing 53/58: adrsp319.wav\n",
            "      Processing 54/58: adrsp378.wav\n",
            "      Processing 55/58: adrsp193.wav\n",
            "      Processing 56/58: adrsp128.wav\n",
            "      Processing 57/58: adrsp161.wav\n",
            "      Processing 58/58: adrsp192.wav\n",
            "\n",
            "  Processing progression test data...\n",
            "    Processing 32 test files...\n",
            "      Processing 1/32: adrspt26.wav\n",
            "      Processing 2/32: adrspt5.wav\n",
            "      Processing 3/32: adrspt20.wav\n",
            "      Processing 4/32: adrspt22.wav\n",
            "      Processing 5/32: adrspt32.wav\n",
            "      Processing 6/32: adrspt11.wav\n",
            "      Processing 7/32: adrspt7.wav\n",
            "      Processing 8/32: adrspt2.wav\n",
            "      Processing 9/32: adrspt8.wav\n",
            "      Processing 10/32: adrspt18.wav\n",
            "      Processing 11/32: adrspt12.wav\n",
            "      Processing 12/32: adrspt17.wav\n",
            "      Processing 13/32: adrspt3.wav\n",
            "      Processing 14/32: adrspt28.wav\n",
            "      Processing 15/32: adrspt4.wav\n",
            "      Processing 16/32: adrspt6.wav\n",
            "      Processing 17/32: adrspt15.wav\n",
            "      Processing 18/32: adrspt21.wav\n",
            "      Processing 19/32: adrspt19.wav\n",
            "      Processing 20/32: adrspt31.wav\n",
            "      Processing 21/32: adrspt29.wav\n",
            "      Processing 22/32: adrspt14.wav\n",
            "      Processing 23/32: adrspt13.wav\n",
            "      Processing 24/32: adrspt23.wav\n",
            "      Processing 25/32: adrspt10.wav\n",
            "      Processing 26/32: adrspt30.wav\n",
            "      Processing 27/32: adrspt16.wav\n",
            "      Processing 28/32: adrspt27.wav\n",
            "      Processing 29/32: adrspt1.wav\n",
            "      Processing 30/32: adrspt25.wav\n",
            "      Processing 31/32: adrspt24.wav\n",
            "      Processing 32/32: adrspt9.wav\n",
            "\n",
            "  Processing diagnosis training data...\n",
            "    Processing 87 ad files...\n",
            "      Processing 1/87: adrso024.wav\n",
            "      Processing 2/87: adrso045.wav\n",
            "      Processing 3/87: adrso043.wav\n",
            "      Processing 4/87: adrso036.wav\n",
            "      Processing 5/87: adrso060.wav\n",
            "      Processing 6/87: adrso074.wav\n",
            "      Processing 7/87: adrso070.wav\n",
            "      Processing 8/87: adrso071.wav\n",
            "      Processing 9/87: adrso072.wav\n",
            "      Processing 10/87: adrso077.wav\n",
            "      Processing 11/87: adrso047.wav\n",
            "      Processing 12/87: adrso068.wav\n",
            "      Processing 13/87: adrso075.wav\n",
            "      Processing 14/87: adrso049.wav\n",
            "      Processing 15/87: adrso053.wav\n",
            "      Processing 16/87: adrso059.wav\n",
            "      Processing 17/87: adrso055.wav\n",
            "      Processing 18/87: adrso126.wav\n",
            "      Processing 19/87: adrso106.wav\n",
            "      Processing 20/87: adrso092.wav\n",
            "      Processing 21/87: adrso123.wav\n",
            "      Processing 22/87: adrso109.wav\n",
            "      Processing 23/87: adrso130.wav\n",
            "      Processing 24/87: adrso093.wav\n",
            "      Processing 25/87: adrso089.wav\n",
            "      Processing 26/87: adrso134.wav\n",
            "      Processing 27/87: adrso128.wav\n",
            "      Processing 28/87: adrso142.wav\n",
            "      Processing 29/87: adrso138.wav\n",
            "      Processing 30/87: adrso125.wav\n",
            "      Processing 31/87: adrso112.wav\n",
            "      Processing 32/87: adrso110.wav\n",
            "      Processing 33/87: adrso187.wav\n",
            "      Processing 34/87: adrso116.wav\n",
            "      Processing 35/87: adrso098.wav\n",
            "      Processing 36/87: adrso144.wav\n",
            "      Processing 37/87: adrso188.wav\n",
            "      Processing 38/87: adrso141.wav\n",
            "      Processing 39/87: adrso218.wav\n",
            "      Processing 40/87: adrso198.wav\n",
            "      Processing 41/87: adrso205.wav\n",
            "      Processing 42/87: adrso206.wav\n",
            "      Processing 43/87: adrso211.wav\n",
            "      Processing 44/87: adrso216.wav\n",
            "      Processing 45/87: adrso209.wav\n",
            "      Processing 46/87: adrso237.wav\n",
            "      Processing 47/87: adrso229.wav\n",
            "      Processing 48/87: adrso249.wav\n",
            "      Processing 49/87: adrso250.wav\n",
            "      Processing 50/87: adrso234.wav\n",
            "      Processing 51/87: adrso228.wav\n",
            "      Processing 52/87: adrso222.wav\n",
            "      Processing 53/87: adrso233.wav\n",
            "      Processing 54/87: adrso223.wav\n",
            "      Processing 55/87: adrso232.wav\n",
            "      Processing 56/87: adrso236.wav\n",
            "      Processing 57/87: adrso253.wav\n",
            "      Processing 58/87: adrso248.wav\n",
            "      Processing 59/87: adrso246.wav\n",
            "      Processing 60/87: adrso247.wav\n",
            "      Processing 61/87: adrso245.wav\n",
            "      Processing 62/87: adrso224.wav\n",
            "      Processing 63/87: adrso189.wav\n",
            "      Processing 64/87: adrso202.wav\n",
            "      Processing 65/87: adrso063.wav\n",
            "      Processing 66/87: adrso244.wav\n",
            "      Processing 67/87: adrso122.wav\n",
            "      Processing 68/87: adrso039.wav\n",
            "      Processing 69/87: adrso190.wav\n",
            "      Processing 70/87: adrso192.wav\n",
            "      Processing 71/87: adrso215.wav\n",
            "      Processing 72/87: adrso090.wav\n",
            "      Processing 73/87: adrso025.wav\n",
            "      Processing 74/87: adrso031.wav\n",
            "      Processing 75/87: adrso197.wav\n",
            "      Processing 76/87: adrso027.wav\n",
            "      Processing 77/87: adrso056.wav\n",
            "      Processing 78/87: adrso054.wav\n",
            "      Processing 79/87: adrso078.wav\n",
            "      Processing 80/87: adrso200.wav\n",
            "      Processing 81/87: adrso028.wav\n",
            "      Processing 82/87: adrso220.wav\n",
            "      Processing 83/87: adrso212.wav\n",
            "      Processing 84/87: adrso046.wav\n",
            "      Processing 85/87: adrso035.wav\n",
            "      Processing 86/87: adrso033.wav\n",
            "      Processing 87/87: adrso032.wav\n",
            "    Processing 79 cn files...\n",
            "      Processing 1/79: adrso002.wav\n",
            "      Processing 2/79: adrso021.wav\n",
            "      Processing 3/79: adrso012.wav\n",
            "      Processing 4/79: adrso022.wav\n",
            "      Processing 5/79: adrso010.wav\n",
            "      Processing 6/79: adrso007.wav\n",
            "      Processing 7/79: adrso016.wav\n",
            "      Processing 8/79: adrso015.wav\n",
            "      Processing 9/79: adrso008.wav\n",
            "      Processing 10/79: adrso014.wav\n",
            "      Processing 11/79: adrso018.wav\n",
            "      Processing 12/79: adrso017.wav\n",
            "      Processing 13/79: adrso003.wav\n",
            "      Processing 14/79: adrso019.wav\n",
            "      Processing 15/79: adrso005.wav\n",
            "      Processing 16/79: adrso169.wav\n",
            "      Processing 17/79: adrso157.wav\n",
            "      Processing 18/79: adrso023.wav\n",
            "      Processing 19/79: adrso148.wav\n",
            "      Processing 20/79: adrso173.wav\n",
            "      Processing 21/79: adrso153.wav\n",
            "      Processing 22/79: adrso172.wav\n",
            "      Processing 23/79: adrso161.wav\n",
            "      Processing 24/79: adrso165.wav\n",
            "      Processing 25/79: adrso164.wav\n",
            "      Processing 26/79: adrso156.wav\n",
            "      Processing 27/79: adrso159.wav\n",
            "      Processing 28/79: adrso168.wav\n",
            "      Processing 29/79: adrso170.wav\n",
            "      Processing 30/79: adrso154.wav\n",
            "      Processing 31/79: adrso152.wav\n",
            "      Processing 32/79: adrso160.wav\n",
            "      Processing 33/79: adrso167.wav\n",
            "      Processing 34/79: adrso183.wav\n",
            "      Processing 35/79: adrso178.wav\n",
            "      Processing 36/79: adrso260.wav\n",
            "      Processing 37/79: adrso182.wav\n",
            "      Processing 38/79: adrso180.wav\n",
            "      Processing 39/79: adrso276.wav\n",
            "      Processing 40/79: adrso264.wav\n",
            "      Processing 41/79: adrso266.wav\n",
            "      Processing 42/79: adrso273.wav\n",
            "      Processing 43/79: adrso177.wav\n",
            "      Processing 44/79: adrso186.wav\n",
            "      Processing 45/79: adrso274.wav\n",
            "      Processing 46/79: adrso262.wav\n",
            "      Processing 47/79: adrso307.wav\n",
            "      Processing 48/79: adrso281.wav\n",
            "      Processing 49/79: adrso286.wav\n",
            "      Processing 50/79: adrso289.wav\n",
            "      Processing 51/79: adrso296.wav\n",
            "      Processing 52/79: adrso299.wav\n",
            "      Processing 53/79: adrso291.wav\n",
            "      Processing 54/79: adrso285.wav\n",
            "      Processing 55/79: adrso315.wav\n",
            "      Processing 56/79: adrso308.wav\n",
            "      Processing 57/79: adrso312.wav\n",
            "      Processing 58/79: adrso298.wav\n",
            "      Processing 59/79: adrso310.wav\n",
            "      Processing 60/79: adrso316.wav\n",
            "      Processing 61/79: adrso309.wav\n",
            "      Processing 62/79: adrso277.wav\n",
            "      Processing 63/79: adrso283.wav\n",
            "      Processing 64/79: adrso302.wav\n",
            "      Processing 65/79: adrso300.wav\n",
            "      Processing 66/79: adrso162.wav\n",
            "      Processing 67/79: adrso278.wav\n",
            "      Processing 68/79: adrso292.wav\n",
            "      Processing 69/79: adrso261.wav\n",
            "      Processing 70/79: adrso265.wav\n",
            "      Processing 71/79: adrso268.wav\n",
            "      Processing 72/79: adrso259.wav\n",
            "      Processing 73/79: adrso151.wav\n",
            "      Processing 74/79: adrso267.wav\n",
            "      Processing 75/79: adrso257.wav\n",
            "      Processing 76/79: adrso263.wav\n",
            "      Processing 77/79: adrso280.wav\n",
            "      Processing 78/79: adrso270.wav\n",
            "      Processing 79/79: adrso158.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 9: Saving transcription results...\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(transcripts)\n",
        "\n",
        "# Save complete results\n",
        "complete_output = os.path.join(OUTPUT_PATH, \"all_transcripts.csv\")\n",
        "df.to_csv(complete_output, index=False)\n",
        "print(f\"✓ Saved complete results to: {complete_output}\")\n",
        "\n",
        "# Save successful transcripts only\n",
        "successful_df = df[df['success'] == True].copy()\n",
        "success_output = os.path.join(OUTPUT_PATH, \"successful_transcripts.csv\")\n",
        "successful_df.to_csv(success_output, index=False)\n",
        "print(f\"✓ Saved successful transcripts to: {success_output}\")\n",
        "\n",
        "# Save by dataset\n",
        "datasets_to_save = df['dataset'].unique()\n",
        "for dataset in datasets_to_save:\n",
        "    dataset_df = df[df['dataset'] == dataset].copy()\n",
        "    dataset_output = os.path.join(OUTPUT_PATH, f\"{dataset}_transcripts.csv\")\n",
        "    dataset_df.to_csv(dataset_output, index=False)\n",
        "    print(f\"✓ Saved {dataset} transcripts to: {dataset_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLjkXZYD7uTh",
        "outputId": "58461634-6f3c-41ae-8bc5-9a908ce457af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 9: Saving transcription results...\n",
            "✓ Saved complete results to: /content/drive/MyDrive/Voice/transcripts/all_transcripts.csv\n",
            "✓ Saved successful transcripts to: /content/drive/MyDrive/Voice/transcripts/successful_transcripts.csv\n",
            "✓ Saved progression_train transcripts to: /content/drive/MyDrive/Voice/transcripts/progression_train_transcripts.csv\n",
            "✓ Saved progression_test transcripts to: /content/drive/MyDrive/Voice/transcripts/progression_test_transcripts.csv\n",
            "✓ Saved diagnosis_train transcripts to: /content/drive/MyDrive/Voice/transcripts/diagnosis_train_transcripts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 10: Summary Statistics\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "total_files = len(df)\n",
        "successful = len(successful_df)\n",
        "failed = total_files - successful\n",
        "\n",
        "print(f\"Total audio files processed: {total_files}\")\n",
        "print(f\"Successful transcriptions: {successful} ({successful/total_files*100:.1f}%)\")\n",
        "print(f\"Failed transcriptions: {failed} ({failed/total_files*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nDataset breakdown:\")\n",
        "for dataset in df['dataset'].unique():\n",
        "    dataset_total = len(df[df['dataset'] == dataset])\n",
        "    dataset_success = len(df[(df['dataset'] == dataset) & (df['success'] == True)])\n",
        "    print(f\"  {dataset}: {dataset_success}/{dataset_total} successful ({dataset_success/dataset_total*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nLabel distribution (successful transcripts only):\")\n",
        "if not successful_df.empty:\n",
        "    print(successful_df['label'].value_counts())\n",
        "\n",
        "print(f\"\\nRecognition methods used:\")\n",
        "if not successful_df.empty:\n",
        "    print(successful_df['recognition_method'].value_counts())\n",
        "\n",
        "print(f\"\\nSample successful transcripts:\")\n",
        "sample_transcripts = successful_df['transcript'].dropna().head(3)\n",
        "for i, transcript in enumerate(sample_transcripts):\n",
        "    print(f\"  Sample {i+1}: {transcript[:200]}...\")\n",
        "\n",
        "print(f\"\\nMost common errors:\")\n",
        "error_df = df[df['success'] == False]\n",
        "if not error_df.empty:\n",
        "    error_counts = error_df['error'].value_counts().head(5)\n",
        "    for error, count in error_counts.items():\n",
        "        print(f\"  {error}: {count} files\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRANSCRIPT EXTRACTION COMPLETE!\")\n",
        "print(f\"All results saved in: {OUTPUT_PATH}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbAoDwrK7uN2",
        "outputId": "3ac7d3f9-c175-4f01-f64b-675475de3da9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 10: Summary Statistics\n",
            "==================================================\n",
            "Total audio files processed: 271\n",
            "Successful transcriptions: 156 (57.6%)\n",
            "Failed transcriptions: 115 (42.4%)\n",
            "\n",
            "Dataset breakdown:\n",
            "  progression_train: 42/73 successful (57.5%)\n",
            "  progression_test: 16/32 successful (50.0%)\n",
            "  diagnosis_train: 98/166 successful (59.0%)\n",
            "\n",
            "Label distribution (successful transcripts only):\n",
            "label\n",
            "ad            52\n",
            "cn            46\n",
            "no_decline    34\n",
            "test          16\n",
            "decline        8\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Recognition methods used:\n",
            "recognition_method\n",
            "google    156\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample successful transcripts:\n",
            "  Sample 1: you can start now...\n",
            "  Sample 2: is in 1 minute time I want you to name as many...\n",
            "  Sample 3: cat dog giraffe...\n",
            "\n",
            "Most common errors:\n",
            "  Google Speech Recognition could not understand audio; Sphinx error: missing PocketSphinx module: ensure that PocketSphinx is set up correctly.: 115 files\n",
            "\n",
            "============================================================\n",
            "TRANSCRIPT EXTRACTION COMPLETE!\n",
            "All results saved in: /content/drive/MyDrive/Voice/transcripts/\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For BERT processing\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE AD CLASSIFICATION PIPELINE\")\n",
        "print(\"Audio Features + BERT + DARTS Architecture\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIu1T0JY71KR",
        "outputId": "5ab9962b-399f-4e5a-f102-99bdd2837ba8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "COMPREHENSIVE AD CLASSIFICATION PIPELINE\n",
            "Audio Features + BERT + DARTS Architecture\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioFeatureExtractor:\n",
        "    def __init__(self, sr=16000, n_mfcc=13, n_fft=2048, hop_length=512):\n",
        "        self.sr = sr\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "    def extract_mfcc_features(self, audio_path):\n",
        "        \"\"\"Extract comprehensive MFCC features including deltas\"\"\"\n",
        "        try:\n",
        "            # Load audio\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            # Extract MFCC features\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc,\n",
        "                                       n_fft=self.n_fft, hop_length=self.hop_length)\n",
        "\n",
        "            # Extract delta features (first derivative)\n",
        "            delta_mfccs = librosa.feature.delta(mfccs)\n",
        "\n",
        "            # Extract delta-delta features (second derivative)\n",
        "            delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
        "\n",
        "            # Combine all MFCC features\n",
        "            combined_mfccs = np.concatenate([mfccs, delta_mfccs, delta2_mfccs], axis=0)\n",
        "\n",
        "            # Statistical features for each coefficient\n",
        "            features = {}\n",
        "\n",
        "            # Mean, std, min, max for each coefficient\n",
        "            features['mfcc_mean'] = np.mean(combined_mfccs, axis=1)\n",
        "            features['mfcc_std'] = np.std(combined_mfccs, axis=1)\n",
        "            features['mfcc_min'] = np.min(combined_mfccs, axis=1)\n",
        "            features['mfcc_max'] = np.max(combined_mfccs, axis=1)\n",
        "            features['mfcc_median'] = np.median(combined_mfccs, axis=1)\n",
        "            features['mfcc_skew'] = self._calculate_skewness(combined_mfccs)\n",
        "            features['mfcc_kurtosis'] = self._calculate_kurtosis(combined_mfccs)\n",
        "\n",
        "            return features, combined_mfccs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting MFCC from {audio_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def extract_spectral_features(self, audio_path):\n",
        "        \"\"\"Extract spectral features\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            features = {}\n",
        "\n",
        "            # Spectral centroid\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "            features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
        "            features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
        "\n",
        "            # Spectral bandwidth\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
        "            features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
        "            features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
        "\n",
        "            # Spectral rolloff\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "            features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
        "            features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
        "\n",
        "            # Zero crossing rate\n",
        "            zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "            features['zcr_mean'] = np.mean(zcr)\n",
        "            features['zcr_std'] = np.std(zcr)\n",
        "\n",
        "            # Chroma features\n",
        "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            features['chroma_mean'] = np.mean(chroma, axis=1)\n",
        "            features['chroma_std'] = np.std(chroma, axis=1)\n",
        "\n",
        "            # Tempo\n",
        "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "            features['tempo'] = tempo\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting spectral features from {audio_path}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def extract_prosodic_features(self, audio_path):\n",
        "        \"\"\"Extract prosodic features (pitch, energy, etc.)\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            features = {}\n",
        "\n",
        "            # Fundamental frequency (pitch)\n",
        "            f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'),\n",
        "                                                       fmax=librosa.note_to_hz('C7'))\n",
        "\n",
        "            # Remove NaN values\n",
        "            f0_clean = f0[~np.isnan(f0)]\n",
        "            if len(f0_clean) > 0:\n",
        "                features['f0_mean'] = np.mean(f0_clean)\n",
        "                features['f0_std'] = np.std(f0_clean)\n",
        "                features['f0_min'] = np.min(f0_clean)\n",
        "                features['f0_max'] = np.max(f0_clean)\n",
        "                features['f0_range'] = np.max(f0_clean) - np.min(f0_clean)\n",
        "            else:\n",
        "                features.update({\n",
        "                    'f0_mean': 0, 'f0_std': 0, 'f0_min': 0,\n",
        "                    'f0_max': 0, 'f0_range': 0\n",
        "                })\n",
        "\n",
        "            # RMS energy\n",
        "            rms = librosa.feature.rms(y=y)[0]\n",
        "            features['rms_mean'] = np.mean(rms)\n",
        "            features['rms_std'] = np.std(rms)\n",
        "\n",
        "            # Spectral contrast\n",
        "            contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "            features['contrast_mean'] = np.mean(contrast, axis=1)\n",
        "            features['contrast_std'] = np.std(contrast, axis=1)\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting prosodic features from {audio_path}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _calculate_skewness(self, data):\n",
        "        \"\"\"Calculate skewness for each row\"\"\"\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        std[std == 0] = 1  # Avoid division by zero\n",
        "        normalized = (data - mean) / std\n",
        "        skewness = np.mean(normalized**3, axis=1)\n",
        "        return skewness\n",
        "\n",
        "    def _calculate_kurtosis(self, data):\n",
        "        \"\"\"Calculate kurtosis for each row\"\"\"\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        std[std == 0] = 1  # Avoid division by zero\n",
        "        normalized = (data - mean) / std\n",
        "        kurtosis = np.mean(normalized**4, axis=1) - 3\n",
        "        return kurtosis\n",
        "\n",
        "    def extract_all_features(self, audio_path):\n",
        "        \"\"\"Extract all audio features\"\"\"\n",
        "        all_features = {}\n",
        "\n",
        "        # MFCC features\n",
        "        mfcc_features, mfcc_matrix = self.extract_mfcc_features(audio_path)\n",
        "        if mfcc_features:\n",
        "            all_features.update(mfcc_features)\n",
        "\n",
        "        # Spectral features\n",
        "        spectral_features = self.extract_spectral_features(audio_path)\n",
        "        all_features.update(spectral_features)\n",
        "\n",
        "        # Prosodic features\n",
        "        prosodic_features = self.extract_prosodic_features(audio_path)\n",
        "        all_features.update(prosodic_features)\n",
        "\n",
        "        # Flatten nested arrays\n",
        "        flattened_features = {}\n",
        "        for key, value in all_features.items():\n",
        "            if isinstance(value, np.ndarray):\n",
        "                if value.ndim == 1:\n",
        "                    for i, v in enumerate(value):\n",
        "                        flattened_features[f\"{key}_{i}\"] = v\n",
        "                else:\n",
        "                    flattened_features[key] = np.mean(value)\n",
        "            else:\n",
        "                flattened_features[key] = value\n",
        "\n",
        "        return flattened_features, mfcc_matrix"
      ],
      "metadata": {
        "id": "dPwPw04z74Ug"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTTextProcessor:\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        if pd.isna(text) or text is None:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to string and clean\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove special characters but keep spaces and basic punctuation\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def extract_bert_features(self, text):\n",
        "        \"\"\"Extract BERT embeddings from text\"\"\"\n",
        "        try:\n",
        "            # Preprocess text\n",
        "            clean_text = self.preprocess_text(text)\n",
        "\n",
        "            if not clean_text:\n",
        "                # Return zero vector for empty text\n",
        "                return np.zeros(768)\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                clean_text,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "                # Use [CLS] token embedding (first token)\n",
        "                cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
        "\n",
        "                # Also compute mean pooling of all tokens\n",
        "                attention_mask = inputs['attention_mask']\n",
        "                token_embeddings = outputs.last_hidden_state\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "                mean_embedding = sum_embeddings / sum_mask\n",
        "                mean_embedding = mean_embedding.squeeze()\n",
        "\n",
        "                # Combine CLS and mean pooling\n",
        "                combined_embedding = (cls_embedding + mean_embedding) / 2\n",
        "\n",
        "                return combined_embedding.numpy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting BERT features: {e}\")\n",
        "            return np.zeros(768)\n",
        "\n",
        "    def extract_linguistic_features(self, text):\n",
        "        \"\"\"Extract basic linguistic features\"\"\"\n",
        "        try:\n",
        "            clean_text = self.preprocess_text(text)\n",
        "\n",
        "            if not clean_text:\n",
        "                return {\n",
        "                    'word_count': 0, 'char_count': 0, 'avg_word_length': 0,\n",
        "                    'sentence_count': 0, 'question_count': 0, 'exclamation_count': 0\n",
        "                }\n",
        "\n",
        "            words = clean_text.split()\n",
        "            sentences = clean_text.split('.')\n",
        "\n",
        "            features = {\n",
        "                'word_count': len(words),\n",
        "                'char_count': len(clean_text),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'question_count': clean_text.count('?'),\n",
        "                'exclamation_count': clean_text.count('!')\n",
        "            }\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting linguistic features: {e}\")\n",
        "            return {'word_count': 0, 'char_count': 0, 'avg_word_length': 0,\n",
        "                   'sentence_count': 0, 'question_count': 0, 'exclamation_count': 0}"
      ],
      "metadata": {
        "id": "rx0_zvrI76wt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedDARTSCell(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_ops=8):\n",
        "        super(ImprovedDARTSCell, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Ensure dimensions match for operations\n",
        "        if input_dim != output_dim:\n",
        "            self.projection = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            self.projection = nn.Identity()\n",
        "\n",
        "        # Define possible operations with proper dimensionality\n",
        "        self.operations = nn.ModuleList([\n",
        "            nn.Identity(),  # Skip connection\n",
        "            nn.ReLU(),      # Activation\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU()),  # Linear + ReLU\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.Tanh()),  # Linear + Tanh\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU(), nn.Dropout(0.1)),  # With dropout\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim // 2), nn.ReLU(), nn.Linear(output_dim // 2, output_dim)),  # Bottleneck\n",
        "            nn.Sequential(nn.LayerNorm(output_dim), nn.ReLU()),  # Layer norm + activation\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU(), nn.Linear(output_dim, output_dim))  # Deep linear\n",
        "        ])\n",
        "\n",
        "        # Architecture parameters (alpha) - learnable weights for each operation\n",
        "        self.alpha = nn.Parameter(torch.randn(len(self.operations)))\n",
        "\n",
        "        # Temperature parameter for gumbel softmax (learnable)\n",
        "        self.temperature = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project input to correct dimension\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Apply Gumbel Softmax for differentiable architecture search\n",
        "        if self.training:\n",
        "            # Use Gumbel Softmax during training\n",
        "            gumbel_weights = F.gumbel_softmax(self.alpha, tau=self.temperature, hard=False)\n",
        "        else:\n",
        "            # Use regular softmax during evaluation\n",
        "            gumbel_weights = F.softmax(self.alpha / self.temperature, dim=0)\n",
        "\n",
        "        # Apply operations\n",
        "        outputs = []\n",
        "        for op in self.operations:\n",
        "            try:\n",
        "                out = op(x)\n",
        "                outputs.append(out)\n",
        "            except Exception as e:\n",
        "                # Fallback to identity if operation fails\n",
        "                outputs.append(x)\n",
        "\n",
        "        # Weighted combination of all operations\n",
        "        result = sum(w * out for w, out in zip(gumbel_weights, outputs))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_selected_operation(self):\n",
        "        \"\"\"Get the operation with highest weight (for inference)\"\"\"\n",
        "        selected_idx = torch.argmax(self.alpha).item()\n",
        "        return selected_idx, self.operations[selected_idx]\n",
        "\n",
        "class MultimodalDARTSClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=256, num_classes=2):\n",
        "        super(MultimodalDARTSClassifier, self).__init__()\n",
        "\n",
        "        # Audio processing branch with DARTS\n",
        "        self.audio_projection = nn.Sequential(\n",
        "            nn.Linear(audio_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.audio_darts_cells = nn.ModuleList([\n",
        "            ImprovedDARTSCell(hidden_dim, hidden_dim) for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        # Text processing branch with DARTS\n",
        "        self.text_projection = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.text_darts_cells = nn.ModuleList([\n",
        "            ImprovedDARTSCell(hidden_dim, hidden_dim) for _ in range(2)\n",
        "        ])\n",
        "\n",
        "        # Cross-modal attention\n",
        "        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
        "\n",
        "        # Fusion and classification\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LayerNorm(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        # Process audio through DARTS\n",
        "        audio_x = self.audio_projection(audio_features)\n",
        "        for cell in self.audio_darts_cells:\n",
        "            residual = audio_x\n",
        "            audio_x = cell(audio_x)\n",
        "            audio_x = audio_x + residual  # Residual connection\n",
        "\n",
        "        # Process text through DARTS\n",
        "        text_x = self.text_projection(text_features)\n",
        "        for cell in self.text_darts_cells:\n",
        "            residual = text_x\n",
        "            text_x = cell(text_x)\n",
        "            text_x = text_x + residual  # Residual connection\n",
        "\n",
        "        # Cross-modal attention\n",
        "        audio_attended, _ = self.cross_attention(\n",
        "            audio_x.unsqueeze(1), text_x.unsqueeze(1), text_x.unsqueeze(1)\n",
        "        )\n",
        "        audio_attended = audio_attended.squeeze(1)\n",
        "\n",
        "        # Fusion\n",
        "        fused = torch.cat([audio_attended, text_x], dim=1)\n",
        "        fused = self.fusion(fused)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(fused)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_architecture_info(self):\n",
        "        \"\"\"Get information about the learned architecture\"\"\"\n",
        "        arch_info = {}\n",
        "\n",
        "        # Audio DARTS info\n",
        "        for i, cell in enumerate(self.audio_darts_cells):\n",
        "            selected_idx, _ = cell.get_selected_operation()\n",
        "            arch_info[f'audio_cell_{i}'] = {\n",
        "                'selected_operation_idx': selected_idx,\n",
        "                'operation_weights': cell.alpha.detach().cpu().numpy(),\n",
        "                'temperature': cell.temperature.item()\n",
        "            }\n",
        "\n",
        "        # Text DARTS info\n",
        "        for i, cell in enumerate(self.text_darts_cells):\n",
        "            selected_idx, _ = cell.get_selected_operation()\n",
        "            arch_info[f'text_cell_{i}'] = {\n",
        "                'selected_operation_idx': selected_idx,\n",
        "                'operation_weights': cell.alpha.detach().cpu().numpy(),\n",
        "                'temperature': cell.temperature.item()\n",
        "            }\n",
        "\n",
        "        return arch_info"
      ],
      "metadata": {
        "id": "srPdUhV779aE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ADDataset(Dataset):\n",
        "    def __init__(self, csv_path=None, audio_features=None, text_features=None, labels=None,\n",
        "                 dataset_type='diagnosis', normalize_audio=True, normalize_text=True):\n",
        "        \"\"\"\n",
        "        Enhanced AD Dataset class for multimodal classification\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to CSV file with transcripts (from transcript extraction)\n",
        "            audio_features: Pre-extracted audio features array\n",
        "            text_features: Pre-extracted text features array\n",
        "            labels: Labels array\n",
        "            dataset_type: 'diagnosis' (AD/CN) or 'progression' (decline/no_decline)\n",
        "            normalize_audio: Whether to normalize audio features\n",
        "            normalize_text: Whether to normalize text features\n",
        "        \"\"\"\n",
        "        self.dataset_type = dataset_type\n",
        "        self.normalize_audio = normalize_audio\n",
        "        self.normalize_text = normalize_text\n",
        "\n",
        "        # Initialize scalers\n",
        "        self.audio_scaler = StandardScaler() if normalize_audio else None\n",
        "        self.text_scaler = StandardScaler() if normalize_text else None\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        if csv_path is not None:\n",
        "            # Load from CSV file (transcript extraction results)\n",
        "            self._load_from_csv(csv_path)\n",
        "        else:\n",
        "            # Load from pre-processed arrays\n",
        "            self._load_from_arrays(audio_features, text_features, labels)\n",
        "\n",
        "    def _load_from_csv(self, csv_path):\n",
        "        \"\"\"Load dataset from CSV file with transcripts\"\"\"\n",
        "        print(f\"Loading dataset from {csv_path}...\")\n",
        "\n",
        "        # Load CSV\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Filter successful transcripts only\n",
        "        df = df[df['success'] == True].copy()\n",
        "        print(f\"Found {len(df)} successful transcripts\")\n",
        "\n",
        "        # Filter by dataset type if specified\n",
        "        if self.dataset_type == 'diagnosis':\n",
        "            df = df[df['dataset'] == 'diagnosis_train'].copy()\n",
        "            valid_labels = ['ad', 'cn']\n",
        "        elif self.dataset_type == 'progression':\n",
        "            df = df[df['dataset'] == 'progression_train'].copy()\n",
        "            valid_labels = ['decline', 'no_decline']\n",
        "        else:\n",
        "            # Keep all data\n",
        "            valid_labels = df['label'].unique()\n",
        "\n",
        "        # Filter valid labels\n",
        "        df = df[df['label'].isin(valid_labels)].copy()\n",
        "        print(f\"After filtering: {len(df)} samples\")\n",
        "\n",
        "        # Extract information\n",
        "        self.file_ids = df['file_id'].tolist()\n",
        "        self.file_paths = df['file_path'].tolist()\n",
        "        self.transcripts = df['transcript'].tolist()\n",
        "        self.raw_labels = df['label'].tolist()\n",
        "\n",
        "        # Encode labels\n",
        "        self.labels = torch.LongTensor(self.label_encoder.fit_transform(self.raw_labels))\n",
        "        self.label_mapping = dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))\n",
        "\n",
        "        print(f\"Label mapping: {self.label_mapping}\")\n",
        "        print(f\"Label distribution: {pd.Series(self.raw_labels).value_counts().to_dict()}\")\n",
        "\n",
        "        # Initialize feature placeholders (will be filled by feature extractors)\n",
        "        self.audio_features = None\n",
        "        self.text_features = None\n",
        "\n",
        "    def _load_from_arrays(self, audio_features, text_features, labels):\n",
        "        \"\"\"Load dataset from pre-processed feature arrays\"\"\"\n",
        "        print(\"Loading dataset from pre-processed arrays...\")\n",
        "\n",
        "        if audio_features is None or text_features is None or labels is None:\n",
        "            raise ValueError(\"All feature arrays must be provided\")\n",
        "\n",
        "        # Convert to numpy arrays if needed\n",
        "        audio_features = np.array(audio_features)\n",
        "        text_features = np.array(text_features)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Ensure same number of samples\n",
        "        assert len(audio_features) == len(text_features) == len(labels), \\\n",
        "            \"All arrays must have the same number of samples\"\n",
        "\n",
        "        # Normalize features if requested\n",
        "        if self.normalize_audio and self.audio_scaler:\n",
        "            audio_features = self.audio_scaler.fit_transform(audio_features)\n",
        "\n",
        "        if self.normalize_text and self.text_scaler:\n",
        "            text_features = self.text_scaler.fit_transform(text_features)\n",
        "\n",
        "        # Convert to tensors\n",
        "        self.audio_features = torch.FloatTensor(audio_features)\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "\n",
        "        # Handle labels\n",
        "        if labels.dtype == 'object' or isinstance(labels[0], str):\n",
        "            # String labels - encode them\n",
        "            self.raw_labels = labels.tolist()\n",
        "            self.labels = torch.LongTensor(self.label_encoder.fit_transform(labels))\n",
        "            self.label_mapping = dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))\n",
        "        else:\n",
        "            # Numeric labels\n",
        "            self.labels = torch.LongTensor(labels)\n",
        "            self.raw_labels = labels.tolist()\n",
        "            self.label_mapping = {i: i for i in range(len(np.unique(labels)))}\n",
        "\n",
        "        print(f\"Dataset loaded: {len(self.labels)} samples\")\n",
        "        print(f\"Audio features shape: {self.audio_features.shape}\")\n",
        "        print(f\"Text features shape: {self.text_features.shape}\")\n",
        "        print(f\"Label mapping: {self.label_mapping}\")\n",
        "\n",
        "    def set_audio_features(self, audio_features):\n",
        "        \"\"\"Set audio features after extraction\"\"\"\n",
        "        audio_features = np.array(audio_features)\n",
        "\n",
        "        if self.normalize_audio and self.audio_scaler:\n",
        "            audio_features = self.audio_scaler.fit_transform(audio_features)\n",
        "\n",
        "        self.audio_features = torch.FloatTensor(audio_features)\n",
        "        print(f\"Audio features set: {self.audio_features.shape}\")\n",
        "\n",
        "    def set_text_features(self, text_features):\n",
        "        \"\"\"Set text features after extraction\"\"\"\n",
        "        text_features = np.array(text_features)\n",
        "\n",
        "        if self.normalize_text and self.text_scaler:\n",
        "            text_features = self.text_scaler.fit_transform(text_features)\n",
        "\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "        print(f\"Text features set: {self.text_features.shape}\")\n",
        "\n",
        "    def extract_features_from_transcripts(self, audio_extractor, text_processor):\n",
        "        \"\"\"\n",
        "        Extract features from audio files and transcripts\n",
        "\n",
        "        Args:\n",
        "            audio_extractor: AudioFeatureExtractor instance\n",
        "            text_processor: BERTTextProcessor instance\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'file_paths') or not hasattr(self, 'transcripts'):\n",
        "            raise ValueError(\"Dataset must be loaded from CSV to extract features\")\n",
        "\n",
        "        print(\"Extracting audio and text features...\")\n",
        "\n",
        "        # Extract audio features\n",
        "        print(\"Extracting audio features...\")\n",
        "        audio_features_list = []\n",
        "        failed_audio = 0\n",
        "\n",
        "        for i, audio_path in enumerate(self.file_paths):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"  Processing audio {i+1}/{len(self.file_paths)}\")\n",
        "\n",
        "            features, _ = audio_extractor.extract_all_features(audio_path)\n",
        "            if features:\n",
        "                # Convert to list of values in consistent order\n",
        "                feature_vector = [features.get(key, 0) for key in sorted(features.keys())]\n",
        "                audio_features_list.append(feature_vector)\n",
        "            else:\n",
        "                # Use zero vector for failed extractions\n",
        "                failed_audio += 1\n",
        "                if audio_features_list:\n",
        "                    audio_features_list.append([0] * len(audio_features_list[0]))\n",
        "                else:\n",
        "                    audio_features_list.append([0] * 100)  # Default size\n",
        "\n",
        "        if failed_audio > 0:\n",
        "            print(f\"  Warning: {failed_audio} audio files failed feature extraction\")\n",
        "\n",
        "        # Extract text features\n",
        "        print(\"Extracting text features...\")\n",
        "        text_features_list = []\n",
        "\n",
        "        for i, transcript in enumerate(self.transcripts):\n",
        "            if i % 20 == 0:\n",
        "                print(f\"  Processing text {i+1}/{len(self.transcripts)}\")\n",
        "\n",
        "            # Extract BERT features\n",
        "            bert_features = text_processor.extract_bert_features(transcript)\n",
        "\n",
        "            # Extract linguistic features\n",
        "            ling_features = text_processor.extract_linguistic_features(transcript)\n",
        "\n",
        "            # Combine features\n",
        "            combined_features = np.concatenate([\n",
        "                bert_features,\n",
        "                [ling_features[key] for key in sorted(ling_features.keys())]\n",
        "            ])\n",
        "\n",
        "            text_features_list.append(combined_features)\n",
        "\n",
        "        # Set features\n",
        "        self.set_audio_features(audio_features_list)\n",
        "        self.set_text_features(text_features_list)\n",
        "\n",
        "        print(f\"Feature extraction complete!\")\n",
        "        print(f\"  Audio features: {self.audio_features.shape}\")\n",
        "        print(f\"  Text features: {self.text_features.shape}\")\n",
        "\n",
        "    def get_feature_info(self):\n",
        "        \"\"\"Get information about the features\"\"\"\n",
        "        info = {\n",
        "            'num_samples': len(self.labels),\n",
        "            'num_classes': len(self.label_mapping),\n",
        "            'label_mapping': self.label_mapping\n",
        "        }\n",
        "\n",
        "        if self.audio_features is not None:\n",
        "            info['audio_feature_dim'] = self.audio_features.shape[1]\n",
        "\n",
        "        if self.text_features is not None:\n",
        "            info['text_feature_dim'] = self.text_features.shape[1]\n",
        "\n",
        "        return info\n",
        "\n",
        "    def get_class_weights(self):\n",
        "        \"\"\"Calculate class weights for imbalanced datasets\"\"\"\n",
        "        if hasattr(self, 'raw_labels'):\n",
        "            label_counts = pd.Series(self.raw_labels).value_counts()\n",
        "            total_samples = len(self.raw_labels)\n",
        "\n",
        "            # Calculate inverse frequency weights\n",
        "            weights = {}\n",
        "            for label, count in label_counts.items():\n",
        "                weights[self.label_mapping[label]] = total_samples / (len(label_counts) * count)\n",
        "\n",
        "            # Convert to tensor\n",
        "            weight_tensor = torch.zeros(len(self.label_mapping))\n",
        "            for class_idx, weight in weights.items():\n",
        "                weight_tensor[class_idx] = weight\n",
        "\n",
        "            return weight_tensor\n",
        "\n",
        "        return None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.audio_features is None or self.text_features is None:\n",
        "            raise ValueError(\"Features not set. Call set_audio_features() and set_text_features() first, or extract_features_from_transcripts()\")\n",
        "\n",
        "        return self.audio_features[idx], self.text_features[idx], self.labels[idx]\n",
        "\n",
        "    def get_sample_info(self, idx):\n",
        "        \"\"\"Get detailed information about a specific sample\"\"\"\n",
        "        info = {\n",
        "            'index': idx,\n",
        "            'label': self.labels[idx].item(),\n",
        "            'raw_label': self.raw_labels[idx] if hasattr(self, 'raw_labels') else None\n",
        "        }\n",
        "\n",
        "        if hasattr(self, 'file_ids'):\n",
        "            info['file_id'] = self.file_ids[idx]\n",
        "\n",
        "        if hasattr(self, 'transcripts'):\n",
        "            info['transcript'] = self.transcripts[idx]\n",
        "\n",
        "        if hasattr(self, 'file_paths'):\n",
        "            info['file_path'] = self.file_paths[idx]\n",
        "\n",
        "        return info"
      ],
      "metadata": {
        "id": "BqkYm54K7_ia"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_from_transcripts(transcript_csv_path, audio_extractor, text_processor,\n",
        "                                  dataset_type='diagnosis', test_size=0.2, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Create train/val/test datasets from transcript CSV\n",
        "\n",
        "    Args:\n",
        "        transcript_csv_path: Path to successful_transcripts.csv\n",
        "        audio_extractor: AudioFeatureExtractor instance\n",
        "        text_processor: BERTTextProcessor instance\n",
        "        dataset_type: 'diagnosis' or 'progression'\n",
        "        test_size: Proportion for test set\n",
        "        val_size: Proportion for validation set\n",
        "\n",
        "    Returns:\n",
        "        train_dataset, val_dataset, test_dataset\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Load full dataset\n",
        "    full_dataset = ADDataset(csv_path=transcript_csv_path, dataset_type=dataset_type)\n",
        "\n",
        "    # Extract features\n",
        "    full_dataset.extract_features_from_transcripts(audio_extractor, text_processor)\n",
        "\n",
        "    # Get indices for splitting\n",
        "    indices = list(range(len(full_dataset)))\n",
        "    labels = [full_dataset.raw_labels[i] for i in indices]\n",
        "\n",
        "    # First split: separate test set\n",
        "    train_val_idx, test_idx = train_test_split(\n",
        "        indices, test_size=test_size, stratify=labels, random_state=42\n",
        "    )\n",
        "\n",
        "    # Second split: separate train and validation\n",
        "    train_labels = [labels[i] for i in train_val_idx]\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        train_val_idx, test_size=val_size/(1-test_size), stratify=train_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    def create_subset(indices):\n",
        "        audio_subset = full_dataset.audio_features[indices]\n",
        "        text_subset = full_dataset.text_features[indices]\n",
        "        label_subset = full_dataset.labels[indices]\n",
        "\n",
        "        return ADDataset(\n",
        "            audio_features=audio_subset,\n",
        "            text_features=text_subset,\n",
        "            labels=label_subset,\n",
        "            dataset_type=dataset_type,\n",
        "            normalize_audio=False,  # Already normalized\n",
        "            normalize_text=False\n",
        "        )\n",
        "\n",
        "    train_dataset = create_subset(train_idx)\n",
        "    val_dataset = create_subset(val_idx)\n",
        "    test_dataset = create_subset(test_idx)\n",
        "\n",
        "    print(f\"\\nDataset split completed:\")\n",
        "    print(f\"  Training: {len(train_dataset)} samples\")\n",
        "    print(f\"  Validation: {len(val_dataset)} samples\")\n",
        "    print(f\"  Test: {len(test_dataset)} samples\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "43gARZP08CDz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrated ADReSSo21 Audio Transcript Extractor and AD Classification Pipeline\n",
        "# Combines transcript extraction with audio and text feature extraction and DARTS classification\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import speech_recognition as sr\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ADReSSo21 INTEGRATED AUDIO TRANSCRIPT AND AD CLASSIFICATION PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Mount Google Drive\n",
        "print(\"\\nMounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted successfully!\")\n",
        "except:\n",
        "    print(\"⚠ Not running in Colab or Drive already mounted\")\n",
        "\n",
        "# STEP 2: Set Up Paths and Configuration\n",
        "print(\"\\nSetting up paths and configuration...\")\n",
        "BASE_PATH = \"/content/drive/MyDrive/Voice/\"\n",
        "EXTRACT_PATH = \"/content/drive/MyDrive/Voice/extracted/\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Voice/transcripts/\"\n",
        "\n",
        "os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "datasets = {\n",
        "    'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "    'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "    'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "}\n",
        "\n",
        "print(f\"✓ Base path: {BASE_PATH}\")\n",
        "print(f\"✓ Extract path: {EXTRACT_PATH}\")\n",
        "print(f\"✓ Output path: {OUTPUT_PATH}\")\n",
        "\n",
        "# # STEP 3: Extract Dataset Files\n",
        "# print(\"\\nExtracting dataset files...\")\n",
        "# def extract_datasets():\n",
        "#     for dataset_name, filename in datasets.items():\n",
        "#         file_path = os.path.join(BASE_PATH, filename)\n",
        "#         if os.path.exists(file_path):\n",
        "#             print(f\"  Extracting {filename}...\")\n",
        "#             try:\n",
        "#                 with tarfile.open(file_path, 'r:gz') as tar:\n",
        "#                     tar.extractall(path=EXTRACT_PATH)\n",
        "#                 print(f\"  ✓ {filename} extracted successfully\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"  ⚠ Error extracting {filename}: {e}\")\n",
        "#         else:\n",
        "#             print(f\"  ⚠ {filename} not found at {file_path}\")\n",
        "\n",
        "# extract_datasets()\n",
        "\n",
        "# # STEP 4: Find All WAV Files\n",
        "# print(\"\\nFinding all WAV files...\")\n",
        "# def find_wav_files():\n",
        "#     wav_files = {\n",
        "#         'progression_train': {'decline': [], 'no_decline': []},\n",
        "#         'progression_test': [],\n",
        "#         'diagnosis_train': {'ad': [], 'cn': []}\n",
        "#     }\n",
        "#     diag_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/diagnosis/train/audio/\")\n",
        "#     ad_path = os.path.join(diag_train_base, \"ad/\")\n",
        "#     if os.path.exists(ad_path):\n",
        "#         ad_wavs = [f for f in os.listdir(ad_path) if f.endswith('.wav')]\n",
        "#         wav_files['diagnosis_train']['ad'] = [os.path.join(ad_path, f) for f in ad_wavs]\n",
        "#         print(f\"  Found {len(ad_wavs)} AD WAV files\")\n",
        "#     cn_path = os.path.join(diag_train_base, \"cn/\")\n",
        "#     if os.path.exists(cn_path):\n",
        "#         cn_wavs = [f for f in os.listdir(cn_path) if f.endswith('.wav')]\n",
        "#         wav_files['diagnosis_train']['cn'] = [os.path.join(cn_path, f) for f in cn_wavs]\n",
        "#         print(f\"  Found {len(cn_wavs)} CN WAV files\")\n",
        "#     return wav_files\n",
        "\n",
        "# wav_files = find_wav_files()\n",
        "\n",
        "# STEP 5: Audio Preprocessing Functions\n",
        "print(\"\\nSetting up audio preprocessing...\")\n",
        "def preprocess_audio(audio_path, target_sr=16000):\n",
        "    try:\n",
        "        audio, sr = librosa.load(audio_path, sr=target_sr)\n",
        "        audio = librosa.util.normalize(audio)\n",
        "        audio_trimmed, _ = librosa.effects.trim(audio, top_db=20)\n",
        "        return audio_trimmed, target_sr\n",
        "    except Exception as e:\n",
        "        print(f\"    Error preprocessing {audio_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def convert_to_wav_if_needed(audio_path):\n",
        "    try:\n",
        "        if not audio_path.endswith('.wav'):\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "            wav_path = audio_path.rsplit('.', 1)[0] + '_converted.wav'\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "            return wav_path\n",
        "        return audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"    Error converting {audio_path}: {e}\")\n",
        "        return audio_path\n",
        "\n",
        "# STEP 6: Speech Recognition Function\n",
        "print(\"\\nSetting up speech recognition...\")\n",
        "def extract_transcript_from_audio(audio_path, method='google'):\n",
        "    recognizer = sr.Recognizer()\n",
        "    try:\n",
        "        wav_path = convert_to_wav_if_needed(audio_path)\n",
        "        audio_data, sr_rate = preprocess_audio(wav_path, target_sr=16000)\n",
        "        if audio_data is None:\n",
        "            return None, \"Preprocessing failed\"\n",
        "        temp_wav = audio_path.replace('.wav', '_temp.wav')\n",
        "        sf.write(temp_wav, audio_data, sr_rate)\n",
        "        with sr.AudioFile(temp_wav) as source:\n",
        "            recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
        "            audio = recognizer.listen(source)\n",
        "        transcript = None\n",
        "        error_msg = \"\"\n",
        "        if method == 'google':\n",
        "            try:\n",
        "                transcript = recognizer.recognize_google(audio)\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg = \"Google Speech Recognition could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg = f\"Google Speech Recognition error: {e}\"\n",
        "        if transcript is None:\n",
        "            try:\n",
        "                transcript = recognizer.recognize_sphinx(audio)\n",
        "                method = 'sphinx'\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg += \"; Sphinx could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg += f\"; Sphinx error: {e}\"\n",
        "        if os.path.exists(temp_wav):\n",
        "            os.remove(temp_wav)\n",
        "        if transcript:\n",
        "            return transcript.strip(), method\n",
        "        else:\n",
        "            return None, error_msg\n",
        "    except Exception as e:\n",
        "        return None, f\"Error processing audio: {str(e)}\"\n",
        "\n",
        "# STEP 7: Process Audio Files and Extract Transcripts\n",
        "print(\"\\nProcessing audio files and extracting transcripts...\")\n",
        "def process_audio_files(wav_files):\n",
        "    all_transcripts = []\n",
        "    print(\"\\n  Processing diagnosis training data...\")\n",
        "    for label in ['ad', 'cn']:\n",
        "        files = wav_files['diagnosis_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "            transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'diagnosis_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method_or_error if transcript else None,\n",
        "                'error': None if transcript else method_or_error,\n",
        "                'success': transcript is not None\n",
        "            })\n",
        "    return all_transcripts\n",
        "\n",
        "transcripts = process_audio_files(wav_files)\n",
        "\n",
        "# STEP 8: Save Transcription Results\n",
        "print(\"\\nSaving transcription results...\")\n",
        "df = pd.DataFrame(transcripts)\n",
        "complete_output = os.path.join(OUTPUT_PATH, \"all_transcripts.csv\")\n",
        "df.to_csv(complete_output, index=False)\n",
        "print(f\"✓ Saved complete results to: {complete_output}\")\n",
        "\n",
        "successful_df = df[df['success'] == True].copy()\n",
        "success_output = os.path.join(OUTPUT_PATH, \"successful_transcripts.csv\")\n",
        "successful_df.to_csv(success_output, index=False)\n",
        "print(f\"✓ Saved successful transcripts to: {success_output}\")\n",
        "\n",
        "# STEP 9: Audio Feature Extraction\n",
        "print(\"\\nSetting up audio feature extraction...\")\n",
        "class AudioFeatureExtractor:\n",
        "    def __init__(self, sr=16000, n_mfcc=13, n_fft=2048, hop_length=512):\n",
        "        self.sr = sr\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "    def extract_mfcc_features(self, audio_path):\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc,\n",
        "                                       n_fft=self.n_fft, hop_length=self.hop_length)\n",
        "            delta_mfccs = librosa.feature.delta(mfccs)\n",
        "            delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
        "            combined_mfccs = np.concatenate([mfccs, delta_mfccs, delta2_mfccs], axis=0)\n",
        "            features = {\n",
        "                'mfcc_mean': np.mean(combined_mfccs, axis=1),\n",
        "                'mfcc_std': np.std(combined_mfccs, axis=1),\n",
        "                'mfcc_min': np.min(combined_mfccs, axis=1),\n",
        "                'mfcc_max': np.max(combined_mfccs, axis=1),\n",
        "                'mfcc_median': np.median(combined_mfccs, axis=1),\n",
        "                'mfcc_skew': self._calculate_skewness(combined_mfccs),\n",
        "                'mfcc_kurtosis': self._calculate_kurtosis(combined_mfccs)\n",
        "            }\n",
        "            return features, combined_mfccs\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting MFCC from {audio_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def extract_spectral_features(self, audio_path):\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "            features = {}\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "            features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
        "            features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
        "            features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
        "            features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "            features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
        "            features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
        "            zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "            features['zcr_mean'] = np.mean(zcr)\n",
        "            features['zcr_std'] = np.std(zcr)\n",
        "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            features['chroma_mean'] = np.mean(chroma, axis=1)\n",
        "            features['chroma_std'] = np.std(chroma, axis=1)\n",
        "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "            features['tempo'] = tempo\n",
        "            return features\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting spectral features from {audio_path}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def extract_prosodic_features(self, audio_path):\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "            features = {}\n",
        "            f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'),\n",
        "                                                       fmax=librosa.note_to_hz('C7'))\n",
        "            f0_clean = f0[~np.isnan(f0)]\n",
        "            if len(f0_clean) > 0:\n",
        "                features['f0_mean'] = np.mean(f0_clean)\n",
        "                features['f0_std'] = np.std(f0_clean)\n",
        "                features['f0_min'] = np.min(f0_clean)\n",
        "                features['f0_max'] = np.max(f0_clean)\n",
        "                features['f0_range'] = np.max(f0_clean) - np.min(f0_clean)\n",
        "            else:\n",
        "                features.update({\n",
        "                    'f0_mean': 0, 'f0_std': 0, 'f0_min': 0,\n",
        "                    'f0_max': 0, 'f0_range': 0\n",
        "                })\n",
        "            rms = librosa.feature.rms(y=y)[0]\n",
        "            features['rms_mean'] = np.mean(rms)\n",
        "            features['rms_std'] = np.std(rms)\n",
        "            contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "            features['contrast_mean'] = np.mean(contrast, axis=1)\n",
        "            features['contrast_std'] = np.std(contrast, axis=1)\n",
        "            return features\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting prosodic features from {audio_path}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _calculate_skewness(self, data):\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        std[std == 0] = 1\n",
        "        normalized = (data - mean) / std\n",
        "        skewness = np.mean(normalized**3, axis=1)\n",
        "        return skewness\n",
        "\n",
        "    def _calculate_kurtosis(self, data):\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        std[std == 0] = 1\n",
        "        normalized = (data - mean) / std\n",
        "        kurtosis = np.mean(normalized**4, axis=1) - 3\n",
        "        return kurtosis\n",
        "\n",
        "    def extract_all_features(self, audio_path):\n",
        "        all_features = {}\n",
        "        mfcc_features, mfcc_matrix = self.extract_mfcc_features(audio_path)\n",
        "        if mfcc_features:\n",
        "            all_features.update(mfcc_features)\n",
        "        spectral_features = self.extract_spectral_features(audio_path)\n",
        "        all_features.update(spectral_features)\n",
        "        prosodic_features = self.extract_prosodic_features(audio_path)\n",
        "        all_features.update(prosodic_features)\n",
        "        flattened_features = {}\n",
        "        for key, value in all_features.items():\n",
        "            if isinstance(value, np.ndarray):\n",
        "                if value.ndim == 1:\n",
        "                    for i, v in enumerate(value):\n",
        "                        flattened_features[f\"{key}_{i}\"] = v\n",
        "                else:\n",
        "                    flattened_features[key] = np.mean(value)\n",
        "            else:\n",
        "                flattened_features[key] = value\n",
        "        return flattened_features, mfcc_matrix\n",
        "\n",
        "# STEP 10: BERT Text Processing\n",
        "print(\"\\nSetting up BERT text processing...\")\n",
        "class BERTTextProcessor:\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        if pd.isna(text) or text is None:\n",
        "            return \"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?]', '', text)\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "    def extract_bert_features(self, text):\n",
        "        try:\n",
        "            clean_text = self.preprocess_text(text)\n",
        "            if not clean_text:\n",
        "                return np.zeros(768)\n",
        "            inputs = self.tokenizer(\n",
        "                clean_text,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
        "                attention_mask = inputs['attention_mask']\n",
        "                token_embeddings = outputs.last_hidden_state\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "                mean_embedding = sum_embeddings / sum_mask\n",
        "                mean_embedding = mean_embedding.squeeze()\n",
        "                combined_embedding = (cls_embedding + mean_embedding) / 2\n",
        "                return combined_embedding.numpy()\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting BERT features: {e}\")\n",
        "            return np.zeros(768)\n",
        "\n",
        "    def extract_linguistic_features(self, text):\n",
        "        try:\n",
        "            clean_text = self.preprocess_text(text)\n",
        "            if not clean_text:\n",
        "                return {\n",
        "                    'word_count': 0, 'char_count': 0, 'avg_word_length': 0,\n",
        "                    'sentence_count': 0, 'question_count': 0, 'exclamation_count': 0\n",
        "                }\n",
        "            words = clean_text.split()\n",
        "            sentences = clean_text.split('.')\n",
        "            features = {\n",
        "                'word_count': len(words),\n",
        "                'char_count': len(clean_text),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'question_count': clean_text.count('?'),\n",
        "                'exclamation_count': clean_text.count('!')\n",
        "            }\n",
        "            return features\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting linguistic features: {e}\")\n",
        "            return {'word_count': 0, 'char_count': 0, 'avg_word_length': 0,\n",
        "                   'sentence_count': 0, 'question_count': 0, 'exclamation_count': 0}\n",
        "\n",
        "# STEP 11: DARTS Architecture\n",
        "print(\"\\nSetting up DARTS architecture...\")\n",
        "class ImprovedDARTSCell(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_ops=8):\n",
        "        super(ImprovedDARTSCell, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        if input_dim != output_dim:\n",
        "            self.projection = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            self.projection = nn.Identity()\n",
        "        self.operations = nn.ModuleList([\n",
        "            nn.Identity(),\n",
        "            nn.ReLU(),\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU()),\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.Tanh()),\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU(), nn.Dropout(0.1)),\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim // 2), nn.ReLU(), nn.Linear(output_dim // 2, output_dim)),\n",
        "            nn.Sequential(nn.LayerNorm(output_dim), nn.ReLU()),\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU(), nn.Linear(output_dim, output_dim))\n",
        "        ])\n",
        "        self.alpha = nn.Parameter(torch.randn(len(self.operations)))\n",
        "        self.temperature = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        if self.training:\n",
        "            gumbel_weights = F.gumbel_softmax(self.alpha, tau=self.temperature, hard=False)\n",
        "        else:\n",
        "            gumbel_weights = F.softmax(self.alpha / self.temperature, dim=0)\n",
        "        outputs = []\n",
        "        for op in self.operations:\n",
        "            try:\n",
        "                out = op(x)\n",
        "                outputs.append(out)\n",
        "            except Exception as e:\n",
        "                outputs.append(x)\n",
        "        result = sum(w * out for w, out in zip(gumbel_weights, outputs))\n",
        "        return result\n",
        "\n",
        "class MultimodalDARTSClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=256, num_classes=2):\n",
        "        super(MultimodalDARTSClassifier, self).__init__()\n",
        "        self.audio_projection = nn.Sequential(\n",
        "            nn.Linear(audio_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.audio_darts_cells = nn.ModuleList([\n",
        "            ImprovedDARTSCell(hidden_dim, hidden_dim) for _ in range(3)\n",
        "        ])\n",
        "        self.text_projection = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.text_darts_cells = nn.ModuleList([\n",
        "            ImprovedDARTSCell(hidden_dim, hidden_dim) for _ in range(2)\n",
        "        ])\n",
        "        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LayerNorm(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_x = self.audio_projection(audio_features)\n",
        "        for cell in self.audio_darts_cells:\n",
        "            residual = audio_x\n",
        "            audio_x = cell(audio_x)\n",
        "            audio_x = audio_x + residual\n",
        "        text_x = self.text_projection(text_features)\n",
        "        for cell in self.text_darts_cells:\n",
        "            residual = text_x\n",
        "            text_x = cell(text_x)\n",
        "            text_x = text_x + residual\n",
        "        audio_attended, _ = self.cross_attention(\n",
        "            audio_x.unsqueeze(1), text_x.unsqueeze(1), text_x.unsqueeze(1)\n",
        "        )\n",
        "        audio_attended = audio_attended.squeeze(1)\n",
        "        fused = torch.cat([audio_attended, text_x], dim=1)\n",
        "        fused = self.fusion(fused)\n",
        "        output = self.classifier(fused)\n",
        "        return output\n",
        "\n",
        "# STEP 12: Dataset Class\n",
        "print(\"\\nSetting up dataset class...\")\n",
        "class ADDataset(Dataset):\n",
        "    def __init__(self, csv_path=None, audio_features=None, text_features=None, labels=None,\n",
        "                 dataset_type='diagnosis', normalize_audio=True, normalize_text=True):\n",
        "        self.dataset_type = dataset_type\n",
        "        self.normalize_audio = normalize_audio\n",
        "        self.normalize_text = normalize_text\n",
        "        self.audio_scaler = StandardScaler() if normalize_audio else None\n",
        "        self.text_scaler = StandardScaler() if normalize_text else None\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        if csv_path is not None:\n",
        "            self._load_from_csv(csv_path)\n",
        "        else:\n",
        "            self._load_from_arrays(audio_features, text_features, labels)\n",
        "\n",
        "    def _load_from_csv(self, csv_path):\n",
        "        print(f\"Loading dataset from {csv_path}...\")\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df = df[df['success'] == True].copy()\n",
        "        print(f\"Found {len(df)} successful transcripts\")\n",
        "        if self.dataset_type == 'diagnosis':\n",
        "            df = df[df['dataset'] == 'diagnosis_train'].copy()\n",
        "            valid_labels = ['ad', 'cn']\n",
        "        elif self.dataset_type == 'progression':\n",
        "            df = df[df['dataset'] == 'progression_train'].copy()\n",
        "            valid_labels = ['decline', 'no_decline']\n",
        "        else:\n",
        "            valid_labels = df['label'].unique()\n",
        "        df = df[df['label'].isin(valid_labels)].copy()\n",
        "        print(f\"After filtering: {len(df)} samples\")\n",
        "        self.file_ids = df['file_id'].tolist()\n",
        "        self.file_paths = df['file_path'].tolist()\n",
        "        self.transcripts = df['transcript'].tolist()\n",
        "        self.raw_labels = df['label'].tolist()\n",
        "        self.labels = torch.LongTensor(self.label_encoder.fit_transform(self.raw_labels))\n",
        "        self.label_mapping = dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))\n",
        "        print(f\"Label mapping: {self.label_mapping}\")\n",
        "        self.audio_features = None\n",
        "        self.text_features = None\n",
        "\n",
        "    def _load_from_arrays(self, audio_features, text_features, labels):\n",
        "        print(\"Loading dataset from pre-processed arrays...\")\n",
        "        if audio_features is None or text_features is None or labels is None:\n",
        "            raise ValueError(\"All feature arrays must be provided\")\n",
        "        audio_features = np.array(audio_features)\n",
        "        text_features = np.array(text_features)\n",
        "        labels = np.array(labels)\n",
        "        assert len(audio_features) == len(text_features) == len(labels)\n",
        "        if self.normalize_audio and self.audio_scaler:\n",
        "            audio_features = self.audio_scaler.fit_transform(audio_features)\n",
        "        if self.normalize_text and self.text_scaler:\n",
        "            text_features = self.text_scaler.fit_transform(text_features)\n",
        "        self.audio_features = torch.FloatTensor(audio_features)\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "        if labels.dtype == 'object' or isinstance(labels[0], str):\n",
        "            self.raw_labels = labels.tolist()\n",
        "            self.labels = torch.LongTensor(self.label_encoder.fit_transform(labels))\n",
        "            self.label_mapping = dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))\n",
        "        else:\n",
        "            self.labels = torch.LongTensor(labels)\n",
        "            self.raw_labels = labels.tolist()\n",
        "            self.label_mapping = {i: i for i in range(len(np.unique(labels)))}\n",
        "        print(f\"Dataset loaded: {len(self.labels)} samples\")\n",
        "\n",
        "    def set_audio_features(self, audio_features):\n",
        "        audio_features = np.array(audio_features)\n",
        "        if self.normalize_audio and self.audio_scaler:\n",
        "            audio_features = self.audio_scaler.fit_transform(audio_features)\n",
        "        self.audio_features = torch.FloatTensor(audio_features)\n",
        "\n",
        "    def set_text_features(self, text_features):\n",
        "        text_features = np.array(text_features)\n",
        "        if self.normalize_text and self.text_scaler:\n",
        "            text_features = self.text_scaler.fit_transform(text_features)\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "\n",
        "    def extract_features_from_transcripts(self, audio_extractor, text_processor):\n",
        "        if not hasattr(self, 'file_paths') or not hasattr(self, 'transcripts'):\n",
        "            raise ValueError(\"Dataset must be loaded from CSV to extract features\")\n",
        "        print(\"Extracting audio and text features...\")\n",
        "        audio_features_list = []\n",
        "        failed_audio = 0\n",
        "        for i, audio_path in enumerate(self.file_paths):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"  Processing audio {i+1}/{len(self.file_paths)}\")\n",
        "            features, _ = audio_extractor.extract_all_features(audio_path)\n",
        "            if features:\n",
        "                feature_vector = [features.get(key, 0) for key in sorted(features.keys())]\n",
        "                audio_features_list.append(feature_vector)\n",
        "            else:\n",
        "                failed_audio += 1\n",
        "                audio_features_list.append([0] * (len(audio_features_list[0]) if audio_features_list else 100))\n",
        "        if failed_audio > 0:\n",
        "            print(f\"  Warning: {failed_audio} audio files failed feature extraction\")\n",
        "        text_features_list = []\n",
        "        for i, transcript in enumerate(self.transcripts):\n",
        "            if i % 20 == 0:\n",
        "                print(f\"  Processing text {i+1}/{len(self.transcripts)}\")\n",
        "            bert_features = text_processor.extract_bert_features(transcript)\n",
        "            ling_features = text_processor.extract_linguistic_features(transcript)\n",
        "            combined_features = np.concatenate([\n",
        "                bert_features,\n",
        "                [ling_features[key] for key in sorted(ling_features.keys())]\n",
        "            ])\n",
        "            text_features_list.append(combined_features)\n",
        "        self.set_audio_features(audio_features_list)\n",
        "        self.set_text_features(text_features_list)\n",
        "        print(f\"Feature extraction complete!\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.audio_features is None or self.text_features is None:\n",
        "            raise ValueError(\"Features not set.\")\n",
        "        return self.audio_features[idx], self.text_features[idx], self.labels[idx]\n",
        "\n",
        "# STEP 13: Create Datasets\n",
        "print(\"\\nCreating train/val/test datasets...\")\n",
        "def create_dataset_from_transcripts(transcript_csv_path, audio_extractor, text_processor,\n",
        "                                  dataset_type='diagnosis', test_size=0.2, val_size=0.1):\n",
        "    full_dataset = ADDataset(csv_path=transcript_csv_path, dataset_type=dataset_type)\n",
        "    full_dataset.extract_features_from_transcripts(audio_extractor, text_processor)\n",
        "    indices = list(range(len(full_dataset)))\n",
        "    labels = [full_dataset.raw_labels[i] for i in indices]\n",
        "    train_val_idx, test_idx = train_test_split(\n",
        "        indices, test_size=test_size, stratify=labels, random_state=42\n",
        "    )\n",
        "    train_labels = [labels[i] for i in train_val_idx]\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        train_val_idx, test_size=val_size/(1-test_size), stratify=train_labels, random_state=42\n",
        "    )\n",
        "    def create_subset(indices):\n",
        "        audio_subset = full_dataset.audio_features[indices]\n",
        "        text_subset = full_dataset.text_features[indices]\n",
        "        label_subset = full_dataset.labels[indices]\n",
        "        return ADDataset(\n",
        "            audio_features=audio_subset,\n",
        "            text_features=text_subset,\n",
        "            labels=label_subset,\n",
        "            dataset_type=dataset_type,\n",
        "            normalize_audio=False,\n",
        "            normalize_text=False\n",
        "        )\n",
        "    train_dataset = create_subset(train_idx)\n",
        "    val_dataset = create_subset(val_idx)\n",
        "    test_dataset = create_subset(test_idx)\n",
        "    print(f\"\\nDataset split completed:\")\n",
        "    print(f\"  Training: {len(train_dataset)} samples\")\n",
        "    print(f\"  Validation: {len(val_dataset)} samples\")\n",
        "    print(f\"  Test: {len(test_dataset)} samples\")\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "# STEP 14: Training Loop\n",
        "print(\"\\nSetting up training loop...\")\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for audio_features, text_features, labels in train_loader:\n",
        "            audio_features, text_features, labels = audio_features.to(device), text_features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(audio_features, text_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}\")\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for audio_features, text_features, labels in val_loader:\n",
        "                audio_features, text_features, labels = audio_features.to(device), text_features.to(device), labels.to(device)\n",
        "                outputs = model(audio_features, text_features)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100 * correct/total:.2f}%\")\n",
        "    return model\n",
        "\n",
        "# STEP 15: Main Execution\n",
        "print(\"\\nStarting main execution...\")\n",
        "audio_extractor = AudioFeatureExtractor()\n",
        "text_processor = BERTTextProcessor()\n",
        "transcript_csv_path = os.path.join(OUTPUT_PATH, \"successful_transcripts.csv\")\n",
        "train_dataset, val_dataset, test_dataset = create_dataset_from_transcripts(\n",
        "    transcript_csv_path, audio_extractor, text_processor, dataset_type='diagnosis'\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Initialize model\n",
        "audio_dim = train_dataset.audio_features.shape[1]\n",
        "text_dim = train_dataset.text_features.shape[1]\n",
        "model = MultimodalDARTSClassifier(audio_dim=audio_dim, text_dim=text_dim, num_classes=2)\n",
        "\n",
        "# Train model\n",
        "print(\"\\nTraining model...\")\n",
        "model = train_model(model, train_loader, val_loader, num_epochs=10)\n",
        "\n",
        "# STEP 16: Evaluate on Test Set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "def evaluate_model(model, test_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, labels in test_loader:\n",
        "            audio_features, text_features, labels = audio_features.to(device), text_features.to(device), labels.to(device)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['ad', 'cn']))\n",
        "\n",
        "evaluate_model(model, test_loader)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PIPELINE EXECUTION COMPLETE!\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6a7eaa4b2d5f403cb80d07312619a667",
            "8dcad052691c478a979a069cfc9caccf",
            "b2b9c39212a94262a3cb2897f979d550",
            "c68c80b82b114c558074fcdb668bd4d9",
            "7c4c625ec9d5470b94c52ad4209a6deb",
            "becc661ac52d43309989eae176a92149",
            "ac9c985f6ef2411fb80c2c7083ec8972",
            "df85847ec24e416390d1c8db8f588760",
            "e29c8dce32344395ad9510c3f86e6170",
            "0d45e4db1c694728944ab31d6f7cedd8",
            "cb8f0335297d4f1f97e26c06a689cc71",
            "d5ebbc015be14acebce22b2172c030df",
            "456da9b8496841c5bedefa1d060bf597",
            "c085f9e257794495a69066787e640f8a",
            "85e73bb9d98146e5872cde14ab9af0b8",
            "5e259c8e9a2e4fe0b5c4b93c16f55978",
            "1b8aad7048ea4be1b4fad8c03f563126",
            "ce12e59fff784143b1e6f7e692218bb0",
            "2224a2f68c1648dbb3964d7e320eeed3",
            "8aef1eb196714d12aede492560c992b8",
            "f21c6aeef74c43afaa59816ab01bfaab",
            "91f1a331a5d7409691e2f1b5b57d2c21",
            "f3ab2005b2fc464894b85871a5f9c1ba",
            "cba99f177641484289fe61d739749c05",
            "a4da2fa8f91544518f8d0f17191dcb11",
            "2b2d247b8fd44f51a900433cad9ed2d0",
            "f2c531929ab946a4a76516d5eeda1807",
            "c5fa90d20fae472ba7d1024d6fa1673f",
            "d2e2a458185b48fdb2101c4bc57471fd",
            "d1032802e3544c228106e4dc089bba4f",
            "53200fd4ef104c2e8ecb338be26794d0",
            "518b0cc591fb46f281b628e48f735c37",
            "1d7dccb3395640bf96b220967b90d2e9",
            "653825fc95694b6786610510b9e95a69",
            "a2baad0073fe41f7b394dd191eb016ef",
            "009fef0472be4b0ab1e12f61ecf72f79",
            "5334458e2e8f4d8a975409f644564829",
            "6a38ec3abfaa46448ab3df6fbcbeef86",
            "74083638268e4d31b45ce70e8432fb68",
            "b33424c9b1f04cf4919eb4001c738d40",
            "53bab7cf2b8643e18818a6b57b1238c5",
            "3ce5b68a94954c20bdf105f1410afd44",
            "b8e7b231ea7942da8bc8944e3bf7c924",
            "852fec41002544ff99e8d3d194062af3",
            "331d8e39401744d0b2fd6ea1c281fafa",
            "3bab993f2a334e079c27930205fdb0d5",
            "9d74e3dae780484986df9bdaed3d0716",
            "88f778eefd7142bb9eafdfe89632fb02",
            "4db5677ece2f4a358f798634e750d049",
            "1bdf4bfe07e24c7b89e6b9ec6d3d109f",
            "4e3da68f42ac40bdb59de8ee7ef2fc25",
            "901d39f2b4154876905fdca3b8b1e846",
            "a5ebd61a7f8740e89c3c0a65dad9b858",
            "c680ad5b71f0475eacc55acc0988f4bf",
            "7c1ce8d3080e4132a03a2a87552ef7ee"
          ]
        },
        "id": "sF3zFSj88zyl",
        "outputId": "dbb58967-73df-444e-c1b9-569cf7cc0cd7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ADReSSo21 INTEGRATED AUDIO TRANSCRIPT AND AD CLASSIFICATION PIPELINE\n",
            "================================================================================\n",
            "\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✓ Google Drive mounted successfully!\n",
            "\n",
            "Setting up paths and configuration...\n",
            "✓ Base path: /content/drive/MyDrive/Voice/\n",
            "✓ Extract path: /content/drive/MyDrive/Voice/extracted/\n",
            "✓ Output path: /content/drive/MyDrive/Voice/transcripts/\n",
            "\n",
            "Setting up audio preprocessing...\n",
            "\n",
            "Setting up speech recognition...\n",
            "\n",
            "Processing audio files and extracting transcripts...\n",
            "\n",
            "  Processing diagnosis training data...\n",
            "    Processing 87 ad files...\n",
            "      Processing 1/87: adrso024.wav\n",
            "      Processing 2/87: adrso045.wav\n",
            "      Processing 3/87: adrso043.wav\n",
            "      Processing 4/87: adrso036.wav\n",
            "      Processing 5/87: adrso060.wav\n",
            "      Processing 6/87: adrso074.wav\n",
            "      Processing 7/87: adrso070.wav\n",
            "      Processing 8/87: adrso071.wav\n",
            "      Processing 9/87: adrso072.wav\n",
            "      Processing 10/87: adrso077.wav\n",
            "      Processing 11/87: adrso047.wav\n",
            "      Processing 12/87: adrso068.wav\n",
            "      Processing 13/87: adrso075.wav\n",
            "      Processing 14/87: adrso049.wav\n",
            "      Processing 15/87: adrso053.wav\n",
            "      Processing 16/87: adrso059.wav\n",
            "      Processing 17/87: adrso055.wav\n",
            "      Processing 18/87: adrso126.wav\n",
            "      Processing 19/87: adrso106.wav\n",
            "      Processing 20/87: adrso092.wav\n",
            "      Processing 21/87: adrso123.wav\n",
            "      Processing 22/87: adrso109.wav\n",
            "      Processing 23/87: adrso130.wav\n",
            "      Processing 24/87: adrso093.wav\n",
            "      Processing 25/87: adrso089.wav\n",
            "      Processing 26/87: adrso134.wav\n",
            "      Processing 27/87: adrso128.wav\n",
            "      Processing 28/87: adrso142.wav\n",
            "      Processing 29/87: adrso138.wav\n",
            "      Processing 30/87: adrso125.wav\n",
            "      Processing 31/87: adrso112.wav\n",
            "      Processing 32/87: adrso110.wav\n",
            "      Processing 33/87: adrso187.wav\n",
            "      Processing 34/87: adrso116.wav\n",
            "      Processing 35/87: adrso098.wav\n",
            "      Processing 36/87: adrso144.wav\n",
            "      Processing 37/87: adrso188.wav\n",
            "      Processing 38/87: adrso141.wav\n",
            "      Processing 39/87: adrso218.wav\n",
            "      Processing 40/87: adrso198.wav\n",
            "      Processing 41/87: adrso205.wav\n",
            "      Processing 42/87: adrso206.wav\n",
            "      Processing 43/87: adrso211.wav\n",
            "      Processing 44/87: adrso216.wav\n",
            "      Processing 45/87: adrso209.wav\n",
            "      Processing 46/87: adrso237.wav\n",
            "      Processing 47/87: adrso229.wav\n",
            "      Processing 48/87: adrso249.wav\n",
            "      Processing 49/87: adrso250.wav\n",
            "      Processing 50/87: adrso234.wav\n",
            "      Processing 51/87: adrso228.wav\n",
            "      Processing 52/87: adrso222.wav\n",
            "      Processing 53/87: adrso233.wav\n",
            "      Processing 54/87: adrso223.wav\n",
            "      Processing 55/87: adrso232.wav\n",
            "      Processing 56/87: adrso236.wav\n",
            "      Processing 57/87: adrso253.wav\n",
            "      Processing 58/87: adrso248.wav\n",
            "      Processing 59/87: adrso246.wav\n",
            "      Processing 60/87: adrso247.wav\n",
            "      Processing 61/87: adrso245.wav\n",
            "      Processing 62/87: adrso224.wav\n",
            "      Processing 63/87: adrso189.wav\n",
            "      Processing 64/87: adrso202.wav\n",
            "      Processing 65/87: adrso063.wav\n",
            "      Processing 66/87: adrso244.wav\n",
            "      Processing 67/87: adrso122.wav\n",
            "      Processing 68/87: adrso039.wav\n",
            "      Processing 69/87: adrso190.wav\n",
            "      Processing 70/87: adrso192.wav\n",
            "      Processing 71/87: adrso215.wav\n",
            "      Processing 72/87: adrso090.wav\n",
            "      Processing 73/87: adrso025.wav\n",
            "      Processing 74/87: adrso031.wav\n",
            "      Processing 75/87: adrso197.wav\n",
            "      Processing 76/87: adrso027.wav\n",
            "      Processing 77/87: adrso056.wav\n",
            "      Processing 78/87: adrso054.wav\n",
            "      Processing 79/87: adrso078.wav\n",
            "      Processing 80/87: adrso200.wav\n",
            "      Processing 81/87: adrso028.wav\n",
            "      Processing 82/87: adrso220.wav\n",
            "      Processing 83/87: adrso212.wav\n",
            "      Processing 84/87: adrso046.wav\n",
            "      Processing 85/87: adrso035.wav\n",
            "      Processing 86/87: adrso033.wav\n",
            "      Processing 87/87: adrso032.wav\n",
            "    Processing 79 cn files...\n",
            "      Processing 1/79: adrso002.wav\n",
            "      Processing 2/79: adrso021.wav\n",
            "      Processing 3/79: adrso012.wav\n",
            "      Processing 4/79: adrso022.wav\n",
            "      Processing 5/79: adrso010.wav\n",
            "      Processing 6/79: adrso007.wav\n",
            "      Processing 7/79: adrso016.wav\n",
            "      Processing 8/79: adrso015.wav\n",
            "      Processing 9/79: adrso008.wav\n",
            "      Processing 10/79: adrso014.wav\n",
            "      Processing 11/79: adrso018.wav\n",
            "      Processing 12/79: adrso017.wav\n",
            "      Processing 13/79: adrso003.wav\n",
            "      Processing 14/79: adrso019.wav\n",
            "      Processing 15/79: adrso005.wav\n",
            "      Processing 16/79: adrso169.wav\n",
            "      Processing 17/79: adrso157.wav\n",
            "      Processing 18/79: adrso023.wav\n",
            "      Processing 19/79: adrso148.wav\n",
            "      Processing 20/79: adrso173.wav\n",
            "      Processing 21/79: adrso153.wav\n",
            "      Processing 22/79: adrso172.wav\n",
            "      Processing 23/79: adrso161.wav\n",
            "      Processing 24/79: adrso165.wav\n",
            "      Processing 25/79: adrso164.wav\n",
            "      Processing 26/79: adrso156.wav\n",
            "      Processing 27/79: adrso159.wav\n",
            "      Processing 28/79: adrso168.wav\n",
            "      Processing 29/79: adrso170.wav\n",
            "      Processing 30/79: adrso154.wav\n",
            "      Processing 31/79: adrso152.wav\n",
            "      Processing 32/79: adrso160.wav\n",
            "      Processing 33/79: adrso167.wav\n",
            "      Processing 34/79: adrso183.wav\n",
            "      Processing 35/79: adrso178.wav\n",
            "      Processing 36/79: adrso260.wav\n",
            "      Processing 37/79: adrso182.wav\n",
            "      Processing 38/79: adrso180.wav\n",
            "      Processing 39/79: adrso276.wav\n",
            "      Processing 40/79: adrso264.wav\n",
            "      Processing 41/79: adrso266.wav\n",
            "      Processing 42/79: adrso273.wav\n",
            "      Processing 43/79: adrso177.wav\n",
            "      Processing 44/79: adrso186.wav\n",
            "      Processing 45/79: adrso274.wav\n",
            "      Processing 46/79: adrso262.wav\n",
            "      Processing 47/79: adrso307.wav\n",
            "      Processing 48/79: adrso281.wav\n",
            "      Processing 49/79: adrso286.wav\n",
            "      Processing 50/79: adrso289.wav\n",
            "      Processing 51/79: adrso296.wav\n",
            "      Processing 52/79: adrso299.wav\n",
            "      Processing 53/79: adrso291.wav\n",
            "      Processing 54/79: adrso285.wav\n",
            "      Processing 55/79: adrso315.wav\n",
            "      Processing 56/79: adrso308.wav\n",
            "      Processing 57/79: adrso312.wav\n",
            "      Processing 58/79: adrso298.wav\n",
            "      Processing 59/79: adrso310.wav\n",
            "      Processing 60/79: adrso316.wav\n",
            "      Processing 61/79: adrso309.wav\n",
            "      Processing 62/79: adrso277.wav\n",
            "      Processing 63/79: adrso283.wav\n",
            "      Processing 64/79: adrso302.wav\n",
            "      Processing 65/79: adrso300.wav\n",
            "      Processing 66/79: adrso162.wav\n",
            "      Processing 67/79: adrso278.wav\n",
            "      Processing 68/79: adrso292.wav\n",
            "      Processing 69/79: adrso261.wav\n",
            "      Processing 70/79: adrso265.wav\n",
            "      Processing 71/79: adrso268.wav\n",
            "      Processing 72/79: adrso259.wav\n",
            "      Processing 73/79: adrso151.wav\n",
            "      Processing 74/79: adrso267.wav\n",
            "      Processing 75/79: adrso257.wav\n",
            "      Processing 76/79: adrso263.wav\n",
            "      Processing 77/79: adrso280.wav\n",
            "      Processing 78/79: adrso270.wav\n",
            "      Processing 79/79: adrso158.wav\n",
            "\n",
            "Saving transcription results...\n",
            "✓ Saved complete results to: /content/drive/MyDrive/Voice/transcripts/all_transcripts.csv\n",
            "✓ Saved successful transcripts to: /content/drive/MyDrive/Voice/transcripts/successful_transcripts.csv\n",
            "\n",
            "Setting up audio feature extraction...\n",
            "\n",
            "Setting up BERT text processing...\n",
            "\n",
            "Setting up DARTS architecture...\n",
            "\n",
            "Setting up dataset class...\n",
            "\n",
            "Creating train/val/test datasets...\n",
            "\n",
            "Setting up training loop...\n",
            "\n",
            "Starting main execution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a7eaa4b2d5f403cb80d07312619a667"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5ebbc015be14acebce22b2172c030df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3ab2005b2fc464894b85871a5f9c1ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "653825fc95694b6786610510b9e95a69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "331d8e39401744d0b2fd6ea1c281fafa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from /content/drive/MyDrive/Voice/transcripts/successful_transcripts.csv...\n",
            "Found 98 successful transcripts\n",
            "After filtering: 98 samples\n",
            "Label mapping: {np.str_('ad'): 0, np.str_('cn'): 1}\n",
            "Extracting audio and text features...\n",
            "  Processing audio 1/98\n",
            "  Processing audio 11/98\n",
            "  Processing audio 21/98\n",
            "  Processing audio 31/98\n",
            "  Processing audio 41/98\n",
            "  Processing audio 51/98\n",
            "  Processing audio 61/98\n",
            "  Processing audio 71/98\n",
            "  Processing audio 81/98\n",
            "  Processing audio 91/98\n",
            "  Processing text 1/98\n",
            "  Processing text 21/98\n",
            "  Processing text 41/98\n",
            "  Processing text 61/98\n",
            "  Processing text 81/98\n",
            "Feature extraction complete!\n",
            "Loading dataset from pre-processed arrays...\n",
            "Dataset loaded: 68 samples\n",
            "Loading dataset from pre-processed arrays...\n",
            "Dataset loaded: 10 samples\n",
            "Loading dataset from pre-processed arrays...\n",
            "Dataset loaded: 20 samples\n",
            "\n",
            "Dataset split completed:\n",
            "  Training: 68 samples\n",
            "  Validation: 10 samples\n",
            "  Test: 20 samples\n",
            "\n",
            "Training model...\n",
            "Epoch 1/10, Train Loss: 0.7809\n",
            "Validation Loss: 0.8117, Accuracy: 50.00%\n",
            "Epoch 2/10, Train Loss: 0.6792\n",
            "Validation Loss: 0.7566, Accuracy: 40.00%\n",
            "Epoch 3/10, Train Loss: 0.7109\n",
            "Validation Loss: 0.7699, Accuracy: 40.00%\n",
            "Epoch 4/10, Train Loss: 0.5074\n",
            "Validation Loss: 0.7879, Accuracy: 50.00%\n",
            "Epoch 5/10, Train Loss: 0.5091\n",
            "Validation Loss: 0.8264, Accuracy: 50.00%\n",
            "Epoch 6/10, Train Loss: 0.3703\n",
            "Validation Loss: 0.8647, Accuracy: 70.00%\n",
            "Epoch 7/10, Train Loss: 0.2536\n",
            "Validation Loss: 1.0495, Accuracy: 50.00%\n",
            "Epoch 8/10, Train Loss: 0.2984\n",
            "Validation Loss: 1.5331, Accuracy: 50.00%\n",
            "Epoch 9/10, Train Loss: 0.4377\n",
            "Validation Loss: 1.6356, Accuracy: 50.00%\n",
            "Epoch 10/10, Train Loss: 0.1431\n",
            "Validation Loss: 1.2054, Accuracy: 70.00%\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Accuracy: 45.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ad       0.50      0.36      0.42        11\n",
            "          cn       0.42      0.56      0.48         9\n",
            "\n",
            "    accuracy                           0.45        20\n",
            "   macro avg       0.46      0.46      0.45        20\n",
            "weighted avg       0.46      0.45      0.45        20\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PIPELINE EXECUTION COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete ADReSSo21 Audio Transcript and AD Classification Pipeline\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import speech_recognition as sr\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "import warnings\n",
        "from sklearn.metrics import classification_report\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPLETE ADReSSo21 AUDIO TRANSCRIPT AND AD CLASSIFICATION PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Set Up Paths (from your code)\n",
        "print(\"\\nSetting up paths...\")\n",
        "BASE_PATH = \"/content/drive/MyDrive/Voice/\"\n",
        "EXTRACT_PATH = \"/content/drive/MyDrive/Voice/extracted/\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Voice/transcripts/\"\n",
        "os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "datasets = {'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'}\n",
        "print(f\"✓ Base path: {BASE_PATH}\")\n",
        "print(f\"✓ Extract path: {EXTRACT_PATH}\")\n",
        "print(f\"✓ Output path: {OUTPUT_PATH}\")\n",
        "\n",
        "# Extract Dataset Files (from your code)\n",
        "print(\"\\nExtracting dataset files...\")\n",
        "def extract_datasets():\n",
        "    for dataset_name, filename in datasets.items():\n",
        "        file_path = os.path.join(BASE_PATH, filename)\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"  Extracting {filename}...\")\n",
        "            try:\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=EXTRACT_PATH)\n",
        "                print(f\"  ✓ {filename} extracted successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Error extracting {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"  ⚠ {filename} not found at {file_path}\")\n",
        "extract_datasets()\n",
        "\n",
        "# Find WAV Files (from your code)\n",
        "print(\"\\nFinding WAV files...\")\n",
        "def find_wav_files():\n",
        "    wav_files = {'diagnosis_train': {'ad': [], 'cn': []}}\n",
        "    diag_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/diagnosis/train/audio/\")\n",
        "    for label in ['ad', 'cn']:\n",
        "        path = os.path.join(diag_train_base, label + \"/\")\n",
        "        if os.path.exists(path):\n",
        "            wavs = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
        "            wav_files['diagnosis_train'][label] = [os.path.join(path, f) for f in wavs]\n",
        "            print(f\"  Found {len(wavs)} {label.upper()} WAV files\")\n",
        "    return wav_files\n",
        "wav_files = find_wav_files()\n",
        "\n",
        "# Audio Preprocessing (from your code)\n",
        "print(\"\\nSetting up audio preprocessing...\")\n",
        "def preprocess_audio(audio_path, target_sr=16000):\n",
        "    try:\n",
        "        audio, sr = librosa.load(audio_path, sr=target_sr)\n",
        "        audio = librosa.util.normalize(audio)\n",
        "        audio, _ = librosa.effects.trim(audio, top_db=20)\n",
        "        return audio, target_sr\n",
        "    except Exception as e:\n",
        "        print(f\"    Error preprocessing {audio_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Additional Audio Conversion\n",
        "def convert_to_wav_if_needed(audio_path):\n",
        "    try:\n",
        "        if not audio_path.endswith('.wav'):\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "            wav_path = audio_path.rsplit('.', 1)[0] + '_converted.wav'\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "            return wav_path\n",
        "        return audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"    Error converting {audio_path}: {e}\")\n",
        "        return audio_path\n",
        "\n",
        "# Speech Recognition with Google/Sphinx\n",
        "print(\"\\nSetting up speech recognition...\")\n",
        "def extract_transcript_from_audio(audio_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "    try:\n",
        "        wav_path = convert_to_wav_if_needed(audio_path)\n",
        "        audio_data, sr_rate = preprocess_audio(wav_path)\n",
        "        if audio_data is None:\n",
        "            return None, None, \"Preprocessing failed\"\n",
        "        temp_wav = audio_path.replace('.wav', '_temp.wav')\n",
        "        sf.write(temp_wav, audio_data, sr_rate)\n",
        "        with sr.AudioFile(temp_wav) as source:\n",
        "            recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
        "            audio = recognizer.listen(source)\n",
        "        transcript = None\n",
        "        error_msg = \"\"\n",
        "        try:\n",
        "            transcript = recognizer.recognize_google(audio)\n",
        "            method = \"google\"\n",
        "        except sr.UnknownValueError:\n",
        "            error_msg = \"Google Speech Recognition could not understand audio\"\n",
        "        except sr.RequestError as e:\n",
        "            error_msg = f\"Google Speech Recognition error: {e}\"\n",
        "        if transcript is None:\n",
        "            try:\n",
        "                transcript = recognizer.recognize_sphinx(audio)\n",
        "                method = \"sphinx\"\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg += \"; Sphinx could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg += f\"; Sphinx error: {e}\"\n",
        "        if os.path.exists(temp_wav):\n",
        "            os.remove(temp_wav)\n",
        "        return transcript, method, None if transcript else error_msg\n",
        "    except Exception as e:\n",
        "        return None, None, f\"Error processing audio: {str(e)}\"\n",
        "\n",
        "# Process Audio Files\n",
        "print(\"\\nProcessing audio files...\")\n",
        "def process_audio_files(wav_files):\n",
        "    all_transcripts = []\n",
        "    print(\"\\n  Processing diagnosis training data...\")\n",
        "    for label in ['ad', 'cn']:\n",
        "        files = wav_files['diagnosis_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "            transcript, method, error = extract_transcript_from_audio(audio_path)\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'diagnosis_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method,\n",
        "                'error': error,\n",
        "                'success': transcript is not None and len(transcript.split()) > 5  # Filter short transcripts\n",
        "            })\n",
        "    return all_transcripts\n",
        "transcripts = process_audio_files(wav_files)\n",
        "\n",
        "# Save Transcription Results\n",
        "print(\"\\nSaving transcription results...\")\n",
        "df = pd.DataFrame(transcripts)\n",
        "complete_output = os.path.join(OUTPUT_PATH, \"all_transcripts.csv\")\n",
        "df.to_csv(complete_output, index=False)\n",
        "print(f\"✓ Saved complete results to: {complete_output}\")\n",
        "successful_df = df[df['success'] == True].copy()\n",
        "success_output = os.path.join(OUTPUT_PATH, \"successful_transcripts.csv\")\n",
        "successful_df.to_csv(success_output, index=False)\n",
        "print(f\"✓ Saved successful transcripts to: {success_output}\")\n",
        "\n",
        "# Audio Feature Extraction\n",
        "print(\"\\nSetting up audio feature extraction...\")\n",
        "class AudioFeatureExtractor:\n",
        "    def __init__(self, sr=16000, n_mfcc=13):\n",
        "        self.sr = sr\n",
        "        self.n_mfcc = n_mfcc\n",
        "    def extract_mfcc_features(self, audio_path):\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc)\n",
        "            return {'mfcc_mean': np.mean(mfccs, axis=1)}\n",
        "        except:\n",
        "            return None\n",
        "    def extract_all_features(self, audio_path):\n",
        "        features = self.extract_mfcc_features(audio_path)\n",
        "        return {k: v for k, v in features.items() if isinstance(v, np.ndarray)} if features else {}\n",
        "\n",
        "# BERT Text Processing\n",
        "print(\"\\nSetting up BERT text processing...\")\n",
        "class BERTTextProcessor:\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).eval()\n",
        "    def preprocess_text(self, text):\n",
        "        if pd.isna(text) or text is None:\n",
        "            return \"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?]', '', text)\n",
        "        return ' '.join(text.split())\n",
        "    def extract_bert_features(self, text):\n",
        "        try:\n",
        "            clean_text = self.preprocess_text(text)\n",
        "            if not clean_text:\n",
        "                return np.zeros(768)\n",
        "            inputs = self.tokenizer(clean_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "        except:\n",
        "            return np.zeros(768)\n",
        "\n",
        "# Simplified DARTS Architecture\n",
        "print(\"\\nSetting up DARTS architecture...\")\n",
        "class ImprovedDARTSCell(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.projection = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()\n",
        "        self.operations = nn.ModuleList([\n",
        "            nn.Identity(),\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU())\n",
        "        ])\n",
        "        self.alpha = nn.Parameter(torch.randn(len(self.operations)))\n",
        "        self.temperature = nn.Parameter(torch.ones(1))\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        weights = F.gumbel_softmax(self.alpha, tau=self.temperature, hard=False) if self.training else F.softmax(self.alpha / self.temperature, dim=0)\n",
        "        return sum(w * op(x) for w, op in zip(weights, self.operations))\n",
        "\n",
        "class MultimodalDARTSClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=64, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.audio_projection = nn.Sequential(nn.Linear(audio_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.3))\n",
        "        self.audio_darts_cell = ImprovedDARTSCell(hidden_dim, hidden_dim)\n",
        "        self.text_projection = nn.Sequential(nn.Linear(text_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.3))\n",
        "        self.text_darts_cell = ImprovedDARTSCell(hidden_dim, hidden_dim)\n",
        "        self.fusion = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim), nn.ReLU(), nn.Dropout(0.3))\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "        self.apply(lambda m: torch.nn.init.xavier_uniform_(m.weight) if isinstance(m, nn.Linear) else None)\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_x = self.audio_projection(audio_features)\n",
        "        audio_x = self.audio_darts_cell(audio_x) + audio_x\n",
        "        text_x = self.text_projection(text_features)\n",
        "        text_x = self.text_darts_cell(text_x) + text_x\n",
        "        fused = torch.cat([audio_x, text_x], dim=1)\n",
        "        fused = self.fusion(fused)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# Dataset Class\n",
        "print(\"\\nSetting up dataset class...\")\n",
        "class ADDataset(Dataset):\n",
        "    def __init__(self, csv_path=None, audio_features=None, text_features=None, labels=None, dataset_type='diagnosis'):\n",
        "        self.dataset_type = dataset_type\n",
        "        self.audio_scaler = StandardScaler()\n",
        "        self.text_scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        if csv_path:\n",
        "            self._load_from_csv(csv_path)\n",
        "        else:\n",
        "            self._load_from_arrays(audio_features, text_features, labels)\n",
        "    def _load_from_csv(self, csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df = df[df['success'] == True].copy()\n",
        "        if self.dataset_type == 'diagnosis':\n",
        "            df = df[df['dataset'] == 'diagnosis_train'].copy()\n",
        "        self.file_ids = df['file_id'].tolist()\n",
        "        self.file_paths = df['file_path'].tolist()\n",
        "        self.transcripts = df['transcript'].tolist()\n",
        "        self.raw_labels = df['label'].tolist()\n",
        "        self.labels = torch.LongTensor(self.label_encoder.fit_transform(self.raw_labels))\n",
        "        self.label_mapping = dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))\n",
        "        print(f\"Loaded {len(df)} samples, Label mapping: {self.label_mapping}\")\n",
        "    def _load_from_arrays(self, audio_features, text_features, labels):\n",
        "        audio_features = self.audio_scaler.fit_transform(np.array(audio_features))\n",
        "        text_features = self.text_scaler.fit_transform(np.array(text_features))\n",
        "        self.audio_features = torch.FloatTensor(audio_features)\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "        self.labels = torch.LongTensor(self.label_encoder.fit_transform(labels)) if isinstance(labels[0], str) else torch.LongTensor(labels)\n",
        "        self.label_mapping = dict(zip(self.label_encoder.classes_, range(len(self.label_encoder.classes_))))\n",
        "    def set_features(self, audio_features, text_features):\n",
        "        audio_features = self.audio_scaler.fit_transform(np.array(audio_features))\n",
        "        text_features = self.text_scaler.fit_transform(np.array(text_features))\n",
        "        self.audio_features = torch.FloatTensor(audio_features)\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "    def extract_features(self, audio_extractor, text_processor):\n",
        "        audio_features_list = []\n",
        "        for i, audio_path in enumerate(self.file_paths):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"  Audio {i+1}/{len(self.file_paths)}\")\n",
        "            features = audio_extractor.extract_all_features(audio_path)\n",
        "            audio_features_list.append([features.get(k, [0]*13)[i%13] for k in sorted(features.keys()) for i in range(13)] if features else [0] * 13)\n",
        "        text_features_list = []\n",
        "        for i, transcript in enumerate(self.transcripts):\n",
        "            if i % 20 == 0:\n",
        "                print(f\"  Text {i+1}/{len(self.transcripts)}\")\n",
        "            text_features_list.append(text_processor.extract_bert_features(transcript))\n",
        "        audio_features = PCA(n_components=10).fit_transform(audio_features_list)\n",
        "        text_features = PCA(n_components=50).fit_transform(text_features_list)\n",
        "        self.set_features(audio_features, text_features)\n",
        "    def get_class_weights(self):\n",
        "        counts = np.bincount(self.labels)\n",
        "        weights = len(self.labels) / (len(counts) * counts)\n",
        "        return torch.FloatTensor(weights)\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.audio_features[idx], self.text_features[idx], self.labels[idx]\n",
        "\n",
        "# Create Datasets\n",
        "print(\"\\nCreating datasets...\")\n",
        "def create_dataset_from_transcripts(transcript_csv_path, audio_extractor, text_processor, dataset_type='diagnosis'):\n",
        "    full_dataset = ADDataset(csv_path=transcript_csv_path, dataset_type=dataset_type)\n",
        "    full_dataset.extract_features(audio_extractor, text_processor)\n",
        "    indices = list(range(len(full_dataset)))\n",
        "    labels = [full_dataset.raw_labels[i] for i in indices]\n",
        "    train_val_idx, test_idx = train_test_split(indices, test_size=0.2, stratify=labels, random_state=42)\n",
        "    train_labels = [labels[i] for i in train_val_idx]\n",
        "    train_idx, val_idx = train_test_split(train_val_idx, test_size=0.15/0.8, stratify=train_labels, random_state=42)\n",
        "    def create_subset(indices):\n",
        "        audio_subset = full_dataset.audio_features[indices]\n",
        "        text_subset = full_dataset.text_features[indices]\n",
        "        label_subset = full_dataset.labels[indices]\n",
        "        return ADDataset(audio_features=audio_subset, text_features=text_subset, labels=label_subset, dataset_type=dataset_type)\n",
        "    train_dataset = create_subset(train_idx)\n",
        "    val_dataset = create_subset(val_idx)\n",
        "    test_dataset = create_subset(test_idx)\n",
        "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "# Training Loop\n",
        "print(\"\\nSetting up training loop...\")\n",
        "def train_model(model, train_loader, val_loader, num_epochs=20):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "    criterion = nn.CrossEntropyLoss(weight=train_loader.dataset.get_class_weights().to(device))\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    counter = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for audio_features, text_features, labels in train_loader:\n",
        "            audio_features, text_features, labels = audio_features.to(device), text_features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(audio_features, text_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for audio_features, text_features, labels in val_loader:\n",
        "                audio_features, text_features, labels = audio_features.to(device), text_features.to(device), labels.to(device)\n",
        "                outputs = model(audio_features, text_features)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Val Acc: {100 * correct/total:.2f}%\")\n",
        "        scheduler.step(val_loss)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), os.path.join(OUTPUT_PATH, \"best_model.pt\"))\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "    model.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, \"best_model.pt\")))\n",
        "    return model\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nSetting up evaluation...\")\n",
        "def evaluate_model(model, test_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device).eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for audio_features, text_features, labels in test_loader:\n",
        "            audio_features, text_features, labels = audio_features.to(device), text_features.to(device), labels.to(device)\n",
        "            outputs = model(audio_features, text_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['ad', 'cn']))\n",
        "\n",
        "# Main Execution\n",
        "print(\"\\nStarting main execution...\")\n",
        "audio_extractor = AudioFeatureExtractor()\n",
        "text_processor = BERTTextProcessor()\n",
        "transcript_csv_path = os.path.join(OUTPUT_PATH, \"successful_transcripts.csv\")\n",
        "train_dataset, val_dataset, test_dataset = create_dataset_from_transcripts(\n",
        "    transcript_csv_path, audio_extractor, text_processor, dataset_type='diagnosis'\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "audio_dim = train_dataset.audio_features.shape[1]\n",
        "text_dim = train_dataset.text_features.shape[1]\n",
        "model = MultimodalDARTSClassifier(audio_dim=audio_dim, text_dim=text_dim, hidden_dim=64, num_classes=2)\n",
        "model = train_model(model, train_loader, val_loader, num_epochs=20)\n",
        "evaluate_model(model, test_loader)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PIPELINE EXECUTION COMPLETE!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "qfYs_WQuF6j4",
        "outputId": "94a32466-6200-45b7-a52b-75f463737fb0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-27-44d0242f4fac>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-44d0242f4fac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}