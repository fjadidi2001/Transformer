    Becker, J. T., Boller, F., Lopez, O. L., Saxton, J., & McGonigle, K. L. (1994). The natural history of Alzheimer's disease: description of study cohort and accuracy of diagnosis. Archives of Neurology, 51(6), 585-594.
    NIA AG03705 and AG05133


for dataset I agreed this permission

Thank you for your interest in DementiaBank. I will add you to our membership list and send you an invitation to join our DementiaBank Google Group, which is an efficient way to communicate about relevant updates and information among members.  

 

Be sure to read and abide by the information about data sharing at the Ground Rules link:  http://talkbank.org/share/ . Your membership and access are contingent upon your adherence to these rules. Specifically, for the Pitt corpus, that means including this citation and acknowledging this grant support in your work:

    Becker, J. T., Boller, F., Lopez, O. L., Saxton, J., & McGonigle, K. L. (1994). The natural history of Alzheimer's disease: description of study cohort and accuracy of diagnosis. Archives of Neurology, 51(6), 585-594.
    NIA AG03705 and AG05133

To access the protected materials, you need to create an account. To do so, go to any of the password protected links (e.g., https://sla.talkbank.org/TBB/dementia ), register as a "New User” with your email address, and choose a password. 


If your students or lab personnel need access, send me (fromm@andrew.cmu.edu) a list of their email addresses and the time period (provide an end date) for their access.  They will then be able to register with their email address and choose a password to use for access. Be sure they understand the ethical principles of shared databases. Feel free to use relevant material on that topic from the TalkBank website (http://talkbank.org/share/ethics.html). They must agree to abide by the Ground Rules and not reuse or recirculate any of the materials.


We expect users to contribute relevant data to DementiaBank as well. When you are ready, send us an email so we can facilitate the uploading process and creation of your corpus. Finally, we encourage you to send us any posters, papers, or presentations that you produce so we can add them to our website’s links and bibliography.   


Best,

Davida

ok, let's dive into Section III.	RESEARCH METHODOLOGY
I wrote this for methodology
First, ask your questions for better writing
Then revised it carefully
Also, you should add figures in the correct place 
Here is a list of the figures with captions:
* **comprehensive_progression_train_file_2.jpg**: Audio Analysis - progression_train_file_2 (Waveform, Spectrogram, MFCC, Chromagram)
* **comprehensive_progression_train_file_1.jpg**: Audio Analysis - progression_train_file_1 (Waveform, Spectrogram, MFCC, Chromagram)
* **comprehensive_diagnosis_train_file_1.jpg**: Audio Analysis - diagnosis_train_file_1 (Waveform, Spectrogram, MFCC, Chromagram)
* **statistical_analysis.png**: Statistical Analysis of Audio Features (Spectral Centroids, Zero Crossing Rates, Spectral Rolloffs, Spectral Bandwidths)
* **comprehensive_progression_test_file_1.jpg**: Audio Analysis - progression_test_file_1 (Waveform, Spectrogram, MFCC, Chromagram)
* **comprehensive_diagnosis_train_file_2.jpg**: Audio Analysis - diagnosis_train_file_2 (Waveform, Spectrogram, MFCC, Chromagram)
* **comprehensive_progression_test_file_2.jpg**: Audio Analysis - progression_test_file_2 (Waveform, Spectrogram, MFCC, Chromagram)
* **download (1).png**: Waveforms of AD (Alzheimer's Disease) and CN (Cognitively Normal) Audio Samples
* **anomaly_detection_results.jpg**: Anomaly Detection Results (Isolation Forest, One-Class SVM, Autoencoder, Combined Anomaly Count)
* **analysis_summary.jpg**: Analysis Summary Report (Generated Files by Directory, Processing Time Distribution, System Information, Dataset Information)
(3).png: MFCC Distributions by Class (Box plots for MFCC features 1-13 comparing AD and CN classes)

(4).jpg: Feature Correlation Heatmap (Correlation matrix of various audio features including duration, amplitude measures, zero-crossing rate, spectral features, silence ratio, and MFCCs)

1.png: Histograms of Audio Features by Class (Distributions of duration, mean_amplitude, rms, zero_crossing_rate, spectral_centroid, spectral_rolloff, and silence_ratio for AD and CN classes)

pca_analysis.png: PCA Analysis of Audio Features (Scatter plot showing the first two principal components, with PC1 accounting for 95.49% variance and PC2 for 3.00% variance)

mfcc_clustering.png: K-means Clustering of MFCC Features (Silhouette Score: 0.547) (Scatter plot of MFCC 1 vs MFCC 2, showing two clusters and their centroids)


class_proportion.png: Diagnosis Task - Class Distribution and Proportion (Bar chart showing the number of samples per class (CN and AD) and a pie chart showing their respective proportions)

training-validation-loss.png: Training and Validation Performance Metrics (Plots for Training and Validation Loss, Accuracy, F1 Score over epochs, and a Validation Confusion Matrix)

features.jpg: Sample Spectrogram - Participant adrso090 (Log-Mel Spectrogram, Delta, and Delta-Delta representations for an AD participant)


This chapter presents a comprehensive overview of the research methodology, detailing the datasets used and the proposed framework for the multimodal detection of Alzheimer’s disease. The primary objective is to describe the data collection and processing pipeline, as well as to introduce the model architecture and training procedure, enabling readers to gain a thorough understanding of the approach adopted to address dementia detection. The ADReSSo dataset, derived from DementiaBank, serves as the foundation of this study and includes audio files and metadata for tasks related to distinguishing Alzheimer’s disease (AD) from healthy controls (CN) and analyzing disease progression.  The proposed framework integrates audio and textual features using advanced deep learning models, which are elaborated below. This chapter is divided into two main sections: Section 3.1 introduces the ADReSSo dataset, detailing its structure, sample size, and associated challenges (e.g., missing transcripts and labeling issues). Section 3.2 analytically describes the proposed framework, focusing on data processing components, model architecture, and training process. The framework leverages transformer models (BERT and ViT) and a cross-attention mechanism to provide an innovative approach to integrating audio and textual data. This section aims to provide a clear and precise overview of the methods used, serving as a foundation for analyzing results and future recommendations.
A.	Dataset
The ADReSSo dataset, extracted from DementiaBank, is a standard resource for dementia detection through speech and text analysis, designed for two tasks: distinguishing Alzheimer’s disease (AD) from healthy controls (CN) and analyzing disease progression (Decline vs. Stable). It includes audio files (.wav), metadata (.csv), and, in some cases, transcripts (.cha) from spontaneous speech.
 
Fig. 1.	Waveform plots of AD and CN audio samples.
 The diagnosis_train set, with 166 audio files and 167 metadata files, provides a balanced distribution (47.6% CN, 52.4% AD) for training machine learning models and includes information such as age and gender. The progression_train set, with 73 audio files and 47 metadata files, and the progression_test set, with 32 audio files and 16 metadata files, are used for disease progression analysis. The absence of transcript files (.cha) limited textual processing, forcing reliance on metadata or alternative text, which reduced the effectiveness of the textual component in the multimodal approach. The small sample size, particularly in progression_test, increased the risk of overfitting. Despite these challenges, the audio quality of ADReSSo and its focus on speech features, such as pauses and frequency variations, make it valuable for Alzheimer’s detection. The structured organization of diagnosis_train (with subfolders ad/ and cn/) facilitated label extraction, but the need for robust methods to address labeling issues and data scarcity will be explored in the proposed framework.
Dataset Name	Task Type	Number of Audio Files	Number of Metadata Files
diagnosis_train	Detection (AD vs. CN)	166	167
progression_train	Progression (Decline vs. Stable)	73	47
progression_test	Progression (Decline vs. Stable)	32	16
    Table III: Specifications of the ADReSSo Dataset

 
Fig. 2.	Statistical Analysis of Audio Features Across Different Categories
B.	Proposed Framework
The proposed framework for this research is designed to detect Alzheimer’s disease and analyze disease progression using multimodal (audio and textual) data. As mentioned, this framework utilizes the ADReSSo dataset, which includes audio files and metadata for Alzheimer’s detection and progression tasks. The primary goal is to integrate audio features (e.g., mel spectrograms) and textual features (speech transcripts) using advanced deep learning models to identify dementia-related patterns. The framework comprises three main stages: data processing, multimodal model architecture, and training and evaluation processes.  Initial data processing aimed to prepare ADReSSo data for model use. This stage involved extracting compressed files (.tgz) from specified Google Drive paths. Data access was achieved by connecting to Google Drive and verifying the presence of files in the diagnosis_train, progression_train, and progression_test sets. This process ensured that audio and metadata files were properly accessible, though the absence of transcript files (.cha) posed a significant challenge, addressed later.  Audio processing is a key component of the framework. Audio files (.wav) were analyzed using advanced signal processing techniques, including initial and final silence removal, amplitude normalization, and conversion of audio signals into frequency-domain features such as mel spectrograms, MFCC coefficients, and spectral features. These features were selected for their ability to capture dementia-related speech patterns, such as pauses and frequency variations. Mel spectrograms served as the primary input for the Vision Transformer (ViT) model, enabling visual analysis of audio features.  The creation of three-channel spectrograms was an innovation of this framework. To enhance ViT performance, mel spectrograms were generated as three-channel inputs (mel spectrogram, delta, and delta-delta), capturing dynamic frequency and temporal changes. These channels were used as image inputs for ViT, enabling the extraction of complex audio features, though high computational requirements were managed using GPU accelerators (verified by CUDA).  Textual processing was another component, challenged by the absence of transcript files (.cha) in ADReSSo. In the absence of direct transcripts, metadata files (.csv) were used to extract textual information. These files contained segmentation data but did not provide full conversation texts. To address this, placeholder texts (e.g., "No transcript available") were used, reducing the effectiveness of the textual modality. Efforts were made to optimize available textual data by cleaning CHAT annotations (e.g., speaker markers and hesitations).  Label extraction was another data processing step. In the diagnosis_train set, labels (AD vs. CN) were accurately extracted using the folder structure (ad/ and cn/), creating a balanced distribution (47.6% CN, 52.4% AD). However, in the progression_train and progression_test sets, missing Decline/No_Decline folder structures and insufficient metadata information posed challenges.  The creation of a paired dataset for integrating audio and textual data was performed by matching audio and text files using participant IDs. In diagnosis_train, 166 paired samples were successfully created, but in progression_train (73 samples) and progression_test (32 samples), incorrect labeling hindered preparation for the progression task. This stage was implemented using a PyTorch Dataset class, enabling simultaneous audio and text processing.  Data splitting into training and validation sets was a critical step. From 239 paired samples (166 from diagnosis_train and 73 from progression_train), 191 samples were allocated for training and 48 for validation. This split aimed to maintain class balance, but incorrect progression data labeling led to an imbalanced validation set (33.3% CN, 66.7% AD).  The proposed framework offers an innovative approach to multimodal Alzheimer’s detection using audio and textual data. Initial data processing ensured effective access to the ADReSSo dataset. Audio processing, a key strength, extracted rich features from audio files (.wav) using advanced signal processing techniques. The innovation of three-channel spectrograms is a standout feature. Textual processing was designed to extract semantic information from text data. Although the ADReSSo-21 dataset lacked transcript files (.cha), the framework creatively used metadata (.csv) files and segmentation data to extract textual information effectively. Cleaning CHAT annotations (e.g., speaker markers and hesitations) enabled the generation of clean, usable texts. This flexible approach demonstrated the framework’s ability to handle incomplete data, providing a foundation for textual integration in the multimodal model. The creation of a paired dataset was another innovation, enabling audio-text integration. By matching audio and text files based on participant IDs, 166 paired samples for diagnosis_train and 73 for progression_train were created.  Data splitting into training and validation sets was carefully performed to ensure fair model evaluation. The multimodal model architecture, a key innovation, combines two advanced models: BERT and Vision Transformer (ViT). The BERT model (bert-base-uncased) processed text, producing 768-dimensional vectors from the [CLS] token, mapped to a 512-dimensional space. This approach enabled deep semantic text analysis, serving as a robust foundation for the textual modality.  Audio processing with ViT used the google/vit-base-patch16-224 model, transforming three-channel spectrograms into 768-dimensional vectors, also mapped to a 512-dimensional space for compatibility with textual features. ViT was chosen for its ability to identify complex visual patterns in spectrograms, enabling precise analysis of Alzheimer’s-related audio features. This approach positioned the framework at the forefront of deep learning technology.  The cross-attention mechanism is the innovative core of this framework, enabling the integration of audio and textual features. Using 8 attention heads and a 512-dimensional space, this mechanism allowed textual features to attend to audio features and vice versa, producing 1024-dimensional fused vectors. This approach identified complex interactions between modalities, significantly improving detection accuracy.  The classification head was designed to convert fused features into final predictions. It consisted of multiple linear layers, reducing dimensions from 1024 to 512, then to 256, and finally to 2 classes, using ReLU activation and a dropout rate of 0.3. Weight initialization with the Xavier uniform method and zero bias initialization enhanced training stability. This architecture significantly improved the model’s accuracy and efficiency in distinguishing Alzheimer’s from healthy controls.  Model optimization used the AdamW optimizer with differential learning rates, preventing catastrophic forgetting in pre-trained models and enabling fine-tuning. Weight decay of 0.01 ensured effective regularization.  Learning rate scheduling was implemented with CosineAnnealingLR, dynamically adjusting the learning rate. With a maximum period (T_max) set to the number of training batches multiplied by 50 epochs, this method ensured stable model convergence, improving training efficiency.  The CrossEntropyLoss criterion was selected for the binary classification task due to its simplicity and effectiveness, comparing model predictions with true labels to enable precise optimization. This criterion allowed the framework to perform effectively on the ADReSSo-21 dataset.  Overfitting management was creatively implemented using early stopping, monitoring validation F1 scores and saving the best model upon no improvement. This approach optimized model performance on limited data, preventing overfitting.  Metric tracking used a dedicated tool to compute loss, accuracy, precision, recall, and F1 score, employing weighted averaging for precise performance analysis and generating classification reports and confusion matrices. This capability transformed the framework into a comprehensive evaluation tool.  Training and validation were conducted using high-performance PyTorch data loaders. The training loader (191 samples, 48 batches) and validation loader (48 samples, 12 batches) were processed on GPU, reducing epoch training time to 62–80 seconds. This efficiency enabled rapid experimentation and continuous improvement.  Error handling was a strength of the framework. Using placeholder texts for missing textual data and random noise for inaccessible audio files, the framework demonstrated high resilience to incomplete data, enhancing system stability in real-world conditions.  Visualization and analysis of results, including loss, accuracy, and F1 score plots, as well as confusion matrices, enabled in-depth performance analysis. These visualizations allowed researchers to identify model strengths and areas for improvement, making the framework a valuable tool for future research.  The framework’s potential for real-world application is a key highlight. By focusing on early Alzheimer’s detection, it can serve as a non-invasive tool in clinical settings, particularly in communities like Iran, where demand for advanced diagnostic tools is growing. Integrating audio and textual data improved detection accuracy, offering high potential for further development.  The framework’s flexibility and scalability enable its extension to other datasets or similar tasks. For instance, collecting Persian-language speech data could adapt the framework for dementia detection in Persian-speaking populations. This flexibility enhances the framework’s value for future research.  Future directions include improving text extraction from metadata files, fine-tuning ViT models, and incorporating audio data augmentation techniques. This framework successfully established a robust foundation for multimodal Alzheimer’s detection, and with these improvements, it could become a leading tool in the field.  ool in clinical settings, particularly in communities like Iran, where demand for advanced diagnostic tools is growing. Integrating audio and textual data improved detection accuracy, offering high potential for further development.  The framework’s flexibility and scalability enable its extension to other datasets or similar tasks. For instance, collecting Persian-language speech data could adapt the framework for dementia detection in Persian-speaking populations. This flexibility enhances the framework’s value for future research.  Future directions include improving text extraction from metadata files, fine-tuning ViT models, and incorporating audio data augmentation techniques. This framework successfully established a robust foundation for multimodal Alzheimer’s detection, and with these improvements, it could become a leading tool in the field.  