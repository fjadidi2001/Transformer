چرا با اینکه بیش از 70 ساله هوش مصنوعی وجود  داره ولی این سال های اخیر معروف شده؟

سال 2017 گوگل یه مقاله‌ برگ‌ریزون برای معرفی ترنسفورمر ها داد ( t داخل chatgpt هم یعنی ترنسفورمر)؛ تیتر مقاله این بود

 Attention Is All You Need

گوگل می‌گفت بابا RNN و LSTM رو بزارید کنار ما یه فرمول خیلی خفن پیدا کردیم... که چهار پنج سال بعد یهویی با مدل هایی که شرکت ها باهاش ساختن و آپگریدش کردن حسابی ترکوند.

ترنسفورمر به صورت ساده فرقش با روش های قدیمی اینه که Self-Attention یا مکانیسم توجه به خود داره در واقع این بخش کمک می‌کنه مدل بفهمه تو جمله هر کلمه، چقدر به بقیه کلمات ربط داره! مثلا جمله زیرو در نظر بگیر:

"من امروز کتاب جدیدی خریدم."

حالا توی مدل ترنسفورمر وقتی می‌خواد بفهمه کلمه‌ی "کتاب" چه نقشی تو جمله داره، به جای اینکه فقط به همون کلمه نگاه کنه، می‌ره و تمام کلمات دیگه جمله رو هم نگاه می‌کنه و وزن هر کدوم رو بر اساس اهمیتش نسبت به "کتاب" حساب می‌کنه.


