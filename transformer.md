

ترنسفورمر به صورت ساده فرقش با روش های قدیمی اینه که Self-Attention یا مکانیسم توجه به خود داره در واقع این بخش کمک می‌کنه مدل بفهمه تو جمله هر کلمه، چقدر به بقیه کلمات ربط داره! مثلا جمله زیرو در نظر بگیر:

"من امروز کتاب جدیدی خریدم."

حالا توی مدل ترنسفورمر وقتی می‌خواد بفهمه کلمه‌ی "کتاب" چه نقشی تو جمله داره، به جای اینکه فقط به همون کلمه نگاه کنه، می‌ره و تمام کلمات دیگه جمله رو هم نگاه می‌کنه و وزن هر کدوم رو بر اساس اهمیتش نسبت به "کتاب" حساب می‌کنه.


